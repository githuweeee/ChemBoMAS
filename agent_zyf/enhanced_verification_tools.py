# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Enhanced Verification Agent Tools - å®ç°7ä¸ªæ ¸å¿ƒä»»åŠ¡çš„å·¥å…·å‡½æ•°"""

import os
import pandas as pd
import numpy as np
import json
from datetime import datetime
from google.adk.tools import ToolContext

# æ³¨æ„ï¼šä»¥ä¸‹å¯¼å…¥éœ€è¦å®‰è£…BayBE
# pip install baybe
try:
    from baybe.utils.chemistry import get_canonical_smiles, name_to_smiles
    from baybe.parameters import CategoricalParameter, NumericalContinuousParameter, NumericalDiscreteParameter
    from baybe.parameters.enum import SubstanceEncoding
    BAYBE_AVAILABLE = True
except ImportError:
    print("Warning: BayBE not installed. Please run: pip install baybe")
    BAYBE_AVAILABLE = False


class SimplifiedSMILESValidator:
    """
    ç®€åŒ–çš„SMILESéªŒè¯å™¨ - æ— éœ€æ‰‹åŠ¨è®¡ç®—æè¿°ç¬¦
    BayBEä¼šåœ¨Campaignä¸­è‡ªåŠ¨å¤„ç†æ‰€æœ‰åˆ†å­æè¿°ç¬¦è®¡ç®—
    """
    
    def validate_smiles_data(self, data: pd.DataFrame) -> dict:
        """
        åªéªŒè¯SMILESæœ‰æ•ˆæ€§ï¼Œä¸è®¡ç®—æè¿°ç¬¦
        """
        validation_results = {
            "canonical_smiles_mapping": {},
            "invalid_smiles": [],
            "substances_validated": []
        }
        
        if not BAYBE_AVAILABLE:
            # é™çº§å¤„ç†ï¼šä½¿ç”¨åŸºæœ¬éªŒè¯
            return self._basic_smiles_validation(data)
        
        # è¯†åˆ«SMILESåˆ—
        smiles_columns = [col for col in data.columns if 'SMILE' in col.upper()]
        
        for col in smiles_columns:
            substance_name = col.split('_')[0] if '_' in col else col
            
            for idx, smiles in data[col].items():
                if pd.isna(smiles) or smiles == '':
                    continue
                    
                try:
                    # åªéªŒè¯å¹¶è·å–è§„èŒƒåŒ–SMILES
                    canonical_smiles = get_canonical_smiles(str(smiles))
                    
                    if canonical_smiles is not None:
                        validation_results["canonical_smiles_mapping"][smiles] = canonical_smiles
                    else:
                        validation_results["invalid_smiles"].append({
                            "substance": substance_name,
                            "row": idx,
                            "smiles": smiles,
                            "error": "æ— æ³•è§£æåˆ†å­ç»“æ„"
                        })
                        
                except Exception as e:
                    validation_results["invalid_smiles"].append({
                        "substance": substance_name,
                        "row": idx, 
                        "smiles": smiles,
                        "error": str(e)
                    })
            
            validation_results["substances_validated"].append(substance_name)
            
        return validation_results
    
    def _basic_smiles_validation(self, data: pd.DataFrame) -> dict:
        """
        åŸºæœ¬SMILESéªŒè¯ï¼ˆå½“BayBEä¸å¯ç”¨æ—¶ï¼‰
        """
        validation_results = {
            "canonical_smiles_mapping": {},
            "invalid_smiles": [],
            "substances_validated": []
        }
        
        smiles_columns = [col for col in data.columns if 'SMILE' in col.upper()]
        
        for col in smiles_columns:
            substance_name = col.split('_')[0] if '_' in col else col
            
            for idx, smiles in data[col].items():
                if pd.isna(smiles) or smiles == '':
                    continue
                    
                # åŸºæœ¬æ ¼å¼æ£€æŸ¥
                if isinstance(smiles, str) and len(smiles) > 0:
                    validation_results["canonical_smiles_mapping"][smiles] = smiles  # ä¿æŒåŸæ ·
                else:
                    validation_results["invalid_smiles"].append({
                        "substance": substance_name,
                        "row": idx,
                        "smiles": smiles,
                        "error": "SMILESæ ¼å¼é”™è¯¯"
                    })
            
            validation_results["substances_validated"].append(substance_name)
            
        return validation_results
    
    def prepare_baybe_parameters(self, data: pd.DataFrame, validation_results: dict) -> list:
        """
        ä¸ºBayBEå‡†å¤‡å‚æ•°å®šä¹‰ï¼Œä½¿ç”¨åŸå§‹SMILES
        BayBEå†…éƒ¨ä¼šè‡ªåŠ¨å¤„ç†æè¿°ç¬¦è®¡ç®—
        """
        if not BAYBE_AVAILABLE:
            return []
            
        parameters = []
        
        # 1. åˆ†å­å‚æ•° - ç›´æ¥ä½¿ç”¨SMILESå­—ç¬¦ä¸²
        smiles_columns = [col for col in data.columns if 'SMILE' in col.upper()]
        for col in smiles_columns:
            substance_name = col.split('_')[0] if '_' in col else col
            
            # è·å–æœ‰æ•ˆçš„SMILESå€¼
            valid_smiles = []
            for smiles in data[col].dropna().unique():
                if str(smiles) in validation_results["canonical_smiles_mapping"]:
                    valid_smiles.append(validation_results["canonical_smiles_mapping"][str(smiles)])
            
            if len(valid_smiles) >= 2:  # BayBEè¦æ±‚è‡³å°‘2ä¸ªå€¼
                param = CategoricalParameter(
                    name=f"{substance_name}_molecule",
                    values=valid_smiles,  # BayBEä¼šè‡ªåŠ¨å¤„ç†è¿™äº›SMILESçš„æè¿°ç¬¦
                    encoding="OHE"
                )
                parameters.append(param)
            elif len(valid_smiles) == 1:
                # åªæœ‰1ä¸ªSMILESæ—¶ï¼Œè·³è¿‡åˆ†å­å‚æ•°ï¼ˆå› ä¸ºæ²¡æœ‰ä¼˜åŒ–ç©ºé—´ï¼‰
                print(f"âš ï¸ {substance_name} åªæœ‰1ä¸ªSMILESå€¼ï¼Œè·³è¿‡åˆ†å­å‚æ•°åˆ›å»º")
            else:
                print(f"âš ï¸ {substance_name} æ²¡æœ‰æœ‰æ•ˆSMILESï¼Œè·³è¿‡å‚æ•°åˆ›å»º")
        
        # 2. æ•°å€¼å‚æ•°ï¼ˆæ¯”ä¾‹ç­‰ï¼‰
        ratio_columns = [col for col in data.columns if 'ratio' in col.lower()]
        for col in ratio_columns:
            # å®‰å…¨çš„æ•°å€¼è½¬æ¢å’ŒèŒƒå›´è®¡ç®—
            numeric_data = pd.to_numeric(data[col], errors='coerce').dropna()
            if len(numeric_data) == 0:
                print(f"âš ï¸ {col} åˆ—æ²¡æœ‰æœ‰æ•ˆçš„æ•°å€¼æ•°æ®ï¼Œè·³è¿‡å‚æ•°åˆ›å»º")
                continue
            min_val = float(numeric_data.min())
            max_val = float(numeric_data.max())
            
            param = NumericalContinuousParameter(
                name=col,
                bounds=(max(0.0, min_val), min(1.0, max_val))
            )
            parameters.append(param)
            
        return parameters


class IntelligentParameterAdvisor:
    """
    åŸºäºåŒ–å­¦çŸ¥è¯†çš„æ™ºèƒ½å‚æ•°å»ºè®®ç³»ç»Ÿ
    """
    
    def analyze_experimental_context(self, data: pd.DataFrame, user_description: str = "") -> dict:
        """
        åˆ†æå®éªŒèƒŒæ™¯ï¼Œæä¾›æ™ºèƒ½å‚æ•°å»ºè®®
        """
        suggestions = {}
        
        # 1. åˆ†æåˆ†å­ç±»å‹å’Œç‰¹æ€§
        molecular_analysis = self._analyze_molecules(data)
        
        # 2. ç”Ÿæˆå‚æ•°è¾¹ç•Œå»ºè®®
        for col in data.columns:
            if 'ratio' in col.lower():
                # å®‰å…¨çš„æ•°å€¼è½¬æ¢
                numeric_data = pd.to_numeric(data[col], errors='coerce').dropna()
                if len(numeric_data) == 0:
                    continue
                current_range = [float(numeric_data.min()), float(numeric_data.max())]
                suggestions[col] = {
                    "current_range": current_range,
                    "suggested_bounds": self._suggest_ratio_bounds(col, current_range),
                    "reasoning": f"åŸºäº{col}çš„å½“å‰å–å€¼èŒƒå›´å’ŒåŒ–å­¦å¸¸è¯†",
                    "constraints": self._suggest_constraints(col)
                }
            elif 'temperature' in col.lower():
                # å®‰å…¨çš„æ•°å€¼è½¬æ¢
                numeric_data = pd.to_numeric(data[col], errors='coerce').dropna()
                if len(numeric_data) == 0:
                    continue
                current_range = [float(numeric_data.min()), float(numeric_data.max())]
                suggestions[col] = {
                    "current_range": current_range,
                    "suggested_bounds": self._suggest_temperature_bounds(current_range),
                    "reasoning": "åŸºäºååº”ç±»å‹å’Œå®‰å…¨è€ƒè™‘",
                }
        
        return suggestions
    
    def _analyze_molecules(self, data: pd.DataFrame) -> dict:
        """
        åˆ†æåˆ†å­ç±»å‹å’Œç‰¹æ€§
        """
        analysis = {}
        
        smiles_columns = [col for col in data.columns if 'SMILE' in col.upper()]
        for col in smiles_columns:
            smiles_list = data[col].dropna().astype(str).tolist()
            analysis[col] = {
                "molecule_count": len(set(smiles_list)),
                "avg_length": np.mean([len(s) for s in smiles_list]),
                "contains_aromatic": any('c' in s.lower() or 'C' in s for s in smiles_list),
                "molecular_diversity": len(set(smiles_list)) / len(smiles_list) if smiles_list else 0
            }
        
        return analysis
    
    def _suggest_ratio_bounds(self, column_name: str, current_range: list) -> tuple:
        """
        å»ºè®®æ¯”ä¾‹å‚æ•°çš„è¾¹ç•Œ
        """
        min_val, max_val = current_range
        
        # åŸºäºåŒ–å­¦å¸¸è¯†çš„å»ºè®®
        if 'catalyst' in column_name.lower():
            # å‚¬åŒ–å‰‚é€šå¸¸æ˜¯å°‘é‡çš„
            return (0.001, 0.1)
        elif 'solvent' in column_name.lower():
            # æº¶å‰‚å¯ä»¥æ˜¯ä¸»è¦æˆåˆ†
            return (0.0, 0.5)
        else:
            # ä¸€èˆ¬ç‰©è´¨çš„åˆç†èŒƒå›´
            buffer = (max_val - min_val) * 0.2
            return (max(0.0, min_val - buffer), min(1.0, max_val + buffer))
    
    def _suggest_temperature_bounds(self, current_range: list) -> tuple:
        """
        å»ºè®®æ¸©åº¦å‚æ•°çš„è¾¹ç•Œ
        """
        min_temp, max_temp = current_range
        
        # åŸºäºå®‰å…¨å’Œå®ç”¨æ€§çš„å»ºè®®
        safety_buffer = 20  # å®‰å…¨ç¼“å†²åŒº
        return (max(20, min_temp - safety_buffer), min(200, max_temp + safety_buffer))
    
    def _suggest_constraints(self, column_name: str) -> list:
        """
        å»ºè®®çº¦æŸæ¡ä»¶
        """
        constraints = []
        
        if 'ratio' in column_name.lower():
            constraints.append({
                "type": "sum_constraint",
                "description": "æ‰€æœ‰æ¯”ä¾‹ä¹‹å’Œåº”ç­‰äº1.0",
                "implementation": "DiscreteSumConstraint"
            })
        
        if 'temperature' in column_name.lower():
            constraints.append({
                "type": "safety_constraint", 
                "description": "æ¸©åº¦åº”åœ¨å®‰å…¨æ“ä½œèŒƒå›´å†…",
                "range": (20, 200)
            })
        
        return constraints


class UserDefinedEncodingHandler:
    """
    è¯†åˆ«å’Œå¤„ç†ç”¨æˆ·åœ¨CSVä¸­æä¾›çš„ç‰¹æ®Šç¼–ç ä¿¡æ¯
    æ”¯æŒåŠ¨æ€è¯†åˆ«å’Œæ ‡å‡†æ ¼å¼å¼•å¯¼çš„æ··åˆç­–ç•¥
    """
    
    def __init__(self):
        # å®šä¹‰åˆ—ç±»å‹è¯†åˆ«è§„åˆ™
        self.column_type_patterns = {
            "ç‰©ç†æ€§è´¨": {
                "keywords": ["density", "viscosity", "refractive", "melting", "boiling", "tg", "å¯†åº¦", "ç²˜åº¦", "æŠ˜å°„", "ç†”ç‚¹", "æ²¸ç‚¹", "ç»ç’ƒåŒ–"],
                "value_type": "numerical",
                "baybe_param_type": "NumericalContinuousParameter"
            },
            "åŠŸèƒ½åˆ†ç±»": {
                "keywords": ["catalyst", "additive", "modifier", "type", "category", "function", "å‚¬åŒ–å‰‚", "æ·»åŠ å‰‚", "æ”¹æ€§å‰‚", "ç±»å‹", "åŠŸèƒ½"],
                "value_type": "categorical", 
                "baybe_param_type": "CategoricalParameter"
            },
            "ä¾›åº”å•†ä¿¡æ¯": {
                "keywords": ["supplier", "vendor", "batch", "lot", "grade", "purity", "ä¾›åº”å•†", "æ‰¹æ¬¡", "ç­‰çº§", "çº¯åº¦"],
                "value_type": "categorical",
                "baybe_param_type": "CategoricalParameter" 
            },
            "æˆæœ¬ä¿¡æ¯": {
                "keywords": ["cost", "price", "availability", "expensive", "cheap", "æˆæœ¬", "ä»·æ ¼", "å¯è·å¾—æ€§"],
                "value_type": "numerical",
                "baybe_param_type": "NumericalContinuousParameter"
            },
            "å·¥è‰ºå‚æ•°": {
                "keywords": ["temperature", "time", "pressure", "speed", "rpm", "æ¸©åº¦", "æ—¶é—´", "å‹åŠ›", "è½¬é€Ÿ"],
                "value_type": "numerical", 
                "baybe_param_type": "NumericalContinuousParameter"
            },
            "é…æ–¹ç‰¹æ€§": {
                "keywords": ["hardener", "crosslinker", "solvent", "diluent", "å›ºåŒ–å‰‚", "äº¤è”å‰‚", "æº¶å‰‚", "ç¨€é‡Šå‰‚"],
                "value_type": "categorical",
                "baybe_param_type": "CategoricalParameter"
            }
        }
    
    def identify_user_special_substances(self, df: pd.DataFrame) -> dict:
        """
        è¯†åˆ«ç”¨æˆ·å®šä¹‰çš„ç‰¹æ®Šç‰©è´¨ï¼ˆSMILESä¸ºç©ºä½†æœ‰åç§°çš„ç‰©è´¨ï¼‰
        """
        user_special_substances = {
            "substances_without_smiles": [],
            "potential_encoding_columns": [],
            "custom_descriptors": {}
        }
        
        # æ‰¾åˆ°æ‰€æœ‰ç‰©è´¨åˆ—å¯¹
        substance_pairs = []
        for col in df.columns:
            if 'name' in col.lower() and 'substance' in col.lower():
                substance_name = col
                # å¯»æ‰¾å¯¹åº”çš„SMILESåˆ—
                substance_prefix = col.replace('_name', '').replace('name', '')
                smiles_col = None
                for scol in df.columns:
                    if substance_prefix in scol and 'SMILE' in scol.upper():
                        smiles_col = scol
                        break
                
                if smiles_col:
                    substance_pairs.append((substance_name, smiles_col))
        
        # è¯†åˆ«ç‰¹æ®Šç‰©è´¨ï¼ˆæœ‰åç§°ä½†SMILESä¸ºç©º/æ— æ•ˆï¼‰
        for name_col, smiles_col in substance_pairs:
            for idx, row in df.iterrows():
                substance_name = row[name_col]
                smiles_value = row[smiles_col]
                
                # å¦‚æœæœ‰ç‰©è´¨åç§°ä½†SMILESä¸ºç©ºæˆ–æ— æ•ˆ
                if (pd.notna(substance_name) and substance_name.strip() != "" and 
                    (pd.isna(smiles_value) or smiles_value == "" or str(smiles_value).strip() == "")):
                    
                    user_special_substances["substances_without_smiles"].append({
                        "name": substance_name,
                        "column_prefix": name_col.replace('_name', '').replace('name', ''),
                        "row": idx + 1
                    })
        
        # å¯»æ‰¾å¯èƒ½çš„è‡ªå®šä¹‰ç¼–ç åˆ—
        for col in df.columns:
            # å¯»æ‰¾åŒ…å«ç‰¹å¾æè¿°çš„åˆ—ï¼ˆä¸æ˜¯standardçš„name/SMILES/ratioåˆ—ï¼‰
            if not any(keyword in col.lower() for keyword in ['name', 'smile', 'ratio', 'target', 'unnamed']):
                if df[col].notna().any():  # å¦‚æœåˆ—æœ‰æ•°æ®
                    user_special_substances["potential_encoding_columns"].append(col)
                    # æ”¶é›†è¯¥åˆ—çš„å”¯ä¸€å€¼ä½œä¸ºå¯èƒ½çš„ç¼–ç 
                    unique_values = df[col].dropna().unique()
                    user_special_substances["custom_descriptors"][col] = unique_values.tolist()
        
        return user_special_substances
    
    def classify_user_columns(self, df: pd.DataFrame) -> dict:
        """
        æ™ºèƒ½åˆ†ç±»ç”¨æˆ·çš„æ‰€æœ‰åˆ—ï¼Œè¯†åˆ«æ½œåœ¨çš„ç¼–ç ä¿¡æ¯
        """
        column_classification = {
            "æ ‡å‡†åˆ—": {"name": [], "smiles": [], "ratio": [], "target": []},
            "è¯†åˆ«çš„æ‰©å±•åˆ—": {},
            "æœªåˆ†ç±»åˆ—": [],
            "å»ºè®®çš„æ ‡å‡†æ ¼å¼": {}
        }
        
        for col in df.columns:
            col_lower = col.lower()
            classified = False
            
            # 1. è¯†åˆ«æ ‡å‡†åˆ—
            if any(keyword in col_lower for keyword in ['name', 'åç§°']):
                column_classification["æ ‡å‡†åˆ—"]["name"].append(col)
                classified = True
            elif any(keyword in col_lower for keyword in ['smile', 'smiles']):
                column_classification["æ ‡å‡†åˆ—"]["smiles"].append(col)
                classified = True
            elif any(keyword in col_lower for keyword in ['ratio', 'æ¯”ä¾‹']):
                column_classification["æ ‡å‡†åˆ—"]["ratio"].append(col)
                classified = True
            elif any(keyword in col_lower for keyword in ['target', 'ç›®æ ‡']):
                column_classification["æ ‡å‡†åˆ—"]["target"].append(col)
                classified = True
            
            # 2. åŠ¨æ€è¯†åˆ«æ‰©å±•åˆ—ç±»å‹
            if not classified:
                for category, pattern_info in self.column_type_patterns.items():
                    if any(keyword in col_lower for keyword in pattern_info["keywords"]):
                        if category not in column_classification["è¯†åˆ«çš„æ‰©å±•åˆ—"]:
                            column_classification["è¯†åˆ«çš„æ‰©å±•åˆ—"][category] = []
                        
                        # åˆ†æåˆ—çš„å®é™…æ•°æ®ç±»å‹
                        sample_data = df[col].dropna().head(10)
                        if len(sample_data) > 0:
                            data_analysis = self._analyze_column_content(sample_data)
                            
                            column_classification["è¯†åˆ«çš„æ‰©å±•åˆ—"][category].append({
                                "column_name": col,
                                "predicted_type": pattern_info["value_type"],
                                "actual_data_type": data_analysis["inferred_type"],
                                "sample_values": data_analysis["sample_values"],
                                "baybe_param_type": pattern_info["baybe_param_type"],
                                "confidence": data_analysis["confidence"]
                            })
                        classified = True
                        break
            
            # 3. æœªèƒ½åˆ†ç±»çš„åˆ—
            if not classified and col.strip() != "" and "unnamed" not in col_lower:
                column_classification["æœªåˆ†ç±»åˆ—"].append(col)
        
        # 4. ç”Ÿæˆæ ‡å‡†æ ¼å¼å»ºè®®
        column_classification["å»ºè®®çš„æ ‡å‡†æ ¼å¼"] = self._generate_standard_format_suggestions(df)
        
        return column_classification
    
    def _analyze_column_content(self, sample_data: pd.Series) -> dict:
        """
        åˆ†æåˆ—å†…å®¹ï¼Œæ¨æ–­æ•°æ®ç±»å‹å’Œç½®ä¿¡åº¦
        """
        analysis = {
            "inferred_type": "unknown",
            "sample_values": sample_data.tolist()[:5],  # å‰5ä¸ªæ ·æœ¬
            "confidence": 0.0
        }
        
        # å°è¯•æ•°å€¼è½¬æ¢
        numeric_conversion = pd.to_numeric(sample_data, errors='coerce')
        numeric_ratio = numeric_conversion.notna().sum() / len(sample_data)
        
        if numeric_ratio >= 0.8:  # 80%ä»¥ä¸Šå¯è½¬æ¢ä¸ºæ•°å€¼
            analysis["inferred_type"] = "numerical"
            analysis["confidence"] = numeric_ratio
        elif len(sample_data.unique()) <= max(10, len(sample_data) * 0.5):  # å”¯ä¸€å€¼è¾ƒå°‘
            analysis["inferred_type"] = "categorical"
            analysis["confidence"] = 1.0 - (len(sample_data.unique()) / len(sample_data))
        else:
            analysis["inferred_type"] = "text"
            analysis["confidence"] = 0.5
        
        return analysis
    
    def _generate_standard_format_suggestions(self, df: pd.DataFrame) -> dict:
        """
        åŸºäºå½“å‰æ•°æ®ç”Ÿæˆæ ‡å‡†æ ¼å¼å»ºè®®
        """
        suggestions = {
            "æ¨èçš„åˆ—å‘½åè§„èŒƒ": {
                "ç‰©è´¨ä¿¡æ¯": [
                    "SubstanceA_name (ç‰©è´¨åç§°)",
                    "SubstanceA_SMILES (åˆ†å­ç»“æ„)", 
                    "SubstanceA_ratio (æ¯”ä¾‹)",
                    "SubstanceA_type (ç‰©è´¨ç±»å‹: resin/hardener/catalyst/solvent/additive)",
                    "SubstanceA_supplier (ä¾›åº”å•†)",
                    "SubstanceA_grade (ç­‰çº§/çº¯åº¦)",
                    "SubstanceA_batch (æ‰¹æ¬¡å·)"
                ],
                "ç‰©ç†æ€§è´¨": [
                    "SubstanceA_density (å¯†åº¦ g/cmÂ³)",
                    "SubstanceA_viscosity (ç²˜åº¦ PaÂ·s)",
                    "SubstanceA_tg (ç»ç’ƒåŒ–æ¸©åº¦ Â°C)",
                    "SubstanceA_melting_point (ç†”ç‚¹ Â°C)"
                ],
                "å·¥è‰ºå‚æ•°": [
                    "Process_temperature (ååº”æ¸©åº¦ Â°C)",
                    "Process_time (ååº”æ—¶é—´ min)",
                    "Process_pressure (å‹åŠ› bar)",
                    "Curing_temperature (å›ºåŒ–æ¸©åº¦ Â°C)"
                ],
                "æˆæœ¬ä¿¡æ¯": [
                    "SubstanceA_cost_per_kg (æˆæœ¬ å…ƒ/kg)",
                    "SubstanceA_availability (å¯è·å¾—æ€§: high/medium/low)"
                ]
            },
            "å½“å‰æ•°æ®æ˜ å°„å»ºè®®": {}
        }
        
        # åŸºäºå½“å‰æ•°æ®æä¾›å…·ä½“çš„é‡å‘½åå»ºè®®
        for col in df.columns:
            if "unnamed" in col.lower():
                continue
                
            col_lower = col.lower()
            mapping_suggestion = None
            
            # å°è¯•æ˜ å°„åˆ°æ ‡å‡†æ ¼å¼
            if any(keyword in col_lower for keyword in ['ç¨€é‡Š', 'diluent', 'solvent']):
                mapping_suggestion = f"{col} â†’ SubstanceX_type (å€¼: solvent/diluent)"
            elif any(keyword in col_lower for keyword in ['å‚¬åŒ–', 'catalyst']):
                mapping_suggestion = f"{col} â†’ SubstanceX_type (å€¼: catalyst)"
            elif any(keyword in col_lower for keyword in ['å¯†åº¦', 'density']):
                mapping_suggestion = f"{col} â†’ SubstanceX_density"
            elif any(keyword in col_lower for keyword in ['ç²˜åº¦', 'viscosity']):
                mapping_suggestion = f"{col} â†’ SubstanceX_viscosity"
                
            if mapping_suggestion:
                suggestions["å½“å‰æ•°æ®æ˜ å°„å»ºè®®"][col] = mapping_suggestion
        
        return suggestions
    
    def create_baybe_parameters_for_special_substances(self, user_special_data: dict, df: pd.DataFrame) -> list:
        """
        ä¸ºç”¨æˆ·å®šä¹‰çš„ç‰¹æ®Šç‰©è´¨åˆ›å»ºBayBEå‚æ•°
        """
        parameters = []
        
        # å¤„ç†æ²¡æœ‰SMILESçš„ç‰¹æ®Šç‰©è´¨
        for special_substance in user_special_data["substances_without_smiles"]:
            substance_name = special_substance["name"]
            column_prefix = special_substance["column_prefix"]
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å¯¹åº”çš„æ¯”ä¾‹åˆ—
            ratio_col = f"{column_prefix}_ratio"
            if ratio_col in df.columns:
                # å¯¹äºç‰¹æ®Šç‰©è´¨ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨åç§°ä½œä¸ºåˆ†ç±»å‚æ•°
                # æˆ–è€…æ ¹æ®æ¯”ä¾‹åˆ›å»ºæ•°å€¼å‚æ•°
                unique_names = df[f"{column_prefix}_name"].dropna().unique()
                if len(unique_names) > 1:
                    # å¦‚æœæœ‰å¤šä¸ªä¸åŒçš„ç‰¹æ®Šç‰©è´¨åç§°ï¼Œåˆ›å»ºåˆ†ç±»å‚æ•°
                    from baybe.parameters import CategoricalParameter
                    param = CategoricalParameter(
                        name=f"{column_prefix}_special_substance",
                        values=[str(name) for name in unique_names],
                        encoding="OHE"  # One-Hot Encoding
                    )
                    parameters.append({
                        "parameter": param,
                        "source": "user_defined_special_substance",
                        "substance_type": "special_without_smiles",
                        "original_column": f"{column_prefix}_name"
                    })
        
        # å¤„ç†è‡ªå®šä¹‰æè¿°ç¬¦åˆ—
        for col, values in user_special_data["custom_descriptors"].items():
            if len(values) > 1:  # åªæœ‰å½“æœ‰å¤šä¸ªä¸åŒå€¼æ—¶æ‰åˆ›å»ºå‚æ•°
                # åˆ¤æ–­æ˜¯æ•°å€¼è¿˜æ˜¯åˆ†ç±»æ•°æ®
                numeric_values = pd.to_numeric(pd.Series(values), errors='coerce').dropna()
                
                if len(numeric_values) == len(values):  # å…¨æ˜¯æ•°å€¼
                    from baybe.parameters import NumericalContinuousParameter
                    param = NumericalContinuousParameter(
                        name=f"custom_{col}",
                        bounds=(float(min(numeric_values)), float(max(numeric_values)))
                    )
                else:  # åˆ†ç±»æ•°æ®
                    from baybe.parameters import CategoricalParameter
                    param = CategoricalParameter(
                        name=f"custom_{col}",
                        values=[str(v) for v in values],
                        encoding="OHE"
                    )
                
                parameters.append({
                    "parameter": param,
                    "source": "user_defined_descriptor",
                    "original_column": col
                })
        
        return parameters
    
    def generate_standard_csv_template(self, num_substances: int = 4) -> str:
        """
        ç”ŸæˆåŒ…å«æ‰©å±•åˆ—ç±»å‹çš„æ ‡å‡†CSVæ¨¡æ¿
        """
        headers = []
        
        # ä¸ºæ¯ä¸ªç‰©è´¨ç”Ÿæˆå®Œæ•´çš„åˆ—é›†åˆ
        for i in range(num_substances):
            substance = chr(65 + i)  # A, B, C, D...
            headers.extend([
                f"Substance{substance}_name",
                f"Substance{substance}_SMILES", 
                f"Substance{substance}_ratio",
                f"Substance{substance}_type",           # åŠŸèƒ½åˆ†ç±»
                f"Substance{substance}_supplier",       # ä¾›åº”å•†ä¿¡æ¯
                f"Substance{substance}_grade",          # ç­‰çº§/çº¯åº¦
                f"Substance{substance}_density",        # ç‰©ç†æ€§è´¨
                f"Substance{substance}_viscosity",      # ç‰©ç†æ€§è´¨
                f"Substance{substance}_cost_per_kg",    # æˆæœ¬ä¿¡æ¯
                f"Substance{substance}_availability",   # å¯è·å¾—æ€§
            ])
        
        # æ·»åŠ å·¥è‰ºå‚æ•°
        headers.extend([
            "Process_temperature",      # å·¥è‰ºå‚æ•°
            "Process_time", 
            "Process_pressure",
            "Curing_temperature",
            "Mixing_speed"
        ])
        
        # æ·»åŠ ç›®æ ‡å˜é‡
        headers.extend([
            "Target_mechanical_strength",
            "Target_thermal_stability", 
            "Target_chemical_resistance",
            "Target_cost_effectiveness"
        ])
        
        # ç”Ÿæˆç¤ºä¾‹æ•°æ®è¡Œ
        example_rows = []
        example_rows.append([
            # SubstanceA (ä¸»æ ‘è„‚)
            "Epoxy_Resin_E51", "CC(C)(C1=CC=C(C=C1)OCC2CO2)C3=CC=C(C=C3)OCC4CO4", "0.6", "epoxy_resin", "Supplier_A", "Industrial_Grade", "1.15", "800", "25.5", "high",
            # SubstanceB (å›ºåŒ–å‰‚)  
            "Hardener_DETA", "NCCNCCN", "0.3", "hardener", "Supplier_B", "Analytical_Grade", "0.95", "20", "18.2", "medium",
            # SubstanceC (ç¨€é‡Šå‰‚)
            "Diluent_A", "", "0.1", "diluent", "Supplier_C", "Industrial_Grade", "0.85", "5", "12.0", "high",
            # SubstanceD (æ·»åŠ å‰‚)
            "Antioxidant_BHT", "CC(C)(C)C1=CC(=C(C(=C1)C(C)(C)C)O)C(C)(C)C", "0.0", "antioxidant", "Supplier_D", "Analytical_Grade", "1.05", "1000", "45.8", "low",
            # å·¥è‰ºå‚æ•°
            "80", "120", "1.0", "150", "500",
            # ç›®æ ‡
            "85", "200", "95", "0.8"
        ])
        
        # æ·»åŠ ç¬¬äºŒè¡Œç¤ºä¾‹æ•°æ®
        example_rows.append([
            # SubstanceA
            "Epoxy_Resin_E44", "CC(C)(C1=CC=C(C=C1)OCC2CO2)C3=CC=C(C=C3)OCC4CO4", "0.7", "epoxy_resin", "Supplier_A", "Industrial_Grade", "1.18", "1200", "28.0", "high",
            # SubstanceB  
            "Hardener_IPDA", "C1CCC(CC1)N", "0.25", "hardener", "Supplier_B", "Analytical_Grade", "0.92", "15", "20.5", "medium",
            # SubstanceC
            "Diluent_B", "", "0.05", "diluent", "Supplier_E", "Industrial_Grade", "0.88", "8", "15.0", "medium",
            # SubstanceD
            "UV_Stabilizer", "CC(C)(C)C1=CC(=C(C(=C1)C(C)(C)C)OCC(=O)OC)C(C)(C)C", "0.0", "uv_stabilizer", "Supplier_F", "Analytical_Grade", "1.02", "2000", "52.3", "low",
            # å·¥è‰ºå‚æ•°
            "90", "90", "1.2", "160", "400",
            # ç›®æ ‡  
            "92", "220", "88", "0.75"
        ])
        
        # æ„å»ºCSVå†…å®¹ï¼ˆä½¿ç”¨è‹±æ–‡é¿å…ç¼–ç é—®é¢˜ï¼‰
        csv_content = ",".join(headers) + "\n"
        for row in example_rows:
            csv_content += ",".join(map(str, row)) + "\n"
        
        return csv_content


def diagnose_data_types(file_path: str) -> str:
    """
    è¯Šæ–­CSVæ•°æ®ä¸­çš„ç±»å‹é—®é¢˜ï¼Œå¸®åŠ©ç”¨æˆ·æ‰¾åˆ°å¯¼è‡´ç±»å‹é”™è¯¯çš„å…·ä½“æ•°æ®
    """
    try:
        df = pd.read_csv(file_path)
        
        diagnosis_report = {
            "problematic_columns": [],
            "mixed_type_cells": [],
            "non_numeric_in_numeric_columns": []
        }
        
        print(f"ğŸ” æ­£åœ¨è¯Šæ–­æ–‡ä»¶: {file_path}")
        print(f"ğŸ“Š æ•°æ®å½¢çŠ¶: {df.shape}")
        
        for col in df.columns:
            print(f"\nğŸ“‹ æ£€æŸ¥åˆ—: {col}")
            
            # æ£€æŸ¥è¯¥åˆ—æ˜¯å¦åº”è¯¥æ˜¯æ•°å€¼åˆ—
            is_expected_numeric = any(keyword in col.lower() for keyword in 
                                    ['ratio', 'temperature', 'target', 'temp', 'conc', 'concentration'])
            
            if is_expected_numeric:
                # å°è¯•è½¬æ¢ä¸ºæ•°å€¼
                numeric_conversion = pd.to_numeric(df[col], errors='coerce')
                failed_indices = df[numeric_conversion.isna() & df[col].notna()].index.tolist()
                
                if failed_indices:
                    diagnosis_report["problematic_columns"].append(col)
                    problematic_values = []
                    
                    for idx in failed_indices[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªé—®é¢˜å€¼
                        problematic_values.append({
                            "row": idx + 1,  # Excelè¡Œå·ä»1å¼€å§‹
                            "value": repr(df.iloc[idx][col]),
                            "type": type(df.iloc[idx][col]).__name__
                        })
                    
                    diagnosis_report["non_numeric_in_numeric_columns"].append({
                        "column": col,
                        "problematic_count": len(failed_indices),
                        "total_count": len(df),
                        "examples": problematic_values
                    })
                    
                    print(f"âŒ å‘ç° {len(failed_indices)} ä¸ªéæ•°å€¼æ¡ç›®åœ¨æ•°å€¼åˆ— '{col}' ä¸­")
                    for example in problematic_values:
                        print(f"   è¡Œ {example['row']}: {example['value']} (ç±»å‹: {example['type']})")
            
            # æ£€æŸ¥æ··åˆç±»å‹
            unique_types = df[col].dropna().apply(type).unique()
            if len(unique_types) > 1:
                diagnosis_report["mixed_type_cells"].append({
                    "column": col,
                    "types_found": [t.__name__ for t in unique_types]
                })
                print(f"âš ï¸ åˆ— '{col}' åŒ…å«æ··åˆæ•°æ®ç±»å‹: {[t.__name__ for t in unique_types]}")
        
        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        if diagnosis_report["problematic_columns"]:
            return f"""
ğŸš¨ **æ•°æ®ç±»å‹è¯Šæ–­ç»“æœ**

âŒ **å‘ç°é—®é¢˜åˆ—**: {len(diagnosis_report["problematic_columns"])} ä¸ª
{chr(10).join([f"   - {col}" for col in diagnosis_report["problematic_columns"]])}

ğŸ“‹ **è¯¦ç»†é—®é¢˜**:
{chr(10).join([f"â€¢ åˆ— '{item['column']}': {item['problematic_count']}/{item['total_count']} ä¸ªéæ•°å€¼æ¡ç›®" 
              for item in diagnosis_report["non_numeric_in_numeric_columns"]])}

ğŸ’¡ **ä¿®å¤å»ºè®®**:
1. æ£€æŸ¥CSVæ–‡ä»¶ä¸­ä¸Šè¿°è¡Œçš„æ•°æ®
2. ç¡®ä¿æ¯”ä¾‹ã€æ¸©åº¦ã€ç›®æ ‡å€¼åˆ—åªåŒ…å«æ•°å­—
3. ç§»é™¤æˆ–ä¿®æ­£éæ•°å€¼æ¡ç›®ï¼ˆå¦‚æ–‡æœ¬ã€ç©ºæ ¼ã€ç‰¹æ®Šå­—ç¬¦ï¼‰
4. ä½¿ç”¨Excelæˆ–æ–‡æœ¬ç¼–è¾‘å™¨æŸ¥çœ‹åŸå§‹CSVæ–‡ä»¶

ğŸ”§ **å…·ä½“æ£€æŸ¥ä½ç½®**:
{chr(10).join([f"åˆ— '{item['column']}':" + chr(10) + chr(10).join([f"   è¡Œ {ex['row']}: {ex['value']}" for ex in item['examples']]) 
              for item in diagnosis_report["non_numeric_in_numeric_columns"]])}
            """
        else:
            return "âœ… æ•°æ®ç±»å‹æ£€æŸ¥é€šè¿‡ï¼Œæ²¡æœ‰å‘ç°æ˜æ˜¾çš„ç±»å‹é—®é¢˜ã€‚"
            
    except Exception as e:
        return f"è¯Šæ–­è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}"


def enhanced_verification(file_path: str, tool_context: ToolContext) -> str:
    """
    Enhanced Verification Agent çš„ä¸»è¦å·¥å…·å‡½æ•°
    å®ç°7ä¸ªæ ¸å¿ƒä»»åŠ¡ï¼š
    1. æ•°æ®è´¨é‡éªŒè¯
    2. SMILESéªŒè¯  
    3. æ™ºèƒ½å‚æ•°å»ºè®®
    4. è‡ªå®šä¹‰ç¼–ç å¤„ç†
    5. ç”¨æˆ·äº¤äº’
    6. å‚æ•°é…ç½®
    7. æ•°æ®æ ‡å‡†åŒ–
    """
    state = tool_context.state
    session_id = state.get("session_id", "unknown")
    
    try:
        # ===== ä»»åŠ¡1: æ•°æ®è´¨é‡éªŒè¯ =====
        quality_report = _perform_data_quality_check(file_path)
        
        if not quality_report["is_valid"]:
            return f"æ•°æ®è´¨é‡æ£€æŸ¥å¤±è´¥ï¼š\n{json.dumps(quality_report, indent=2, ensure_ascii=False)}"
        
        # ===== ä»»åŠ¡2: SMILESéªŒè¯ =====
        df = pd.read_csv(file_path)
        smiles_validator = SimplifiedSMILESValidator()
        smiles_validation = smiles_validator.validate_smiles_data(df)
        
        # ===== ä»»åŠ¡3: æ™ºèƒ½å‚æ•°å»ºè®® =====
        parameter_advisor = IntelligentParameterAdvisor()
        parameter_suggestions = parameter_advisor.analyze_experimental_context(df)
        
        # ===== ä»»åŠ¡4: ç”¨æˆ·å®šä¹‰ç¼–ç è¯†åˆ« =====
        encoding_handler = UserDefinedEncodingHandler()
        
        # æ™ºèƒ½åˆ†ç±»æ‰€æœ‰ç”¨æˆ·åˆ—
        column_classification = encoding_handler.classify_user_columns(df)
        
        # è¯†åˆ«ç”¨æˆ·æä¾›çš„ç‰¹æ®Šç‰©è´¨å’Œç¼–ç ä¿¡æ¯
        user_special_data = encoding_handler.identify_user_special_substances(df)
        
        # ä¸ºç‰¹æ®Šç‰©è´¨åˆ›å»ºBayBEå‚æ•°
        special_parameters = encoding_handler.create_baybe_parameters_for_special_substances(user_special_data, df)
        
        # æ•´ç†ç¼–ç ä¿¡æ¯ç”¨äºåç»­å¤„ç†
        custom_encodings = {
            "column_classification": column_classification,
            "user_special_substances": user_special_data,
            "baybe_parameters": special_parameters,
            "encoding_strategy": "user_defined"  # æ ‡æ˜è¿™æ˜¯ç”¨æˆ·å®šä¹‰çš„ç¼–ç 
        }
        
        # ===== ä»»åŠ¡5 & 6: ç”¨æˆ·äº¤äº’å’Œå‚æ•°é…ç½®å‡†å¤‡ =====
        # å‡†å¤‡ç”¨æˆ·äº¤äº’æ‰€éœ€çš„ä¿¡æ¯
        user_interaction_data = _prepare_user_interaction_data(
            df, quality_report, smiles_validation, parameter_suggestions, custom_encodings
        )
        
        # ===== ä»»åŠ¡7: æ•°æ®æ ‡å‡†åŒ– =====
        standardized_data = _standardize_data(df, smiles_validation)
        
        # ä¿å­˜çŠ¶æ€ä¿¡æ¯
        state["verification_results"] = {
            "quality_report": quality_report,
            "smiles_validation": smiles_validation,
            "parameter_suggestions": parameter_suggestions,
            "custom_encodings": custom_encodings,
            "standardized_data_path": f"standardized_data_{session_id}.csv",
            "ready_for_user_interaction": True
        }
        
        # ä¿å­˜æ ‡å‡†åŒ–æ•°æ®
        output_path = f"standardized_data_{session_id}.csv"
        standardized_data.to_csv(output_path, index=False)
        
        # ç”Ÿæˆç”¨æˆ·äº¤äº’æç¤º
        return _generate_user_interaction_prompt(user_interaction_data)
        
    except Exception as e:
        return f"Enhanced Verification å¤„ç†é”™è¯¯: {str(e)}"


def _perform_data_quality_check(file_path: str) -> dict:
    """
    æ‰§è¡Œæ•°æ®è´¨é‡æ£€æŸ¥ï¼ˆä»»åŠ¡1ï¼‰
    """
    try:
        df = pd.read_csv(file_path)
        
        quality_report = {
            "is_valid": True,
            "issues": [],
            "statistics": {
                "total_rows": len(df),
                "total_columns": len(df.columns),
                "missing_percentage": (df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100
            }
        }
        
        # æ£€æŸ¥ç¼ºå¤±å€¼
        missing_cols = df.isnull().sum()
        high_missing_cols = missing_cols[missing_cols > len(df) * 0.5].index.tolist()
        if high_missing_cols:
            quality_report["issues"].append({
                "type": "high_missing_data",
                "columns": high_missing_cols,
                "severity": "warning"
            })
        
        # æ£€æŸ¥å¿…éœ€åˆ—
        required_patterns = ['Substance', 'SMILE', 'Target_']
        for pattern in required_patterns:
            matching_cols = [col for col in df.columns if pattern in str(col)]
            if not matching_cols:
                quality_report["issues"].append({
                    "type": "missing_required_columns",
                    "pattern": pattern,
                    "severity": "error"
                })
                quality_report["is_valid"] = False
        
        # æ£€æŸ¥æ•°å€¼åˆ—çš„å¼‚å¸¸å€¼
        for col in df.columns:
            # å°è¯•å°†åˆ—è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
            numeric_data = pd.to_numeric(df[col], errors='coerce')
            # åªå¤„ç†è‡³å°‘æœ‰ä¸€äº›æœ‰æ•ˆæ•°å€¼çš„åˆ—
            if numeric_data.notna().sum() < len(df) * 0.1:  # å¦‚æœæœ‰æ•ˆæ•°å€¼å°‘äº10%ï¼Œè·³è¿‡
                continue
                
            # ä½¿ç”¨æ¸…ç†åçš„æ•°å€¼æ•°æ®è®¡ç®—ç»Ÿè®¡é‡
            clean_data = numeric_data.dropna()
            if len(clean_data) < 2:  # éœ€è¦è‡³å°‘2ä¸ªå€¼æ¥è®¡ç®—IQR
                continue
                
            Q1 = clean_data.quantile(0.25)
            Q3 = clean_data.quantile(0.75)
            IQR = Q3 - Q1
            outliers = clean_data[(clean_data < Q1 - 1.5*IQR) | (clean_data > Q3 + 1.5*IQR)]
            
            if not outliers.empty:
                quality_report["issues"].append({
                    "type": "outliers_detected",
                    "column": col,
                    "count": len(outliers),
                    "severity": "info"
                })
        
        return quality_report
        
    except Exception as e:
        return {
            "is_valid": False,
            "error": str(e),
            "issues": [{"type": "file_read_error", "severity": "error"}]
        }


def _prepare_user_interaction_data(df, quality_report, smiles_validation, parameter_suggestions, custom_encodings):
    """
    å‡†å¤‡ç”¨æˆ·äº¤äº’æ‰€éœ€çš„æ•°æ®ï¼ˆä»»åŠ¡5æ”¯æŒï¼‰
    """
    # è¯†åˆ«ç›®æ ‡å˜é‡
    target_columns = [col for col in df.columns if col.startswith('Target_')]
    
    # è¯†åˆ«å¯è°ƒå˜é‡
    adjustable_vars = []
    ratio_cols = [col for col in df.columns if 'ratio' in col.lower()]
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    adjustable_vars = list(set(ratio_cols + numeric_cols) - set(target_columns))
    
    interaction_data = {
        "data_summary": {
            "total_experiments": len(df),
            "substances_count": len([col for col in df.columns if 'Substance' in col and 'name' in col]),
            "targets_count": len(target_columns),
            "adjustable_variables_count": len(adjustable_vars)
        },
        "target_variables": target_columns,
        "adjustable_variables": adjustable_vars,
        "parameter_suggestions": parameter_suggestions,
        "smiles_status": {
            "valid_smiles": len(smiles_validation["canonical_smiles_mapping"]),
            "invalid_smiles": len(smiles_validation["invalid_smiles"]),
            "substances_validated": smiles_validation["substances_validated"]
        },
        "special_molecules": custom_encodings,
        "quality_score": 100 - quality_report["statistics"]["missing_percentage"]
    }
    
    return interaction_data


def _standardize_data(df: pd.DataFrame, smiles_validation: dict) -> pd.DataFrame:
    """
    æ•°æ®æ ‡å‡†åŒ–å¤„ç†ï¼ˆä»»åŠ¡7ï¼‰
    """
    standardized_df = df.copy()
    
    # 1. æ›¿æ¢ä¸ºè§„èŒƒåŒ–SMILES
    smiles_columns = [col for col in df.columns if 'SMILE' in col.upper()]
    for col in smiles_columns:
        for original_smiles, canonical_smiles in smiles_validation["canonical_smiles_mapping"].items():
            standardized_df[col] = standardized_df[col].replace(original_smiles, canonical_smiles)
    
    # 2. å®‰å…¨çš„æ•°æ®ç±»å‹æ ‡å‡†åŒ–å’Œç¼ºå¤±å€¼å¤„ç†
    for col in standardized_df.columns:
        if col not in smiles_columns:  # è·³è¿‡SMILESåˆ—
            # å°è¯•è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
            numeric_data = pd.to_numeric(standardized_df[col], errors='coerce')
            valid_numeric_ratio = numeric_data.notna().sum() / len(standardized_df)
            
            # å¦‚æœè‡³å°‘50%çš„æ•°æ®å¯ä»¥è½¬æ¢ä¸ºæ•°å€¼ï¼Œåˆ™è®¤ä¸ºè¿™æ˜¯æ•°å€¼åˆ—
            if valid_numeric_ratio >= 0.5:
                standardized_df[col] = numeric_data
                # ç”¨ä¸­ä½æ•°å¡«å……ç¼ºå¤±å€¼
                median_val = numeric_data.median()
                if not pd.isna(median_val):
                    standardized_df[col] = standardized_df[col].fillna(median_val)
                    
    # 3. ç‰¹å®šåˆ—ç±»å‹å¼ºåˆ¶è½¬æ¢
    for col in standardized_df.columns:
        if 'ratio' in col.lower() or 'temperature' in col.lower() or 'target' in col.lower():
            # å¼ºåˆ¶è½¬æ¢ä¸ºæ•°å€¼ï¼Œæ— æ•ˆå€¼è®¾ä¸ºNaN
            standardized_df[col] = pd.to_numeric(standardized_df[col], errors='coerce')
        elif col.startswith('Target_'):
            standardized_df[col] = pd.to_numeric(standardized_df[col], errors='coerce')
    
    return standardized_df


def _generate_user_interaction_prompt(interaction_data: dict) -> str:
    """
    ç”Ÿæˆç”¨æˆ·äº¤äº’æç¤ºï¼ˆä»»åŠ¡5ï¼‰
    """
    prompt = f"""
ğŸ” **æ•°æ®éªŒè¯å®Œæˆ - éœ€è¦æ‚¨çš„ä¼˜åŒ–ç›®æ ‡ç¡®è®¤**

ğŸ“Š **æ•°æ®æ¦‚è§ˆ**:
- å®éªŒæ•°é‡: {interaction_data['data_summary']['total_experiments']}
- ç‰©è´¨ç§ç±»: {interaction_data['data_summary']['substances_count']}
- ç›®æ ‡å˜é‡: {interaction_data['data_summary']['targets_count']}
- å¯è°ƒå˜é‡: {interaction_data['data_summary']['adjustable_variables_count']}
- æ•°æ®è´¨é‡è¯„åˆ†: {interaction_data['quality_score']:.1f}/100

ğŸ¯ **ç›®æ ‡å˜é‡**: {', '.join(interaction_data['target_variables'])}

ğŸ”§ **å¯è°ƒå˜é‡**: {', '.join(interaction_data['adjustable_variables'])}

ğŸ§ª **SMILESéªŒè¯çŠ¶æ€**:
- æœ‰æ•ˆåˆ†å­: {interaction_data['smiles_status']['valid_smiles']}
- æ— æ•ˆåˆ†å­: {interaction_data['smiles_status']['invalid_smiles']}
- å·²éªŒè¯ç‰©è´¨: {', '.join(interaction_data['smiles_status']['substances_validated'])}

ğŸ’¡ **æ™ºèƒ½å‚æ•°å»ºè®®**:
"""
    
    # æ·»åŠ å‚æ•°å»ºè®®è¯¦æƒ…
    for param, suggestion in interaction_data['parameter_suggestions'].items():
        prompt += f"\nğŸ“Œ **{param}**:"
        prompt += f"\n   - å½“å‰èŒƒå›´: {suggestion['current_range']}"
        prompt += f"\n   - å»ºè®®èŒƒå›´: {suggestion['suggested_bounds']}" 
        prompt += f"\n   - ç†ç”±: {suggestion['reasoning']}"
    
    # æ·»åŠ æ™ºèƒ½åˆ—åˆ†ç±»ç»“æœ
    if interaction_data['special_molecules'].get('column_classification'):
        classification = interaction_data['special_molecules']['column_classification']
        
        prompt += f"\n\nğŸ“‹ **æ•°æ®ç»“æ„åˆ†æ**:"
        
        # æ˜¾ç¤ºè¯†åˆ«çš„æ‰©å±•åˆ—
        if classification['è¯†åˆ«çš„æ‰©å±•åˆ—']:
            prompt += f"\nğŸ¯ **æ™ºèƒ½è¯†åˆ«çš„æ‰©å±•åˆ—ç±»å‹**:"
            for category, columns in classification['è¯†åˆ«çš„æ‰©å±•åˆ—'].items():
                if columns:
                    prompt += f"\n   ğŸ“Œ {category}:"
                    for col_info in columns:
                        confidence_str = f"({col_info['confidence']:.1%}ç½®ä¿¡åº¦)" if col_info['confidence'] > 0 else ""
                        prompt += f"\n      - {col_info['column_name']}: {col_info['actual_data_type']} {confidence_str}"
                        prompt += f"\n        æ ·æœ¬å€¼: {col_info['sample_values'][:3]}"
        
        # æ˜¾ç¤ºç‰¹æ®Šç‰©è´¨
        if interaction_data['special_molecules'].get('user_special_substances', {}).get('substances_without_smiles'):
            user_special = interaction_data['special_molecules']['user_special_substances']
            prompt += f"\n\nğŸ”¬ **è¯†åˆ«åˆ°æ‚¨çš„ç‰¹æ®Šç‰©è´¨**:"
            special_substances_summary = {}
            for substance in user_special['substances_without_smiles']:
                name = substance['name']
                if name not in special_substances_summary:
                    special_substances_summary[name] = []
                special_substances_summary[name].append(substance['row'])
            
            for name, rows in special_substances_summary.items():
                prompt += f"\n   - {name}: å‡ºç°åœ¨ {len(rows)} ä¸ªå®éªŒä¸­ï¼Œæ— SMILESï¼Œå°†ä½¿ç”¨åç§°ç¼–ç "
        
        # æ˜¾ç¤ºæœªåˆ†ç±»åˆ—å»ºè®®
        if classification['æœªåˆ†ç±»åˆ—']:
            prompt += f"\n\nâ“ **æœªåˆ†ç±»çš„åˆ—** (å¯èƒ½éœ€è¦æ‚¨çš„è¯´æ˜):"
            for col in classification['æœªåˆ†ç±»åˆ—']:
                prompt += f"\n   - {col}"
        
        # æ˜¾ç¤ºæ ‡å‡†æ ¼å¼å»ºè®®
        if classification['å»ºè®®çš„æ ‡å‡†æ ¼å¼']['å½“å‰æ•°æ®æ˜ å°„å»ºè®®']:
            prompt += f"\n\nğŸ’¡ **æ•°æ®æ ¼å¼ä¼˜åŒ–å»ºè®®**:"
            for current_col, suggestion in classification['å»ºè®®çš„æ ‡å‡†æ ¼å¼']['å½“å‰æ•°æ®æ˜ å°„å»ºè®®'].items():
                prompt += f"\n   - {suggestion}"
    
    prompt += f"""

â“ **è¯·å›ç­”ä»¥ä¸‹é—®é¢˜ä»¥å®Œæˆä¼˜åŒ–é…ç½®**:

1. **ä¼˜åŒ–ç›®æ ‡ç¡®è®¤**: 
   å¯¹äºæ¯ä¸ªç›®æ ‡å˜é‡ï¼Œè¯·æŒ‡å®šï¼š
   - ä¼˜åŒ–æ–¹å‘ (æœ€å¤§åŒ–/æœ€å°åŒ–/ç›®æ ‡å€¼åŒ¹é…)
   - æœŸæœ›çš„ç›®æ ‡å€¼èŒƒå›´
   - ä¼˜å…ˆçº§æƒé‡

2. **å‚æ•°è¾¹ç•Œç¡®è®¤**:
   æ˜¯å¦æ¥å—ä¸Šè¿°æ™ºèƒ½å»ºè®®çš„å‚æ•°èŒƒå›´ï¼Ÿå¦‚éœ€è°ƒæ•´è¯·è¯´æ˜ã€‚

3. **çº¦æŸæ¡ä»¶**:
   æ˜¯å¦æœ‰ç‰¹æ®Šçš„çº¦æŸæ¡ä»¶ï¼ˆå¦‚æŸäº›ç‰©è´¨ä¸èƒ½åŒæ—¶ä½¿ç”¨ï¼‰ï¼Ÿ

4. **å®éªŒè®¾è®¡å‚æ•°**:
   - è®¡åˆ’çš„å®éªŒæ‰¹æ¬¡å¤§å° (batch_size)
   - æœ€å¤§å®éªŒè½®æ•° (n_doe_iterations)
   - é¢„ç®—çº¦æŸ (æ€»å®éªŒæ•°é‡é™åˆ¶)

è¯·æä¾›æ‚¨çš„å›ç­”ï¼Œæˆ‘å°†æ ¹æ®æ‚¨çš„éœ€æ±‚ç”Ÿæˆä¼˜åŒ–é…ç½®ã€‚
"""
    
    return prompt


# ä¸»è¦çš„å¢å¼ºéªŒè¯å·¥å…·å‡½æ•°
def collect_optimization_goals(user_response: str, tool_context: ToolContext) -> str:
    """
    æ”¶é›†ç”¨æˆ·çš„ä¼˜åŒ–ç›®æ ‡å’Œé…ç½®ï¼ˆä»»åŠ¡5å’Œ6ï¼‰
    """
    state = tool_context.state
    verification_results = state.get("verification_results", {})
    
    try:
        # è§£æç”¨æˆ·å“åº”ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…å¯èƒ½éœ€è¦æ›´å¤æ‚çš„NLPï¼‰
        optimization_config = _parse_user_response(user_response, verification_results)
        
        # ç”ŸæˆBayBEå…¼å®¹çš„é…ç½®
        baybe_config = _generate_baybe_config(optimization_config, verification_results)
        
        # æ›´æ–°çŠ¶æ€
        state["optimization_config"] = optimization_config
        state["baybe_campaign_config"] = baybe_config
        state["verification_status"] = "completed_with_user_input"
        state["ready_for_searchspace_construction"] = True
        
        return f"""
âœ… **ä¼˜åŒ–é…ç½®å·²å®Œæˆ**

ğŸ“‹ **é…ç½®æ‘˜è¦**:
- ç›®æ ‡æ•°é‡: {len(optimization_config.get('targets', []))}
- å‚æ•°æ•°é‡: {len(optimization_config.get('parameters', []))}
- çº¦æŸæ¡ä»¶: {len(optimization_config.get('constraints', []))}
- ç‰¹æ®Šç¼–ç : {len(verification_results.get('custom_encodings', {}))}

ğŸš€ **ä¸‹ä¸€æ­¥**: ç³»ç»Ÿå°†æ„å»ºBayBEæœç´¢ç©ºé—´å¹¶å‡†å¤‡ä¼˜åŒ–Campaignã€‚

ğŸ“„ **BayBEé…ç½®å·²ä¿å­˜åˆ°ä¼šè¯çŠ¶æ€**ï¼Œå¯ä»¥ä¼ é€’ç»™SearchSpace Construction Agentã€‚
        """
        
    except Exception as e:
        return f"è§£æç”¨æˆ·é…ç½®æ—¶å‡ºé”™: {str(e)}\nè¯·é‡æ–°æä¾›é…ç½®ä¿¡æ¯ã€‚"


def _parse_user_response(user_response: str, verification_results: dict) -> dict:
    """
    è§£æç”¨æˆ·å“åº”ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
    """
    # è¿™é‡Œæ˜¯ç®€åŒ–çš„è§£æé€»è¾‘ï¼Œå®é™…åº”ç”¨ä¸­å¯èƒ½éœ€è¦æ›´å¤æ‚çš„NLP
    config = {
        "targets": [],
        "parameters": verification_results.get("parameter_suggestions", {}),
        "constraints": [],
        "experimental_settings": {
            "batch_size": 5,  # é»˜è®¤å€¼
            "max_iterations": 20
        }
    }
    
    # ç®€å•çš„å…³é”®è¯æå–ï¼ˆå®é™…åº”è¯¥ä½¿ç”¨æ›´æ™ºèƒ½çš„è§£æï¼‰
    if "æœ€å¤§åŒ–" in user_response or "maximize" in user_response.lower():
        config["default_optimization"] = "MAX"
    elif "æœ€å°åŒ–" in user_response or "minimize" in user_response.lower():
        config["default_optimization"] = "MIN"
    
    return config


def _generate_baybe_config(optimization_config: dict, verification_results: dict) -> dict:
    """
    ç”ŸæˆBayBEå…¼å®¹çš„é…ç½®æ ¼å¼ï¼ˆä»»åŠ¡6ï¼‰
    """
    if not BAYBE_AVAILABLE:
        return {"error": "BayBE not available"}
    
    # æ ¹æ®å¼€å‘æ–‡æ¡£çš„æ ‡å‡†æ ¼å¼ç”Ÿæˆé…ç½®
    baybe_config = {
        "campaign_info": {
            "name": "chemical_optimization",
            "created_at": datetime.now().isoformat(),
            "description": "ChemBoMAS Enhanced Verification Agent generated configuration"
        },
        "targets": [],
        "parameters": [],
        "constraints": [],
        "objective_config": {
            "type": "DesirabilityObjective",
            "weights": [1.0],  # é»˜è®¤æƒé‡
            "scalarizer": "GEOM_MEAN"
        },
        "experimental_config": {
            "batch_size": optimization_config["experimental_settings"]["batch_size"],
            "recommender": "TwoPhaseMetaRecommender"
        }
    }
    
    return baybe_config


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("ğŸ§ª Enhanced Verification Tools åŠŸèƒ½æµ‹è¯•")
    print("=" * 50)
    
    # æµ‹è¯•1: SMILESéªŒè¯å™¨
    print("\n1. æµ‹è¯•SMILESéªŒè¯å™¨...")
    validator = SimplifiedSMILESValidator()
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_data = pd.DataFrame({
        'SubstanceA_SMILE': ['CCO', 'CCCCO', 'invalid_smiles', ''],
        'SubstanceB_SMILE': ['CC(C)O', 'CCCCCO', 'CCC', 'another_invalid'],
        'SubstanceA_ratio': [0.5, 0.6, 0.7, 0.8],
        'Target_alpha_tg': [80, 85, 90, 95]
    })
    
    validation_results = validator.validate_smiles_data(test_data)
    print(f"   æœ‰æ•ˆSMILES: {len(validation_results['canonical_smiles_mapping'])}")
    print(f"   æ— æ•ˆSMILES: {len(validation_results['invalid_smiles'])}")
    print(f"   éªŒè¯çš„ç‰©è´¨: {validation_results['substances_validated']}")
    
    # æµ‹è¯•2: å‚æ•°å»ºè®®å™¨
    print("\n2. æµ‹è¯•å‚æ•°å»ºè®®å™¨...")
    advisor = IntelligentParameterAdvisor()
    suggestions = advisor.analyze_experimental_context(test_data, "ç¯æ°§æ ‘è„‚å›ºåŒ–å®éªŒ")
    print(f"   å‚æ•°å»ºè®®æ•°é‡: {len(suggestions)}")
    for param, suggestion in suggestions.items():
        print(f"   {param}: {suggestion['current_range']} â†’ {suggestion['suggested_bounds']}")
    
    # æµ‹è¯•3: ç”¨æˆ·å®šä¹‰ç¼–ç å¤„ç†å™¨
    print("\n3. æµ‹è¯•ç”¨æˆ·å®šä¹‰ç¼–ç å¤„ç†å™¨...")
    encoder = UserDefinedEncodingHandler()
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_df = pd.DataFrame({
        'SubstanceA_name': ['æ ‘è„‚A', 'æ ‘è„‚B'],
        'SubstanceA_SMILES': ['CCO', 'CCCCO'], 
        'SubstanceB_name': ['ç¨€é‡Šå‰‚A', 'ç¨€é‡Šå‰‚B'],
        'SubstanceB_SMILES': ['', ''],  # ç‰¹æ®Šç‰©è´¨
        'SubstanceA_density': [1.15, 1.18],  # ç‰©ç†æ€§è´¨
        'Process_temperature': [80, 90]  # å·¥è‰ºå‚æ•°
    })
    
    user_special_data = encoder.identify_user_special_substances(test_df)
    classification = encoder.classify_user_columns(test_df)
    
    print(f"   è¯†åˆ«åˆ°ç‰¹æ®Šç‰©è´¨: {len(user_special_data['substances_without_smiles'])} ä¸ª")
    print(f"   è¯†åˆ«åˆ°æ‰©å±•åˆ—ç±»å‹: {len(classification['è¯†åˆ«çš„æ‰©å±•åˆ—'])} ç§")
    
    # æµ‹è¯•4: BayBEå¯ç”¨æ€§
    print("\n4. æµ‹è¯•BayBEå¯ç”¨æ€§...")
    if BAYBE_AVAILABLE:
        print("   âœ… BayBEå·²å®‰è£…ï¼Œå¯ä»¥ä½¿ç”¨å®Œæ•´åŠŸèƒ½")
        
        # æµ‹è¯•BayBEå‚æ•°åˆ›å»º
        try:
            parameters = validator.prepare_baybe_parameters(test_data, validation_results)
            print(f"   âœ… æˆåŠŸåˆ›å»º {len(parameters)} ä¸ªBayBEå‚æ•°")
        except Exception as e:
            print(f"   âŒ BayBEå‚æ•°åˆ›å»ºå¤±è´¥: {e}")
    else:
        print("   âš ï¸ BayBEæœªå®‰è£…ï¼Œä½¿ç”¨é™çº§æ¨¡å¼")
        print("   å»ºè®®è¿è¡Œ: pip install baybe")
    
    print("\n" + "=" * 50)
    print("ğŸ“Š Enhanced Verification Tools æµ‹è¯•å®Œæˆ")
    
    if BAYBE_AVAILABLE:
        print("ğŸ‰ æ‰€æœ‰åŠŸèƒ½å¯ç”¨ï¼ç³»ç»Ÿå·²å‡†å¤‡å¥½è¿›è¡Œå®Œæ•´çš„BayBEé›†æˆ")
    else:
        print("ğŸ”§ æ ¸å¿ƒåŠŸèƒ½å¯ç”¨ï¼å®‰è£…BayBEåå³å¯ä½¿ç”¨å®Œæ•´åŠŸèƒ½")
        print("   è¿è¡Œ: pip install baybe")
