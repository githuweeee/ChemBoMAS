

"""Enhanced Verification Agent Tools - å®ç°7ä¸ªæ ¸å¿ƒä»»åŠ¡çš„å·¥å…·å‡½æ•°"""

import os
import uuid
import pandas as pd
import numpy as np
import json
from datetime import datetime
from typing import Dict, List, Tuple, Optional
from google.adk.tools import ToolContext


def _read_csv_clean(path: str) -> pd.DataFrame:
    """
    è¯»å– CSV å¹¶æ¸…ç†åˆ—åï¼ˆå» BOM/ç©ºç™½ï¼Œç§»é™¤å¸¸è§ç´¢å¼•åˆ—å¦‚ Unnamed: 0ï¼‰
    """
    df = pd.read_csv(path, encoding="utf-8-sig")
    df.columns = [c.replace("\ufeff", "").strip() for c in df.columns]
    df = df.loc[:, ~df.columns.str.match(r"^Unnamed:\s*\d+$")]
    return df


def _detect_suspicious_headers(df: pd.DataFrame) -> list:
    """
    æ£€æµ‹ç–‘ä¼¼è¢«è¯´æ˜æ–‡å­—/å‚æ•°èŒƒå›´æ±¡æŸ“çš„è¡¨å¤´åˆ—åã€‚
    """
    suspicious = []
    for col in df.columns:
        col_str = str(col)
        # æ˜æ˜¾çš„è¯´æ˜æ€§æ–‡æœ¬/å¥å­å…³é”®è¯
        if any(token in col_str for token in ["æœ€å¤§åŒ–", "æœ€å°åŒ–", "æ¥å—å»ºè®®", "æ²¡æœ‰å…¶ä»–çº¦æŸ", "æ¯æ‰¹", "æœ€å¤š", "æ€»å…±", "å¸•ç´¯æ‰˜", "çº¦æŸ"]):
            suspicious.append(col_str)
            continue
        # å«æœ‰ä¸­æ–‡æ ‡ç‚¹æˆ–é•¿å¥ç‰¹å¾
        if any(token in col_str for token in ["ï¼Œ", "ã€‚", "ï¼š"]):
            if len(col_str) > 20:
                suspicious.append(col_str)
                continue
        # å‚æ•°èŒƒå›´ç›´æ¥å†™å…¥è¡¨å¤´çš„å¸¸è§å½¢å¼
        if "[" in col_str and "]" in col_str and ("ratio" in col_str.lower() or "target" in col_str.lower()):
            suspicious.append(col_str)
            continue
    return suspicious


def _reset_verification_state(state: dict, reason: str) -> None:
    """æ¸…ç†éªŒè¯ç›¸å…³çŠ¶æ€ï¼Œé¿å…æ±¡æŸ“åç»­æµç¨‹"""
    for key in [
        "verification_results",
        "baybe_campaign_config",
        "optimization_config",
        "ready_for_optimization",
        "searchspace_info",
        "campaign_built",
    ]:
        state.pop(key, None)
    state["verification_status"] = f"failed:{reason}"

# å¯¼å…¥åŒ–å­¦çŸ¥è¯†åº“
from .chemistry_knowledge_base import ChemistryKnowledgeBase

# æ³¨æ„ï¼šä»¥ä¸‹å¯¼å…¥éœ€è¦å®‰è£…BayBE
# pip install baybe
try:
    from baybe.utils.chemistry import get_canonical_smiles, name_to_smiles
    from baybe.parameters import CategoricalParameter, NumericalContinuousParameter, NumericalDiscreteParameter
    from baybe.parameters.enum import SubstanceEncoding
    BAYBE_AVAILABLE = True
except ImportError:
    print("Warning: BayBE not installed. Please run: pip install baybe")
    BAYBE_AVAILABLE = False


class SimplifiedSMILESValidator:
    """
    ç®€åŒ–çš„SMILESéªŒè¯å™¨ - æ— éœ€æ‰‹åŠ¨è®¡ç®—æè¿°ç¬¦
    BayBEä¼šåœ¨Campaignä¸­è‡ªåŠ¨å¤„ç†æ‰€æœ‰åˆ†å­æè¿°ç¬¦è®¡ç®—
    """
    
    def validate_smiles_data(self, data: pd.DataFrame) -> dict:
        """
        åªéªŒè¯SMILESæœ‰æ•ˆæ€§ï¼Œä¸è®¡ç®—æè¿°ç¬¦
        """
        validation_results = {
            "canonical_smiles_mapping": {},
            "invalid_smiles": [],
            "substances_validated": []
        }
        
        if not BAYBE_AVAILABLE:
            # é™çº§å¤„ç†ï¼šä½¿ç”¨åŸºæœ¬éªŒè¯
            return self._basic_smiles_validation(data)
        
        # è¯†åˆ«SMILESåˆ—
        smiles_columns = [col for col in data.columns if 'SMILE' in col.upper()]
        
        for col in smiles_columns:
            substance_name = col.split('_')[0] if '_' in col else col
            
            for idx, smiles in data[col].items():
                if pd.isna(smiles) or smiles == '':
                    continue
                    
                try:
                    # åªéªŒè¯å¹¶è·å–è§„èŒƒåŒ–SMILES
                    canonical_smiles = get_canonical_smiles(str(smiles))
                    
                    if canonical_smiles is not None:
                        validation_results["canonical_smiles_mapping"][smiles] = canonical_smiles
                    else:
                        validation_results["invalid_smiles"].append({
                            "substance": substance_name,
                            "row": idx,
                            "smiles": smiles,
                            "error": "æ— æ³•è§£æåˆ†å­ç»“æ„"
                        })
                        
                except Exception as e:
                    validation_results["invalid_smiles"].append({
                        "substance": substance_name,
                        "row": idx, 
                        "smiles": smiles,
                        "error": str(e)
                    })
            
            validation_results["substances_validated"].append(substance_name)
            
        return validation_results
    

def _auto_correct_invalid_smiles(
    df: pd.DataFrame,
    validation_results: dict,
) -> list:
    """
    å°è¯•æ ¹æ®åŒ–åˆç‰©åç§°è‡ªåŠ¨çº æ­£æ— æ•ˆçš„SMILESï¼ˆåœ¨è¿­ä»£å¼€å§‹å‰æ‰§è¡Œï¼‰

    ç­–ç•¥ï¼š
    1. ä½¿ç”¨éªŒè¯é˜¶æ®µè®°å½•çš„ invalid_smiles åˆ—è¡¨ï¼Œæ‰¾åˆ°å¯¹åº”çš„è¡Œå’Œç‰©è´¨
    2. åœ¨åŒä¸€è¡Œä¸­æŸ¥æ‰¾å¯¹åº”çš„åç§°åˆ—ï¼ˆå¦‚ SubstanceA_nameï¼‰
    3. ä½¿ç”¨ name_to_smiles å°†åç§°è½¬æ¢ä¸ºSMILES
    4. ä½¿ç”¨ get_canonical_smiles è§„èŒƒåŒ–åå†™å›åŸDataFrame

    æ³¨æ„ï¼š
    - åªåœ¨ BayBE å¯ç”¨æ—¶å¯ç”¨ï¼ˆéœ€è¦ name_to_smiles å’Œ get_canonical_smilesï¼‰
    - åªä¿®æ”¹å½“å‰ DataFrameï¼Œä¸ç›´æ¥ä¿®æ”¹ validation_results

    Returns:
        list[dict]: è‡ªåŠ¨çº æ­£è®°å½•åˆ—è¡¨
    """
    if not BAYBE_AVAILABLE:
        return []

    invalid_items = validation_results.get("invalid_smiles", [])
    if not invalid_items:
        return []

    corrections = []

    # æ‰€æœ‰ SMILES åˆ—
    smiles_columns = [col for col in df.columns if "SMILE" in str(col).upper()]

    for item in invalid_items:
        try:
            substance = item.get("substance")
            row_idx = item.get("row")
            orig_smiles = item.get("smiles")

            if row_idx is None or row_idx not in df.index:
                continue

            if orig_smiles is None or (isinstance(orig_smiles, float) and np.isnan(orig_smiles)):
                continue

            orig_smiles_str = str(orig_smiles).strip()
            if not orig_smiles_str:
                continue

            # æ‰¾åˆ°å¯¹åº”çš„SMILESåˆ—ï¼šåŒä¸€ç‰©è´¨ï¼Œä¸”è¯¥è¡Œå€¼ç­‰äºåŸå§‹SMILES
            target_col = None
            for col in smiles_columns:
                col_prefix = str(col).split("_")[0] if "_" in str(col) else str(col)
                if substance and col_prefix != str(substance):
                    continue
                cell_val = df.at[row_idx, col]
                if str(cell_val).strip() == orig_smiles_str:
                    target_col = col
                    break

            if target_col is None:
                continue

            # æŸ¥æ‰¾å¯¹åº”çš„åç§°åˆ—ï¼ˆå‚è€ƒ _extract_smiles_name_mapping_with_canonical çš„å‰ç¼€é€»è¾‘ï¼‰
            col_upper = str(target_col).upper()
            if target_col.endswith("_molecule"):
                prefix = target_col.rsplit("_molecule", 1)[0]
            else:
                idx = col_upper.find("_SMILE")
                prefix = target_col[:idx] if idx > 0 else str(target_col).split("_")[0]

            name_col = None
            candidates = [
                f"{prefix}_name",
                f"{prefix}_NAME",
                f"{prefix}_Name",
                f"{prefix}name",
                f"{prefix}NAME",
            ]
            for cand in candidates:
                if cand in df.columns and cand != target_col:
                    name_col = cand
                    break
            if name_col is None:
                # å†å°è¯•å¤§å°å†™ä¸æ•æ„ŸåŒ¹é…
                for df_col in df.columns:
                    if str(df_col).upper() == f"{prefix.upper()}_NAME" and df_col != target_col:
                        name_col = df_col
                        break

            if name_col is None:
                continue

            name_val = df.at[row_idx, name_col]
            if name_val is None or (isinstance(name_val, float) and np.isnan(name_val)):
                continue

            name_str = str(name_val).strip()
            if not name_str:
                continue

            # ä½¿ç”¨åç§°å°è¯•ç”Ÿæˆ SMILES
            try:
                generated_smiles = name_to_smiles(name_str)
            except Exception as e:
                print(f"[WARN] name_to_smiles å¤±è´¥: substance={substance}, name={name_str}, error={e}")
                continue

            if not generated_smiles:
                continue

            try:
                canonical = get_canonical_smiles(str(generated_smiles)) or str(generated_smiles)
            except Exception as e:
                print(f"[WARN] get_canonical_smiles å¤±è´¥: smiles={generated_smiles}, error={e}")
                continue

            if not canonical:
                continue

            # å†™å› DataFrameï¼ˆåœ¨æ ‡å‡†åŒ–å‰ç›´æ¥ä¿®æ­£åŸå§‹æ•°æ®ï¼‰
            df.at[row_idx, target_col] = canonical

            corrections.append(
                {
                    "substance": substance,
                    "row": int(row_idx),
                    "original_smiles": orig_smiles_str,
                    "corrected_smiles": canonical,
                    "name_column": name_col,
                    "name_value": name_str,
                }
            )

        except Exception as e:
            print(f"[WARN] _auto_correct_invalid_smiles å†…éƒ¨é”™è¯¯: {e}")
            continue

    if corrections:
        print(f"[DEBUG] è‡ªåŠ¨çº æ­£äº† {len(corrections)} ä¸ªSMILES: "
              f"{[c['substance'] for c in corrections[:3]]}"
              f"{' ...' if len(corrections) > 3 else ''}")

    return corrections

    def _basic_smiles_validation(self, data: pd.DataFrame) -> dict:
        """
        åŸºæœ¬SMILESéªŒè¯ï¼ˆå½“BayBEä¸å¯ç”¨æ—¶ï¼‰
        """
        validation_results = {
            "canonical_smiles_mapping": {},
            "invalid_smiles": [],
            "substances_validated": []
        }
        
        smiles_columns = [col for col in data.columns if 'SMILE' in col.upper()]
        
        for col in smiles_columns:
            substance_name = col.split('_')[0] if '_' in col else col
            
            for idx, smiles in data[col].items():
                if pd.isna(smiles) or smiles == '':
                    continue
                    
                # åŸºæœ¬æ ¼å¼æ£€æŸ¥
                if isinstance(smiles, str) and len(smiles) > 0:
                    validation_results["canonical_smiles_mapping"][smiles] = smiles  # ä¿æŒåŸæ ·
                else:
                    validation_results["invalid_smiles"].append({
                        "substance": substance_name,
                        "row": idx,
                        "smiles": smiles,
                        "error": "SMILESæ ¼å¼é”™è¯¯"
                    })
            
            validation_results["substances_validated"].append(substance_name)
            
        return validation_results
    
    def prepare_baybe_parameters(self, data: pd.DataFrame, validation_results: dict) -> list:
        """
        ä¸ºBayBEå‡†å¤‡å‚æ•°å®šä¹‰ï¼Œä½¿ç”¨åŸå§‹SMILES
        BayBEå†…éƒ¨ä¼šè‡ªåŠ¨å¤„ç†æè¿°ç¬¦è®¡ç®—
        """
        if not BAYBE_AVAILABLE:
            return []
            
        parameters = []
        
        # 1. åˆ†å­å‚æ•° - ç›´æ¥ä½¿ç”¨SMILESå­—ç¬¦ä¸²
        smiles_columns = [col for col in data.columns if 'SMILE' in col.upper()]
        for col in smiles_columns:
            substance_name = col.split('_')[0] if '_' in col else col
            
            # è·å–æœ‰æ•ˆçš„SMILESå€¼
            valid_smiles = []
            for smiles in data[col].dropna().unique():
                if str(smiles) in validation_results["canonical_smiles_mapping"]:
                    valid_smiles.append(validation_results["canonical_smiles_mapping"][str(smiles)])
            
            if len(valid_smiles) >= 2:  # BayBEè¦æ±‚è‡³å°‘2ä¸ªå€¼
                param = CategoricalParameter(
                    name=f"{substance_name}_molecule",
                    values=valid_smiles,  # BayBEä¼šè‡ªåŠ¨å¤„ç†è¿™äº›SMILESçš„æè¿°ç¬¦
                    encoding="OHE"
                )
                parameters.append(param)
            elif len(valid_smiles) == 1:
                # åªæœ‰1ä¸ªSMILESæ—¶ï¼Œè·³è¿‡åˆ†å­å‚æ•°ï¼ˆå› ä¸ºæ²¡æœ‰ä¼˜åŒ–ç©ºé—´ï¼‰
                print(f"âš ï¸ {substance_name} åªæœ‰1ä¸ªSMILESå€¼ï¼Œè·³è¿‡åˆ†å­å‚æ•°åˆ›å»º")
            else:
                print(f"âš ï¸ {substance_name} æ²¡æœ‰æœ‰æ•ˆSMILESï¼Œè·³è¿‡å‚æ•°åˆ›å»º")
        
        # 2. æ•°å€¼å‚æ•°ï¼ˆæ¯”ä¾‹ç­‰ï¼‰
        ratio_columns = [col for col in data.columns if 'ratio' in col.lower()]
        for col in ratio_columns:
            # å®‰å…¨çš„æ•°å€¼è½¬æ¢å’ŒèŒƒå›´è®¡ç®—
            numeric_data = pd.to_numeric(data[col], errors='coerce').dropna()
            if len(numeric_data) == 0:
                print(f"âš ï¸ {col} åˆ—æ²¡æœ‰æœ‰æ•ˆçš„æ•°å€¼æ•°æ®ï¼Œè·³è¿‡å‚æ•°åˆ›å»º")
                continue
            min_val = float(numeric_data.min())
            max_val = float(numeric_data.max())
            
            param = NumericalContinuousParameter(
                name=col,
                bounds=(max(0.0, min_val), min(1.0, max_val))
            )
            parameters.append(param)
            
        return parameters


class IntelligentParameterAdvisor:
    """
    åŸºäºåŒ–å­¦çŸ¥è¯†åº“çš„æ™ºèƒ½å‚æ•°å»ºè®®ç³»ç»Ÿ
    
    æ¶æ„è®¾è®¡åŸåˆ™ï¼š
    1. çŸ¥è¯†åº“(KB)æä¾›ç¡¬çº¦æŸ - å¦‚"ç¯æ°§å›ºåŒ–ä¸è¶…è¿‡250Â°C"
    2. è®¡ç®—å·¥å…·æä¾›ç‰©è´¨å±æ€§ - å¦‚RDKitè®¡ç®—åˆ†å­é‡ã€LogPç­‰
    3. LLMè´Ÿè´£æ•´åˆå’Œäº¤äº’ - ç†è§£ç”¨æˆ·æ„å›¾ï¼Œé€‰æ‹©åˆé€‚çš„çŸ¥è¯†åº“æ¡ç›®
    4. ç”¨æˆ·æœ€ç»ˆç¡®è®¤ - é¢†åŸŸä¸“å®¶æ‹æ¿
    
    æ³¨æ„ï¼šLLMä¸é€‚åˆç›´æ¥æ¨æ¼”ç²¾ç¡®çš„æ‰©å±•ç™¾åˆ†æ¯”ï¼Œåº”è¯¥ç”±çŸ¥è¯†åº“æä¾›å…¸å‹èŒƒå›´
    """
    
    def __init__(self):
        """åˆå§‹åŒ–å‚æ•°å»ºè®®å™¨ï¼ŒåŠ è½½åŒ–å­¦çŸ¥è¯†åº“"""
        self.knowledge_base = ChemistryKnowledgeBase()
        self.reaction_type = None  # ç¼“å­˜è¯†åˆ«çš„ååº”ç±»å‹
    
    def analyze_experimental_context(self, data: pd.DataFrame, user_description: str = "") -> dict:
        """
        åˆ†æå®éªŒèƒŒæ™¯ï¼Œæä¾›æ™ºèƒ½å‚æ•°å»ºè®®
        
        æµç¨‹ï¼š
        1. è¯†åˆ«ååº”ç±»å‹ï¼ˆä»ç‰©è´¨åç§°å’Œç”¨æˆ·æè¿°ï¼‰
        2. ä»çŸ¥è¯†åº“è·å–è¯¥ååº”ç±»å‹çš„å…¸å‹å‚æ•°èŒƒå›´
        3. ç»“åˆå½“å‰æ•°æ®èŒƒå›´ï¼Œç”Ÿæˆå»ºè®®è¾¹ç•Œ
        4. è¿”å›å»ºè®®ä¾›ç”¨æˆ·ç¡®è®¤ï¼ˆè€Œéç›´æ¥ä½¿ç”¨ï¼‰
        """
        suggestions = {}
        
        # 1. è¯†åˆ«ååº”ç±»å‹
        substances = self._extract_substance_names(data)
        self.reaction_type = self.knowledge_base.identify_reaction_type(
            substances, user_description
        )
        
        # 2. ä»çŸ¥è¯†åº“è·å–å‚æ•°å»ºè®®
        kb_suggestions = self.knowledge_base.get_parameter_suggestions(
            self.reaction_type, data
        )
        
        # 3. åˆ†æåˆ†å­ç±»å‹å’Œç‰¹æ€§
        molecular_analysis = self._analyze_molecules(data)
        
        # 4. ç”Ÿæˆç»¼åˆå‚æ•°è¾¹ç•Œå»ºè®®
        for col in data.columns:
            # è·³è¿‡ç›®æ ‡åˆ—
            if col.startswith('Target_'):
                continue
            
            numeric_data = pd.to_numeric(data[col], errors='coerce').dropna()
            if len(numeric_data) == 0:
                continue
            
            # ===== è¿ç»­å‚æ•°å¤„ç†ï¼ˆç¦ç”¨ç¦»æ•£å‚æ•°å»ºè®®ï¼‰=====
            current_range = (float(numeric_data.min()), float(numeric_data.max()))
            if 'ratio' in col.lower():
                # ä»çŸ¥è¯†åº“è·å–è¾¹ç•Œå»ºè®®
                suggested_bounds, reasoning = self._get_ratio_bounds_from_kb(
                    col, current_range, kb_suggestions
                )
                
                suggestions[col] = {
                    "current_range": [float(x) for x in current_range],
                    "suggested_bounds": [float(x) for x in suggested_bounds] if suggested_bounds else None,
                    "reasoning": reasoning,
                    "source": "knowledge_base",  # æ ‡æ˜æ¥æº
                    "constraints": self._suggest_constraints(col),
                    "requires_user_confirmation": True  # éœ€è¦ç”¨æˆ·ç¡®è®¤
                }
                
            elif 'temperature' in col.lower():
                # ä»çŸ¥è¯†åº“è·å–æ¸©åº¦è¾¹ç•Œå»ºè®®
                suggested_bounds, reasoning, safety_note = self._get_temperature_bounds_from_kb(
                    current_range, kb_suggestions
                )
                
                suggestions[col] = {
                    "current_range": [float(x) for x in current_range],
                    "suggested_bounds": [float(x) for x in suggested_bounds] if suggested_bounds else None,
                    "reasoning": reasoning,
                    "safety_note": safety_note,
                    "source": "knowledge_base",
                    "requires_user_confirmation": True
                }
            else:
                # å…¶ä»–æ•°å€¼å‚æ•°ï¼ˆé»˜è®¤è¿ç»­ï¼‰
                suggestions[col] = {
                    "current_range": [float(x) for x in current_range],
                    "suggested_bounds": [float(x) for x in current_range],  # é»˜è®¤ä½¿ç”¨å½“å‰èŒƒå›´
                    "source": "data_analysis",
                    "requires_user_confirmation": True
                }
        
        # 5. æ·»åŠ ååº”ç±»å‹ä¿¡æ¯å’Œå®‰å…¨è­¦å‘Š
        suggestions["_reaction_info"] = {
            "identified_type": self.reaction_type,
            "reaction_name": self.knowledge_base.REACTION_TYPES.get(
                self.reaction_type, {}
            ).get("name", "æœªçŸ¥ååº”ç±»å‹"),
            "safety_warnings": kb_suggestions.get("safety_warnings", []),
            "molecular_analysis": molecular_analysis
        }
        
        return suggestions
    
    def _extract_substance_names(self, data: pd.DataFrame) -> List[str]:
        """ä»æ•°æ®ä¸­æå–ç‰©è´¨åç§°"""
        substances = []
        name_columns = [col for col in data.columns if 'name' in col.lower()]
        
        for col in name_columns:
            substances.extend(data[col].dropna().astype(str).unique().tolist())
        
        return substances
    
    def _analyze_molecules(self, data: pd.DataFrame) -> dict:
        """åˆ†æåˆ†å­ç±»å‹å’Œç‰¹æ€§"""
        analysis = {}
        
        smiles_columns = [col for col in data.columns if 'SMILE' in col.upper()]
        for col in smiles_columns:
            smiles_list = data[col].dropna().astype(str).tolist()
            # ç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯PythonåŸç”Ÿç±»å‹ï¼Œä»¥ä¾¿JSONåºåˆ—åŒ–
            avg_len = float(np.mean([len(s) for s in smiles_list])) if smiles_list else 0.0
            mol_diversity = float(len(set(smiles_list)) / len(smiles_list)) if smiles_list else 0.0
            analysis[col] = {
                "molecule_count": int(len(set(smiles_list))),
                "avg_length": avg_len,
                "contains_aromatic": bool(any('c' in s.lower() or 'C' in s for s in smiles_list)),
                "molecular_diversity": mol_diversity
            }
        
        return analysis
    
    def _get_ratio_bounds_from_kb(
        self, 
        column_name: str, 
        current_range: Tuple[float, float],
        kb_suggestions: dict
    ) -> Tuple[Tuple[float, float], str]:
        """
        ä»çŸ¥è¯†åº“è·å–æ¯”ä¾‹å‚æ•°çš„å»ºè®®è¾¹ç•Œ
        
        ç­–ç•¥ï¼š
        1. ä¼˜å…ˆä½¿ç”¨çŸ¥è¯†åº“ä¸­è¯¥ååº”ç±»å‹çš„å…¸å‹èŒƒå›´
        2. ç»“åˆå½“å‰æ•°æ®èŒƒå›´ï¼Œå–å¹¶é›†ä»¥æ‰©å¤§æ¢ç´¢ç©ºé—´
        3. åº”ç”¨å®‰å…¨çº¦æŸï¼ˆå¦‚æ¯”ä¾‹å¿…é¡»åœ¨0-1ä¹‹é—´ï¼‰
        """
        min_val, max_val = current_range
        
        # ä»çŸ¥è¯†åº“è·å–è¯¥ååº”ç±»å‹çš„æ¯”ä¾‹å»ºè®®
        kb_ratio_info = kb_suggestions.get("ratios", {})
        kb_individual_bounds = kb_ratio_info.get("individual_bounds", {})
        
        # æ£€æŸ¥çŸ¥è¯†åº“ä¸­æ˜¯å¦æœ‰è¯¥åˆ—çš„å…·ä½“å»ºè®®
        if column_name in kb_individual_bounds:
            kb_bounds = kb_individual_bounds[column_name]
            kb_min = kb_bounds.get("min", 0.05)
            kb_max = kb_bounds.get("max", 0.95)
            
            # å–å½“å‰èŒƒå›´å’ŒçŸ¥è¯†åº“èŒƒå›´çš„å¹¶é›†ï¼Œæ‰©å¤§æ¢ç´¢ç©ºé—´
            suggested_min = min(min_val, kb_min)
            suggested_max = max(max_val, kb_max)
            
            reaction_name = self.knowledge_base.REACTION_TYPES.get(
                self.reaction_type, {}
            ).get("name", "åŒ–å­¦ååº”")
            reasoning = f"åŸºäº{reaction_name}çš„å…¸å‹é…æ¯”èŒƒå›´ï¼Œç»“åˆæ‚¨å½“å‰æ•°æ®çš„æ¢ç´¢èŒƒå›´"
        else:
            # çŸ¥è¯†åº“ä¸­æ²¡æœ‰å…·ä½“å»ºè®®ï¼Œä½¿ç”¨åŸºäºç‰©è´¨ç±»å‹çš„è§„åˆ™
            suggested_min, suggested_max, reasoning = self._infer_ratio_bounds_by_substance_type(
                column_name, current_range
            )
        
        # åº”ç”¨ç¡¬çº¦æŸï¼šæ¯”ä¾‹å¿…é¡»åœ¨0-1ä¹‹é—´
        suggested_min = max(0.0, suggested_min)
        suggested_max = min(1.0, suggested_max)
        
        return (suggested_min, suggested_max), reasoning
    
    def _infer_ratio_bounds_by_substance_type(
        self, 
        column_name: str, 
        current_range: Tuple[float, float]
    ) -> Tuple[float, float, str]:
        """
        æ ¹æ®ç‰©è´¨ç±»å‹æ¨æ–­æ¯”ä¾‹è¾¹ç•Œï¼ˆå½“çŸ¥è¯†åº“ä¸­æ²¡æœ‰å…·ä½“ä¿¡æ¯æ—¶ï¼‰
        ä½¿ç”¨ææ–™å±æ€§çŸ¥è¯†åº“ä¸­çš„è§„åˆ™
        """
        min_val, max_val = current_range
        col_lower = column_name.lower()
        
        # æ ¹æ®ç‰©è´¨ç±»å‹åº”ç”¨ä¸åŒçš„è§„åˆ™ï¼ˆéœ€è¿›ä¸€æ­¥å®Œå–„ï¼‰
        if 'catalyst' in col_lower or 'å‚¬åŒ–' in col_lower:
            # å‚¬åŒ–å‰‚é€šå¸¸ç”¨é‡å°‘
            kb_info = self.knowledge_base.REACTION_TYPES.get(
                self.reaction_type, {}
            ).get("catalyst_concentration", (0.001, 0.1))
            return (
                kb_info[0], 
                kb_info[1],
                f"å‚¬åŒ–å‰‚å…¸å‹æµ“åº¦èŒƒå›´ {kb_info[0]*100:.1f}%-{kb_info[1]*100:.1f}%"
            )
            
        elif 'hardener' in col_lower or 'å›ºåŒ–å‰‚' in col_lower:
            # å›ºåŒ–å‰‚æœ‰åŒ–å­¦è®¡é‡æ¯”è¦æ±‚
            ratio_info = self.knowledge_base.SAFETY_CONSTRAINTS.get(
                "ratio_constraints", {}
            ).get("epoxy_hardener", {})
            acceptable_range = ratio_info.get("acceptable_range", (0.2, 0.5))
            return (
                acceptable_range[0],
                acceptable_range[1],
                f"å›ºåŒ–å‰‚é…æ¯”èŒƒå›´ï¼Œè€ƒè™‘åŒ–å­¦è®¡é‡æ¯” (æ¬ å›ºåŒ–é£é™©<{ratio_info.get('under_cure_risk', '0.8')})"
            )
            
        elif 'diluent' in col_lower or 'ç¨€é‡Š' in col_lower or 'solvent' in col_lower:
            # ç¨€é‡Šå‰‚/æº¶å‰‚æœ‰æœ€å¤§ç”¨é‡é™åˆ¶
            diluent_info = self.knowledge_base.MATERIAL_PROPERTIES.get(
                "diluents", {}
            ).get("reactive_diluents", {})
            max_conc = diluent_info.get("max_concentration", 0.3)
            return (
                0.0,
                max_conc,
                f"ç¨€é‡Šå‰‚æœ€å¤§ç”¨é‡é™åˆ¶ä¸º {max_conc*100:.0f}%ï¼Œè¿‡é‡ä¼šå½±å“æ€§èƒ½"
            )
            
        else:
            # ä¸€èˆ¬ç‰©è´¨ï¼šåŸºäºå½“å‰èŒƒå›´é€‚åº¦æ‰©å±•ï¼ˆç›®å‰è¿™é‡Œå†™æ­»äº†ï¼Œå¾…è¿›ä¸€æ­¥è®¨è®ºï¼‰
            # æ‰©å±•å› å­åŸºäºæ•°æ®ç¨€ç–æ€§ï¼Œè€ŒéLLMæ¨æ¼”çš„ç™¾åˆ†æ¯”
            data_span = max_val - min_val
            if data_span < 0.1:
                # æ•°æ®èŒƒå›´å¾ˆçª„ï¼Œå»ºè®®æ‰©å¤§æ¢ç´¢
                expansion = 0.15
                reasoning = f"å½“å‰æ•°æ®ä»…æ¢ç´¢äº†{data_span*100:.0f}%çš„èŒƒå›´ï¼Œå»ºè®®æ‰©å¤§è‡³Â±15%ä»¥å‘ç°æ½œåœ¨æœ€ä¼˜ç‚¹"
            else:
                expansion = 0.1
                reasoning = f"åŸºäºå½“å‰æ•°æ®èŒƒå›´é€‚åº¦æ‰©å±•Â±10%"
            
            return (
                max(0.0, min_val - expansion),
                min(1.0, max_val + expansion),
                reasoning
            )
    
    def _get_temperature_bounds_from_kb(
        self, 
        current_range: Tuple[float, float],
        kb_suggestions: dict
    ) -> Tuple[Tuple[float, float], str, str]:
        """
        ä»çŸ¥è¯†åº“è·å–æ¸©åº¦å‚æ•°çš„å»ºè®®è¾¹ç•Œ
        
        ç­–ç•¥ï¼š
        1. ä½¿ç”¨çŸ¥è¯†åº“ä¸­è¯¥ååº”ç±»å‹çš„å…¸å‹æ¸©åº¦èŒƒå›´
        2. åº”ç”¨å®‰å…¨çº¦æŸï¼ˆå¦‚æœ€é«˜æ¸©åº¦é™åˆ¶ï¼‰
        3. è¿”å›å®‰å…¨æç¤ºä¿¡æ¯
        """
        min_temp, max_temp = current_range
        
        # ä»çŸ¥è¯†åº“è·å–æ¸©åº¦å»ºè®®
        kb_temp_info = kb_suggestions.get("temperature", {})
        
        if kb_temp_info:
            # çŸ¥è¯†åº“æœ‰è¯¥ååº”ç±»å‹çš„æ¸©åº¦å»ºè®®
            recommended_range = kb_temp_info.get("recommended_range", (-200, 400))
            optimal_range = kb_temp_info.get("optimal_range", recommended_range)
            safety_note = kb_temp_info.get("safety_note", "è¯·æ³¨æ„æ¸©åº¦å®‰å…¨æ§åˆ¶")
            
            # å–å½“å‰èŒƒå›´å’Œæ¨èèŒƒå›´çš„å¹¶é›†
            suggested_min = min(min_temp, recommended_range[0])
            suggested_max = max(max_temp, recommended_range[1])
            
            reasoning = (
                f"åŸºäº{kb_temp_info.get('reasoning', 'åŒ–å­¦ååº”')}ï¼Œ"
                f"å…¸å‹èŒƒå›´{recommended_range[0]}-{recommended_range[1]}Â°Cï¼Œ"
                f"æœ€ä¼˜èŒƒå›´{optimal_range[0]}-{optimal_range[1]}Â°C"
            )
        else:
            # ä½¿ç”¨å®‰å…¨çº¦æŸä¸­çš„é»˜è®¤å€¼
            safety_limits = self.knowledge_base.SAFETY_CONSTRAINTS.get(
                "temperature_limits", {}
            ).get("epoxy_systems", {})
            
            safe_max = safety_limits.get("safe_max", 200)
            flash_point = safety_limits.get("flash_point_concern", 150)
            
            # åŸºäºå½“å‰èŒƒå›´é€‚åº¦æ‰©å±•ï¼Œä½†ä¸è¶…è¿‡å®‰å…¨é™åˆ¶
            buffer = 20
            suggested_min = max(20, min_temp - buffer)
            suggested_max = min(safe_max, max_temp + buffer)
            
            reasoning = f"åŸºäºé€šç”¨å®‰å…¨è€ƒè™‘ï¼Œæ¸©åº¦èŒƒå›´ {suggested_min}-{suggested_max}Â°C"
            safety_note = f"å®‰å…¨ä¸Šé™: {safe_max}Â°C, é—ªç‚¹å…³æ³¨æ¸©åº¦: {flash_point}Â°C"
        
        # æœ€ç»ˆå®‰å…¨æ£€æŸ¥
        safety_limits = self.knowledge_base.SAFETY_CONSTRAINTS.get(
            "temperature_limits", {}
        ).get("epoxy_systems", {})
        absolute_max = safety_limits.get("decomposition_risk", 300)
        suggested_max = min(suggested_max, absolute_max)
        
        return (suggested_min, suggested_max), reasoning, safety_note
    
    def _suggest_constraints(self, column_name: str) -> list:
        """å»ºè®®çº¦æŸæ¡ä»¶"""
        constraints = []
        
        if 'ratio' in column_name.lower():
            constraints.append({
                "type": "sum_constraint",
                "description": "æ‰€æœ‰æ¯”ä¾‹ä¹‹å’Œåº”ç­‰äº1.0",
                "implementation": "ContinuousLinearConstraint",
                "source": "knowledge_base"
            })
        
        return constraints
    
    def get_reaction_summary(self) -> str:
        """è·å–å½“å‰è¯†åˆ«çš„ååº”ç±»å‹æ‘˜è¦"""
        if self.reaction_type:
            return self.knowledge_base.get_reaction_info_summary(self.reaction_type)
        return "å°šæœªè¯†åˆ«ååº”ç±»å‹"
    
    def validate_proposed_conditions(self, conditions: dict) -> Tuple[bool, List[str]]:
        """éªŒè¯æè®®çš„å®éªŒæ¡ä»¶æ˜¯å¦åˆç†"""
        if self.reaction_type:
            return self.knowledge_base.validate_experimental_conditions(
                conditions, self.reaction_type
            )
        return True, ["æœªè¯†åˆ«ååº”ç±»å‹ï¼Œæ— æ³•è¿›è¡Œä¸“ä¸šéªŒè¯"]


class UserDefinedEncodingHandler:
    """
    è¯†åˆ«å’Œå¤„ç†ç”¨æˆ·åœ¨CSVä¸­æä¾›çš„ç‰¹æ®Šç¼–ç ä¿¡æ¯
    æ”¯æŒåŠ¨æ€è¯†åˆ«å’Œæ ‡å‡†æ ¼å¼å¼•å¯¼çš„æ··åˆç­–ç•¥
    """
    
    def __init__(self):
        # å®šä¹‰åˆ—ç±»å‹è¯†åˆ«è§„åˆ™
        self.column_type_patterns = {
            "ç‰©ç†æ€§è´¨": {
                "keywords": ["density", "viscosity", "refractive", "melting", "boiling", "tg", "å¯†åº¦", "ç²˜åº¦", "æŠ˜å°„", "ç†”ç‚¹", "æ²¸ç‚¹", "ç»ç’ƒåŒ–"],
                "value_type": "numerical",
                "baybe_param_type": "NumericalContinuousParameter"
            },
            "åŠŸèƒ½åˆ†ç±»": {
                "keywords": ["catalyst", "additive", "modifier", "type", "category", "function", "å‚¬åŒ–å‰‚", "æ·»åŠ å‰‚", "æ”¹æ€§å‰‚", "ç±»å‹", "åŠŸèƒ½"],
                "value_type": "categorical", 
                "baybe_param_type": "CategoricalParameter"
            },
            "ä¾›åº”å•†ä¿¡æ¯": {
                "keywords": ["supplier", "vendor", "batch", "lot", "grade", "purity", "ä¾›åº”å•†", "æ‰¹æ¬¡", "ç­‰çº§", "çº¯åº¦"],
                "value_type": "categorical",
                "baybe_param_type": "CategoricalParameter" 
            },
            "æˆæœ¬ä¿¡æ¯": {
                "keywords": ["cost", "price", "availability", "expensive", "cheap", "æˆæœ¬", "ä»·æ ¼", "å¯è·å¾—æ€§"],
                "value_type": "numerical",
                "baybe_param_type": "NumericalContinuousParameter"
            },
            "å·¥è‰ºå‚æ•°": {
                "keywords": ["temperature", "time", "pressure", "speed", "rpm", "æ¸©åº¦", "æ—¶é—´", "å‹åŠ›", "è½¬é€Ÿ"],
                "value_type": "numerical", 
                "baybe_param_type": "NumericalContinuousParameter"
            },
            "é…æ–¹ç‰¹æ€§": {
                "keywords": ["hardener", "crosslinker", "solvent", "diluent", "å›ºåŒ–å‰‚", "äº¤è”å‰‚", "æº¶å‰‚", "ç¨€é‡Šå‰‚"],
                "value_type": "categorical",
                "baybe_param_type": "CategoricalParameter"
            }
        }
    
    def identify_user_special_substances(self, df: pd.DataFrame) -> dict:
        """
        è¯†åˆ«ç”¨æˆ·å®šä¹‰çš„ç‰¹æ®Šç‰©è´¨ï¼ˆSMILESä¸ºç©ºä½†æœ‰åç§°çš„ç‰©è´¨ï¼‰
        """
        user_special_substances = {
            "substances_without_smiles": [],
            "potential_encoding_columns": [],
            "custom_descriptors": {}
        }
        
        # æ‰¾åˆ°æ‰€æœ‰ç‰©è´¨åˆ—å¯¹
        substance_pairs = []
        for col in df.columns:
            if 'name' in col.lower() and 'substance' in col.lower():
                substance_name = col
                # å¯»æ‰¾å¯¹åº”çš„SMILESåˆ—
                substance_prefix = col.replace('_name', '').replace('name', '')
                smiles_col = None
                for scol in df.columns:
                    if substance_prefix in scol and 'SMILE' in scol.upper():
                        smiles_col = scol
                        break
                
                if smiles_col:
                    substance_pairs.append((substance_name, smiles_col))
        
        # è¯†åˆ«ç‰¹æ®Šç‰©è´¨ï¼ˆæœ‰åç§°ä½†SMILESä¸ºç©º/æ— æ•ˆï¼‰
        for name_col, smiles_col in substance_pairs:
            for idx, row in df.iterrows():
                substance_name = row[name_col]
                smiles_value = row[smiles_col]
                
                # å¦‚æœæœ‰ç‰©è´¨åç§°ä½†SMILESä¸ºç©ºæˆ–æ— æ•ˆ
                if (pd.notna(substance_name) and substance_name.strip() != "" and 
                    (pd.isna(smiles_value) or smiles_value == "" or str(smiles_value).strip() == "")):
                    
                    user_special_substances["substances_without_smiles"].append({
                        "name": substance_name,
                        "column_prefix": name_col.replace('_name', '').replace('name', ''),
                        "row": idx + 1
                    })
        
        # å¯»æ‰¾å¯èƒ½çš„è‡ªå®šä¹‰ç¼–ç åˆ—
        for col in df.columns:
            # å¯»æ‰¾åŒ…å«ç‰¹å¾æè¿°çš„åˆ—ï¼ˆä¸æ˜¯standardçš„name/SMILES/ratioåˆ—ï¼‰
            if not any(keyword in col.lower() for keyword in ['name', 'smile', 'ratio', 'target', 'unnamed']):
                if df[col].notna().any():  # å¦‚æœåˆ—æœ‰æ•°æ®
                    user_special_substances["potential_encoding_columns"].append(col)
                    # æ”¶é›†è¯¥åˆ—çš„å”¯ä¸€å€¼ä½œä¸ºå¯èƒ½çš„ç¼–ç 
                    unique_values = df[col].dropna().unique()
                    user_special_substances["custom_descriptors"][col] = unique_values.tolist()
        
        return user_special_substances
    
    def classify_user_columns(self, df: pd.DataFrame) -> dict:
        """
        æ™ºèƒ½åˆ†ç±»ç”¨æˆ·çš„æ‰€æœ‰åˆ—ï¼Œè¯†åˆ«æ½œåœ¨çš„ç¼–ç ä¿¡æ¯
        """
        column_classification = {
            "æ ‡å‡†åˆ—": {"name": [], "smiles": [], "ratio": [], "target": []},
            "è¯†åˆ«çš„æ‰©å±•åˆ—": {},
            "æœªåˆ†ç±»åˆ—": [],
            "å»ºè®®çš„æ ‡å‡†æ ¼å¼": {}
        }
        
        for col in df.columns:
            col_lower = col.lower()
            classified = False
            
            # 1. è¯†åˆ«æ ‡å‡†åˆ—
            if any(keyword in col_lower for keyword in ['name', 'åç§°']):
                column_classification["æ ‡å‡†åˆ—"]["name"].append(col)
                classified = True
            elif any(keyword in col_lower for keyword in ['smile', 'smiles']):
                column_classification["æ ‡å‡†åˆ—"]["smiles"].append(col)
                classified = True
            elif any(keyword in col_lower for keyword in ['ratio', 'æ¯”ä¾‹']):
                column_classification["æ ‡å‡†åˆ—"]["ratio"].append(col)
                classified = True
            elif any(keyword in col_lower for keyword in ['target', 'ç›®æ ‡']):
                column_classification["æ ‡å‡†åˆ—"]["target"].append(col)
                classified = True
            
            # 2. åŠ¨æ€è¯†åˆ«æ‰©å±•åˆ—ç±»å‹
            if not classified:
                for category, pattern_info in self.column_type_patterns.items():
                    if any(keyword in col_lower for keyword in pattern_info["keywords"]):
                        if category not in column_classification["è¯†åˆ«çš„æ‰©å±•åˆ—"]:
                            column_classification["è¯†åˆ«çš„æ‰©å±•åˆ—"][category] = []
                        
                        # åˆ†æåˆ—çš„å®é™…æ•°æ®ç±»å‹
                        sample_data = df[col].dropna().head(10)
                        if len(sample_data) > 0:
                            data_analysis = self._analyze_column_content(sample_data)
                            
                            column_classification["è¯†åˆ«çš„æ‰©å±•åˆ—"][category].append({
                                "column_name": col,
                                "predicted_type": pattern_info["value_type"],
                                "actual_data_type": data_analysis["inferred_type"],
                                "sample_values": data_analysis["sample_values"],
                                "baybe_param_type": pattern_info["baybe_param_type"],
                                "confidence": data_analysis["confidence"]
                            })
                        classified = True
                        break
            
            # 3. æœªèƒ½åˆ†ç±»çš„åˆ—
            if not classified and col.strip() != "" and "unnamed" not in col_lower:
                column_classification["æœªåˆ†ç±»åˆ—"].append(col)
        
        # 4. ç”Ÿæˆæ ‡å‡†æ ¼å¼å»ºè®®
        column_classification["å»ºè®®çš„æ ‡å‡†æ ¼å¼"] = self._generate_standard_format_suggestions(df)
        
        return column_classification
    
    def _analyze_column_content(self, sample_data: pd.Series) -> dict:
        """
        åˆ†æåˆ—å†…å®¹ï¼Œæ¨æ–­æ•°æ®ç±»å‹å’Œç½®ä¿¡åº¦
        """
        analysis = {
            "inferred_type": "unknown",
            "sample_values": sample_data.tolist()[:5],  # å‰5ä¸ªæ ·æœ¬
            "confidence": 0.0
        }
        
        # å°è¯•æ•°å€¼è½¬æ¢
        numeric_conversion = pd.to_numeric(sample_data, errors='coerce')
        numeric_ratio = numeric_conversion.notna().sum() / len(sample_data)
        
        if numeric_ratio >= 0.8:  # 80%ä»¥ä¸Šå¯è½¬æ¢ä¸ºæ•°å€¼
            analysis["inferred_type"] = "numerical"
            analysis["confidence"] = numeric_ratio
        elif len(sample_data.unique()) <= max(10, len(sample_data) * 0.5):  # å”¯ä¸€å€¼è¾ƒå°‘
            analysis["inferred_type"] = "categorical"
            analysis["confidence"] = 1.0 - (len(sample_data.unique()) / len(sample_data))
        else:
            analysis["inferred_type"] = "text"
            analysis["confidence"] = 0.5
        
        return analysis
    
    def _generate_standard_format_suggestions(self, df: pd.DataFrame) -> dict:
        """
        åŸºäºå½“å‰æ•°æ®ç”Ÿæˆæ ‡å‡†æ ¼å¼å»ºè®®
        """
        suggestions = {
            "æ¨èçš„åˆ—å‘½åè§„èŒƒ": {
                "ç‰©è´¨ä¿¡æ¯": [
                    "SubstanceA_name (ç‰©è´¨åç§°)",
                    "SubstanceA_SMILES (åˆ†å­ç»“æ„)", 
                    "SubstanceA_ratio (æ¯”ä¾‹)",
                    "SubstanceA_type (ç‰©è´¨ç±»å‹: resin/hardener/catalyst/solvent/additive)",
                    "SubstanceA_supplier (ä¾›åº”å•†)",
                    "SubstanceA_grade (ç­‰çº§/çº¯åº¦)",
                    "SubstanceA_batch (æ‰¹æ¬¡å·)"
                ],
                "ç‰©ç†æ€§è´¨": [
                    "SubstanceA_density (å¯†åº¦ g/cmÂ³)",
                    "SubstanceA_viscosity (ç²˜åº¦ PaÂ·s)",
                    "SubstanceA_tg (ç»ç’ƒåŒ–æ¸©åº¦ Â°C)",
                    "SubstanceA_melting_point (ç†”ç‚¹ Â°C)"
                ],
                "å·¥è‰ºå‚æ•°": [
                    "Process_temperature (ååº”æ¸©åº¦ Â°C)",
                    "Process_time (ååº”æ—¶é—´ min)",
                    "Process_pressure (å‹åŠ› bar)",
                    "Curing_temperature (å›ºåŒ–æ¸©åº¦ Â°C)"
                ],
                "æˆæœ¬ä¿¡æ¯": [
                    "SubstanceA_cost_per_kg (æˆæœ¬ å…ƒ/kg)",
                    "SubstanceA_availability (å¯è·å¾—æ€§: high/medium/low)"
                ]
            },
            "å½“å‰æ•°æ®æ˜ å°„å»ºè®®": {}
        }
        
        # åŸºäºå½“å‰æ•°æ®æä¾›å…·ä½“çš„é‡å‘½åå»ºè®®
        for col in df.columns:
            if "unnamed" in col.lower():
                continue
                
            col_lower = col.lower()
            mapping_suggestion = None
            
            # å°è¯•æ˜ å°„åˆ°æ ‡å‡†æ ¼å¼
            if any(keyword in col_lower for keyword in ['ç¨€é‡Š', 'diluent', 'solvent']):
                mapping_suggestion = f"{col} â†’ SubstanceX_type (å€¼: solvent/diluent)"
            elif any(keyword in col_lower for keyword in ['å‚¬åŒ–', 'catalyst']):
                mapping_suggestion = f"{col} â†’ SubstanceX_type (å€¼: catalyst)"
            elif any(keyword in col_lower for keyword in ['å¯†åº¦', 'density']):
                mapping_suggestion = f"{col} â†’ SubstanceX_density"
            elif any(keyword in col_lower for keyword in ['ç²˜åº¦', 'viscosity']):
                mapping_suggestion = f"{col} â†’ SubstanceX_viscosity"
                
            if mapping_suggestion:
                suggestions["å½“å‰æ•°æ®æ˜ å°„å»ºè®®"][col] = mapping_suggestion
        
        return suggestions
    
    def create_baybe_parameters_for_special_substances(self, user_special_data: dict, df: pd.DataFrame) -> list:
        """
        ä¸ºç”¨æˆ·å®šä¹‰çš„ç‰¹æ®Šç‰©è´¨åˆ›å»ºBayBEå‚æ•°é…ç½®ï¼ˆå¯åºåˆ—åŒ–çš„é…ç½®ï¼Œéå®é™…å¯¹è±¡ï¼‰
        
        æ³¨æ„ï¼šæ­¤å‡½æ•°è¿”å›çš„æ˜¯å‚æ•°é…ç½®ä¿¡æ¯ï¼ˆå¯JSONåºåˆ—åŒ–ï¼‰ï¼Œ
        è€Œä¸æ˜¯å®é™…çš„BayBE Parameterå¯¹è±¡ã€‚å®é™…å¯¹è±¡åº”åœ¨ Recommender Agent ä¸­åˆ›å»ºã€‚
        """
        parameter_configs = []
        
        # å¤„ç†æ²¡æœ‰SMILESçš„ç‰¹æ®Šç‰©è´¨
        for special_substance in user_special_data["substances_without_smiles"]:
            substance_name = special_substance["name"]
            column_prefix = special_substance["column_prefix"]
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å¯¹åº”çš„æ¯”ä¾‹åˆ—
            ratio_col = f"{column_prefix}_ratio"
            if ratio_col in df.columns:
                # å¯¹äºç‰¹æ®Šç‰©è´¨ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨åç§°ä½œä¸ºåˆ†ç±»å‚æ•°
                unique_names = df[f"{column_prefix}_name"].dropna().unique()
                if len(unique_names) > 1:
                    # å­˜å‚¨å‚æ•°é…ç½®ï¼Œè€Œéå®é™…çš„BayBEå¯¹è±¡
                    parameter_configs.append({
                        "param_type": "CategoricalParameter",
                        "name": f"{column_prefix}_special_substance",
                        "values": [str(name) for name in unique_names],
                        "encoding": "OHE",
                        "source": "user_defined_special_substance",
                        "substance_type": "special_without_smiles",
                        "original_column": f"{column_prefix}_name"
                    })
        
        # å¤„ç†è‡ªå®šä¹‰æè¿°ç¬¦åˆ—
        for col, values in user_special_data["custom_descriptors"].items():
            if len(values) > 1:  # åªæœ‰å½“æœ‰å¤šä¸ªä¸åŒå€¼æ—¶æ‰åˆ›å»ºå‚æ•°
                # åˆ¤æ–­æ˜¯æ•°å€¼è¿˜æ˜¯åˆ†ç±»æ•°æ®
                numeric_values = pd.to_numeric(pd.Series(values), errors='coerce').dropna()
                
                if len(numeric_values) == len(values):  # å…¨æ˜¯æ•°å€¼
                    parameter_configs.append({
                        "param_type": "NumericalContinuousParameter",
                        "name": f"custom_{col}",
                        "bounds": [float(min(numeric_values)), float(max(numeric_values))],
                        "source": "user_defined_descriptor",
                        "original_column": col
                    })
                else:  # åˆ†ç±»æ•°æ®
                    # ç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯å¯åºåˆ—åŒ–çš„
                    serializable_values = []
                    for v in values:
                        if isinstance(v, (str, int, float, bool)) or v is None:
                            serializable_values.append(v)
                        else:
                            serializable_values.append(str(v))
                    
                    parameter_configs.append({
                        "param_type": "CategoricalParameter",
                        "name": f"custom_{col}",
                        "values": serializable_values,
                        "encoding": "OHE",
                        "source": "user_defined_descriptor",
                        "original_column": col
                    })
        
        return parameter_configs
    
    def generate_standard_csv_template(self, num_substances: int = 4) -> str:
        """
        ç”ŸæˆåŒ…å«æ‰©å±•åˆ—ç±»å‹çš„æ ‡å‡†CSVæ¨¡æ¿
        """
        headers = []
        
        # ä¸ºæ¯ä¸ªç‰©è´¨ç”Ÿæˆå®Œæ•´çš„åˆ—é›†åˆ
        for i in range(num_substances):
            substance = chr(65 + i)  # A, B, C, D...
            headers.extend([
                f"Substance{substance}_name",
                f"Substance{substance}_SMILES", 
                f"Substance{substance}_ratio",
                f"Substance{substance}_type",           # åŠŸèƒ½åˆ†ç±»
                f"Substance{substance}_supplier",       # ä¾›åº”å•†ä¿¡æ¯
                f"Substance{substance}_grade",          # ç­‰çº§/çº¯åº¦
                f"Substance{substance}_density",        # ç‰©ç†æ€§è´¨
                f"Substance{substance}_viscosity",      # ç‰©ç†æ€§è´¨
                f"Substance{substance}_cost_per_kg",    # æˆæœ¬ä¿¡æ¯
                f"Substance{substance}_availability",   # å¯è·å¾—æ€§
            ])
        
        # æ·»åŠ å·¥è‰ºå‚æ•°
        headers.extend([
            "Process_temperature",      # å·¥è‰ºå‚æ•°
            "Process_time", 
            "Process_pressure",
            "Curing_temperature",
            "Mixing_speed"
        ])
        
        # æ·»åŠ ç›®æ ‡å˜é‡
        headers.extend([
            "Target_mechanical_strength",
            "Target_thermal_stability", 
            "Target_chemical_resistance",
            "Target_cost_effectiveness"
        ])
        
        # ç”Ÿæˆç¤ºä¾‹æ•°æ®è¡Œ
        example_rows = []
        example_rows.append([
            # SubstanceA (ä¸»æ ‘è„‚)
            "Epoxy_Resin_E51", "CC(C)(C1=CC=C(C=C1)OCC2CO2)C3=CC=C(C=C3)OCC4CO4", "0.6", "epoxy_resin", "Supplier_A", "Industrial_Grade", "1.15", "800", "25.5", "high",
            # SubstanceB (å›ºåŒ–å‰‚)  
            "Hardener_DETA", "NCCNCCN", "0.3", "hardener", "Supplier_B", "Analytical_Grade", "0.95", "20", "18.2", "medium",
            # SubstanceC (ç¨€é‡Šå‰‚)
            "Diluent_A", "", "0.1", "diluent", "Supplier_C", "Industrial_Grade", "0.85", "5", "12.0", "high",
            # SubstanceD (æ·»åŠ å‰‚)
            "Antioxidant_BHT", "CC(C)(C)C1=CC(=C(C(=C1)C(C)(C)C)O)C(C)(C)C", "0.0", "antioxidant", "Supplier_D", "Analytical_Grade", "1.05", "1000", "45.8", "low",
            # å·¥è‰ºå‚æ•°
            "80", "120", "1.0", "150", "500",
            # ç›®æ ‡
            "85", "200", "95", "0.8"
        ])
        
        # æ·»åŠ ç¬¬äºŒè¡Œç¤ºä¾‹æ•°æ®
        example_rows.append([
            # SubstanceA
            "Epoxy_Resin_E44", "CC(C)(C1=CC=C(C=C1)OCC2CO2)C3=CC=C(C=C3)OCC4CO4", "0.7", "epoxy_resin", "Supplier_A", "Industrial_Grade", "1.18", "1200", "28.0", "high",
            # SubstanceB  
            "Hardener_IPDA", "C1CCC(CC1)N", "0.25", "hardener", "Supplier_B", "Analytical_Grade", "0.92", "15", "20.5", "medium",
            # SubstanceC
            "Diluent_B", "", "0.05", "diluent", "Supplier_E", "Industrial_Grade", "0.88", "8", "15.0", "medium",
            # SubstanceD
            "UV_Stabilizer", "CC(C)(C)C1=CC(=C(C(=C1)C(C)(C)C)OCC(=O)OC)C(C)(C)C", "0.0", "uv_stabilizer", "Supplier_F", "Analytical_Grade", "1.02", "2000", "52.3", "low",
            # å·¥è‰ºå‚æ•°
            "90", "90", "1.2", "160", "400",
            # ç›®æ ‡  
            "92", "220", "88", "0.75"
        ])
        
        # æ„å»ºCSVå†…å®¹ï¼ˆä½¿ç”¨è‹±æ–‡é¿å…ç¼–ç é—®é¢˜ï¼‰
        csv_content = ",".join(headers) + "\n"
        for row in example_rows:
            csv_content += ",".join(map(str, row)) + "\n"
        
        return csv_content


def diagnose_data_types(file_path: str) -> str:
    """
    è¯Šæ–­CSVæ•°æ®ä¸­çš„ç±»å‹é—®é¢˜ï¼Œå¸®åŠ©ç”¨æˆ·æ‰¾åˆ°å¯¼è‡´ç±»å‹é”™è¯¯çš„å…·ä½“æ•°æ®
    """
    try:
        # å…¼å®¹å¯¹è¯é‡Œç›´æ¥ç²˜è´´CSVå†…å®¹çš„åœºæ™¯
        if ',' in file_path and '\n' in file_path and not os.path.exists(file_path):
            temp_path = f"temp_diagnose_{uuid.uuid4().hex[:8]}.csv"
            with open(temp_path, "w", encoding="utf-8") as f:
                f.write(file_path)
            file_path = temp_path

        df = _read_csv_clean(file_path)
        
        diagnosis_report = {
            "problematic_columns": [],
            "mixed_type_cells": [],
            "non_numeric_in_numeric_columns": []
        }
        
        print(f"ğŸ” æ­£åœ¨è¯Šæ–­æ–‡ä»¶: {file_path}")
        print(f"ğŸ“Š æ•°æ®å½¢çŠ¶: {df.shape}")
        
        for col in df.columns:
            print(f"\nğŸ“‹ æ£€æŸ¥åˆ—: {col}")
            
            # æ£€æŸ¥è¯¥åˆ—æ˜¯å¦åº”è¯¥æ˜¯æ•°å€¼åˆ—
            is_expected_numeric = any(keyword in col.lower() for keyword in 
                                    ['ratio', 'temperature', 'target', 'temp', 'conc', 'concentration'])
            
            if is_expected_numeric:
                # å°è¯•è½¬æ¢ä¸ºæ•°å€¼
                numeric_conversion = pd.to_numeric(df[col], errors='coerce')
                failed_indices = df[numeric_conversion.isna() & df[col].notna()].index.tolist()
                
                if failed_indices:
                    diagnosis_report["problematic_columns"].append(col)
                    problematic_values = []
                    
                    for idx in failed_indices[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªé—®é¢˜å€¼
                        problematic_values.append({
                            "row": idx + 1,  # Excelè¡Œå·ä»1å¼€å§‹
                            "value": repr(df.iloc[idx][col]),
                            "type": type(df.iloc[idx][col]).__name__
                        })
                    
                    diagnosis_report["non_numeric_in_numeric_columns"].append({
                        "column": col,
                        "problematic_count": len(failed_indices),
                        "total_count": len(df),
                        "examples": problematic_values
                    })
                    
                    print(f"âŒ å‘ç° {len(failed_indices)} ä¸ªéæ•°å€¼æ¡ç›®åœ¨æ•°å€¼åˆ— '{col}' ä¸­")
                    for example in problematic_values:
                        print(f"   è¡Œ {example['row']}: {example['value']} (ç±»å‹: {example['type']})")
            
            # æ£€æŸ¥æ··åˆç±»å‹
            unique_types = df[col].dropna().apply(type).unique()
            if len(unique_types) > 1:
                diagnosis_report["mixed_type_cells"].append({
                    "column": col,
                    "types_found": [t.__name__ for t in unique_types]
                })
                print(f"âš ï¸ åˆ— '{col}' åŒ…å«æ··åˆæ•°æ®ç±»å‹: {[t.__name__ for t in unique_types]}")
        
        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        if diagnosis_report["problematic_columns"]:
            return f"""
ğŸš¨ **æ•°æ®ç±»å‹è¯Šæ–­ç»“æœ**

âŒ **å‘ç°é—®é¢˜åˆ—**: {len(diagnosis_report["problematic_columns"])} ä¸ª
{chr(10).join([f"   - {col}" for col in diagnosis_report["problematic_columns"]])}

ğŸ“‹ **è¯¦ç»†é—®é¢˜**:
{chr(10).join([f"â€¢ åˆ— '{item['column']}': {item['problematic_count']}/{item['total_count']} ä¸ªéæ•°å€¼æ¡ç›®" 
              for item in diagnosis_report["non_numeric_in_numeric_columns"]])}

ğŸ’¡ **ä¿®å¤å»ºè®®**:
1. æ£€æŸ¥CSVæ–‡ä»¶ä¸­ä¸Šè¿°è¡Œçš„æ•°æ®
2. ç¡®ä¿æ¯”ä¾‹ã€æ¸©åº¦ã€ç›®æ ‡å€¼åˆ—åªåŒ…å«æ•°å­—
3. ç§»é™¤æˆ–ä¿®æ­£éæ•°å€¼æ¡ç›®ï¼ˆå¦‚æ–‡æœ¬ã€ç©ºæ ¼ã€ç‰¹æ®Šå­—ç¬¦ï¼‰
4. ä½¿ç”¨Excelæˆ–æ–‡æœ¬ç¼–è¾‘å™¨æŸ¥çœ‹åŸå§‹CSVæ–‡ä»¶

ğŸ”§ **å…·ä½“æ£€æŸ¥ä½ç½®**:
{chr(10).join([f"åˆ— '{item['column']}':" + chr(10) + chr(10).join([f"   è¡Œ {ex['row']}: {ex['value']}" for ex in item['examples']]) 
              for item in diagnosis_report["non_numeric_in_numeric_columns"]])}
            """
        else:
            return "âœ… æ•°æ®ç±»å‹æ£€æŸ¥é€šè¿‡ï¼Œæ²¡æœ‰å‘ç°æ˜æ˜¾çš„ç±»å‹é—®é¢˜ã€‚"
            
    except Exception as e:
        return f"è¯Šæ–­è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}"


def _extract_smiles_name_mapping(df: pd.DataFrame) -> dict:
    """
    ä» DataFrame ä¸­æå– SMILES -> åç§° çš„æ˜ å°„ï¼ˆä½¿ç”¨åŸå§‹ SMILESï¼‰
    
    Returns:
        dict: {smiles_string: friendly_name, ...}
    """
    mapping = {}
    
    # æ‰¾æ‰€æœ‰å¯èƒ½çš„åˆ†å­åˆ—ï¼ˆ*_molecule æˆ– *_SMILE/SMILESï¼‰
    for col in df.columns:
        is_molecule_col = col.endswith("_molecule") or 'SMILE' in col.upper()
        if not is_molecule_col:
            continue
        
        # ç¡®å®šå‰ç¼€ä»¥ä¾¿æŸ¥æ‰¾å¯¹åº”çš„ name åˆ—
        if col.endswith("_molecule"):
            prefix = col.rsplit("_molecule", 1)[0]
        elif '_SMILE' in col.upper():
            prefix = col.split('_')[0] if '_' in col else col.replace('SMILES', '').replace('SMILE', '')
        else:
            continue
        
        # æŸ¥æ‰¾å¯¹åº”çš„ name åˆ—
        name_col = None
        for candidate in [f"{prefix}_name", f"{prefix}name", prefix]:
            if candidate in df.columns and candidate != col:
                name_col = candidate
                break
        
        if name_col is None:
            continue
        
        # æå–éç©ºçš„ (smiles, name) å¯¹
        for idx, row in df.iterrows():
            smiles = row[col]
            name = row[name_col]
            
            if pd.isna(smiles) or pd.isna(name):
                continue
            
            smiles_str = str(smiles).strip()
            name_str = str(name).strip()
            
            if not smiles_str or not name_str:
                continue
            
            if smiles_str not in mapping:
                mapping[smiles_str] = name_str
    
    print(f"[DEBUG] Extracted SMILES-to-name mapping (raw): {len(mapping)} entries")
    return mapping


def _extract_smiles_name_mapping_with_canonical(df: pd.DataFrame, canonical_mapping: dict) -> dict:
    """
    ä» DataFrame ä¸­æå– SMILES -> åç§° çš„æ˜ å°„ï¼Œä½¿ç”¨è§„èŒƒåŒ–åçš„ SMILES ä½œä¸ºé”®
    
    è¿™æ ·å¯ä»¥ç¡®ä¿ BayBE æ¨èä¸­ä½¿ç”¨çš„è§„èŒƒåŒ– SMILES èƒ½æ­£ç¡®åŒ¹é…åˆ°åŒ–åˆç‰©åç§°ã€‚
    
    Args:
        df: åŸå§‹æ•°æ® DataFrame
        canonical_mapping: åŸå§‹ SMILES â†’ è§„èŒƒåŒ– SMILES çš„æ˜ å°„
        
    Returns:
        dict: {canonical_smiles: friendly_name, ...}
    """
    # é¦–å…ˆæå–åŸå§‹ SMILES â†’ åç§°æ˜ å°„
    raw_mapping = {}
    
    print(f"[DEBUG] _extract_smiles_name_mapping_with_canonical: DataFrame columns = {list(df.columns)}")
    
    for col in df.columns:
        col_upper = col.upper()
        is_molecule_col = col.endswith("_molecule") or 'SMILE' in col_upper
        if not is_molecule_col:
            continue
        
        # ç¡®å®šå‰ç¼€ä»¥ä¾¿æŸ¥æ‰¾å¯¹åº”çš„ name åˆ—
        if col.endswith("_molecule"):
            prefix = col.rsplit("_molecule", 1)[0]
        elif '_SMILE' in col_upper:
            # å¤„ç† SubstanceA_SMILE æˆ– SubstanceA_SMILES æ ¼å¼
            # æ‰¾åˆ° _SMILE çš„ä½ç½®å¹¶æˆªå–å‰ç¼€
            idx = col_upper.find('_SMILE')
            prefix = col[:idx] if idx > 0 else col.split('_')[0]
        else:
            continue
        
        # æŸ¥æ‰¾å¯¹åº”çš„ name åˆ—ï¼ˆæ”¯æŒå¤§å°å†™å˜ä½“ï¼‰
        name_col = None
        # å°è¯•å¤šç§å¯èƒ½çš„åˆ—åæ ¼å¼
        candidates = [
            f"{prefix}_name",      # SubstanceA_name
            f"{prefix}_NAME",      # SubstanceA_NAME
            f"{prefix}_Name",      # SubstanceA_Name
            f"{prefix}name",       # SubstanceAname
            f"{prefix}NAME",       # SubstanceANAME
        ]
        
        for candidate in candidates:
            if candidate in df.columns and candidate != col:
                name_col = candidate
                break
        
        # å¦‚æœè¿˜æ²¡æ‰¾åˆ°ï¼Œå°è¯•ä¸åŒºåˆ†å¤§å°å†™åŒ¹é…
        if name_col is None:
            for df_col in df.columns:
                if df_col.upper() == f"{prefix.upper()}_NAME" and df_col != col:
                    name_col = df_col
                    break
        
        if name_col is None:
            print(f"[DEBUG] No name column found for {col}, prefix={prefix}, tried: {candidates[:3]}")
            continue
        
        print(f"[DEBUG] Found SMILES-name pair: {col} -> {name_col}")
        
        # æå–éç©ºçš„ (smiles, name) å¯¹
        for idx, row in df.iterrows():
            smiles = row[col]
            name = row[name_col]
            
            if pd.isna(smiles) or pd.isna(name):
                continue
            
            smiles_str = str(smiles).strip()
            name_str = str(name).strip()
            
            if not smiles_str or not name_str:
                continue
            
            if smiles_str not in raw_mapping:
                raw_mapping[smiles_str] = name_str
    
    print(f"[DEBUG] Raw SMILES-to-name mapping: {len(raw_mapping)} entries")
    
    # è½¬æ¢ä¸ºè§„èŒƒåŒ– SMILES ä½œä¸ºé”®
    canonical_name_mapping = {}
    
    for original_smiles, name in raw_mapping.items():
        # æŸ¥æ‰¾è§„èŒƒåŒ–åçš„ SMILES
        canonical_smiles = canonical_mapping.get(original_smiles, original_smiles)
        
        if canonical_smiles not in canonical_name_mapping:
            canonical_name_mapping[canonical_smiles] = name
            
        # åŒæ—¶ä¿ç•™åŸå§‹ SMILES ä½œä¸ºé”®ï¼ˆä»¥é˜²è§„èŒƒåŒ–å¤±è´¥ï¼‰
        if original_smiles not in canonical_name_mapping:
            canonical_name_mapping[original_smiles] = name
    
    print(f"[DEBUG] Canonical SMILES-to-name mapping: {len(canonical_name_mapping)} entries")
    
    # æ‰“å°å‡ ä¸ªç¤ºä¾‹ä¾¿äºè°ƒè¯•
    for i, (k, v) in enumerate(canonical_name_mapping.items()):
        if i < 3:
            print(f"[DEBUG]   Example {i+1}: '{k[:30]}...' -> '{v}'")
    
    return canonical_name_mapping


def _resolve_file_path_or_content(file_path: str, state: dict, session_id: str) -> str:
    """
    æ™ºèƒ½å¤„ç†æ–‡ä»¶è·¯å¾„ vs æ–‡ä»¶å†…å®¹
    
    æœ‰æ—¶ LLM ä¼šä¼ é€’ CSV å†…å®¹è€Œä¸æ˜¯æ–‡ä»¶è·¯å¾„ï¼Œè¿™ä¸ªå‡½æ•°ä¼šï¼š
    1. ä¼˜å…ˆä½¿ç”¨ state ä¸­å·²ä¿å­˜çš„ current_data_path
    2. å¦‚æœ file_path æ˜¯å®é™…è·¯å¾„ï¼Œç›´æ¥ä½¿ç”¨
    3. å¦‚æœ file_path æ˜¯ CSV å†…å®¹ï¼Œå†™å…¥ä¸´æ—¶æ–‡ä»¶
    
    Returns:
        str: æœ‰æ•ˆçš„æ–‡ä»¶è·¯å¾„ï¼Œæˆ–ä»¥ "Error:" å¼€å¤´çš„é”™è¯¯æ¶ˆæ¯
    """
    import os
    import uuid
    
    # ç­–ç•¥1: ä¼˜å…ˆä½¿ç”¨ state ä¸­çš„ current_data_path
    current_data_path = state.get("current_data_path")
    if current_data_path and os.path.exists(current_data_path):
        print(f"[DEBUG] Using current_data_path from state: {current_data_path}")
        return current_data_path
    
    # ç­–ç•¥2: æ£€æŸ¥ file_path æ˜¯å¦æ˜¯æœ‰æ•ˆçš„æ–‡ä»¶è·¯å¾„
    if os.path.exists(file_path):
        print(f"[DEBUG] Using provided file_path: {file_path}")
        return file_path
    
    # ç­–ç•¥3: æ£€æŸ¥ file_path æ˜¯å¦æ˜¯ CSV å†…å®¹ï¼ˆåŒ…å«é€—å·å’Œæ¢è¡Œç¬¦ï¼‰
    if ',' in file_path and '\n' in file_path:
        print(f"[DEBUG] Detected CSV content instead of file path, writing to temp file...")
        
        # ç¡®å®šä¿å­˜ç›®å½•
        session_dir = state.get("session_dir")
        if session_dir and os.path.exists(session_dir):
            temp_file_path = os.path.join(session_dir, f"temp_uploaded_{uuid.uuid4().hex[:8]}.csv")
        else:
            temp_file_path = f"temp_uploaded_{uuid.uuid4().hex[:8]}.csv"
        
        try:
            with open(temp_file_path, 'w', encoding='utf-8') as f:
                f.write(file_path)
            print(f"[DEBUG] CSV content written to: {temp_file_path}")
            
            # æ›´æ–° state ä¸­çš„è·¯å¾„
            state["current_data_path"] = temp_file_path
            
            return temp_file_path
        except Exception as e:
            return f"Error: æ— æ³•å†™å…¥ä¸´æ—¶æ–‡ä»¶: {str(e)}"
    
    # ç­–ç•¥4: æ— æ³•è¯†åˆ«çš„è¾“å…¥
    return f"Error: æ— æ•ˆçš„æ–‡ä»¶è·¯å¾„ '{file_path[:50]}...'ã€‚è¯·æä¾›æœ‰æ•ˆçš„CSVæ–‡ä»¶è·¯å¾„æˆ–ç¡®ä¿æ–‡ä»¶å·²ä¸Šä¼ ã€‚"


def enhanced_verification(file_path: str, tool_context: ToolContext) -> str:
    """
    Enhanced Verification Agent çš„ä¸»è¦å·¥å…·å‡½æ•°
    å®ç°7ä¸ªæ ¸å¿ƒä»»åŠ¡ï¼š
    1. æ•°æ®è´¨é‡éªŒè¯
    2. SMILESéªŒè¯  
    3. æ™ºèƒ½å‚æ•°å»ºè®®
    4. è‡ªå®šä¹‰ç¼–ç å¤„ç†
    5. ç”¨æˆ·äº¤äº’
    6. å‚æ•°é…ç½®
    7. æ•°æ®æ ‡å‡†åŒ–
    """
    state = tool_context.state
    session_id = state.get("session_id", "unknown")
    
    try:
        # ===== æ™ºèƒ½å¤„ç†æ–‡ä»¶è·¯å¾„ vs æ–‡ä»¶å†…å®¹ =====
        # æœ‰æ—¶ LLM ä¼šä¼ é€’ CSV å†…å®¹è€Œä¸æ˜¯æ–‡ä»¶è·¯å¾„
        actual_file_path = _resolve_file_path_or_content(file_path, state, session_id)
        
        if actual_file_path.startswith("Error:"):
            return actual_file_path
        
        print(f"[DEBUG] enhanced_verification: using file_path = {actual_file_path}")
        
        # ===== ä»»åŠ¡1: æ•°æ®è´¨é‡éªŒè¯ =====
        quality_report = _perform_data_quality_check(actual_file_path)
        
        if not quality_report["is_valid"]:
            return f"æ•°æ®è´¨é‡æ£€æŸ¥å¤±è´¥ï¼š\n{json.dumps(quality_report, indent=2, ensure_ascii=False)}"
        
        # ===== ä»»åŠ¡2: SMILESéªŒè¯ =====
        df = _read_csv_clean(actual_file_path)
        suspicious_headers = _detect_suspicious_headers(df)
        if suspicious_headers:
            suspicious_preview = "\n".join([f"- {h}" for h in suspicious_headers[:5]])
            _reset_verification_state(state, "header_contamination")
            state.pop("smiles_to_name_map", None)
            state.pop("original_data_format", None)
            state.pop("standardized_data_path", None)
            return (
                "æ•°æ®è¡¨å¤´ç–‘ä¼¼è¢«è¯´æ˜æ–‡å­—æ±¡æŸ“ï¼Œå¯¼è‡´åˆ—é”™ä½/ç±»å‹é”™è¯¯ã€‚\n"
                "æ£€æµ‹åˆ°ä»¥ä¸‹å¯ç–‘åˆ—åï¼š\n"
                f"{suspicious_preview}\n\n"
                "è¯·ä½¿ç”¨æ ‡å‡†æ¨¡æ¿é‡æ–°å¯¼å…¥ CSVï¼Œç¡®ä¿è¡¨å¤´åªåŒ…å«å­—æ®µåï¼ˆä¸è¦åŒ…å«ç›®æ ‡æè¿°ã€å‚æ•°èŒƒå›´æˆ–çº¦æŸè¯´æ˜ï¼‰ã€‚"
            )
        smiles_validator = SimplifiedSMILESValidator()
        # ç¬¬ä¸€æ¬¡éªŒè¯ï¼ˆç”¨äºè¯†åˆ«æ— æ•ˆSMILESï¼‰
        smiles_validation_initial = smiles_validator.validate_smiles_data(df)

        # åœ¨è¿­ä»£å¼€å§‹å‰ï¼Œå°è¯•æ ¹æ®åç§°è‡ªåŠ¨çº æ­£æ— æ•ˆSMILES
        auto_corrections = _auto_correct_invalid_smiles(df, smiles_validation_initial)
        if auto_corrections:
            # è®°å½•åˆ°çŠ¶æ€ä¸­ï¼Œä¾¿äºåç»­è°ƒè¯•å’Œå‘ç”¨æˆ·è§£é‡Š
            state["smiles_autocorrections"] = auto_corrections
            print(f"[DEBUG] è‡ªåŠ¨çº æ­£SMILESå®Œæˆ: {len(auto_corrections)} æ¡")

        # ä½¿ç”¨ä¿®æ­£åçš„æ•°æ®é‡æ–°éªŒè¯SMILESï¼Œè·å–æœ€ç»ˆçš„è§„èŒƒåŒ–æ˜ å°„
        smiles_validation = smiles_validator.validate_smiles_data(df)
        
        # ===== ä¿å­˜åŸå§‹è¡¨æ ¼æ ¼å¼ï¼ˆç”¨äºåç»­æ¨èè¡¨æ ¼å¤åˆ»ï¼‰ =====
        original_column_order = list(df.columns)
        original_column_types = {col: str(df[col].dtype) for col in df.columns}
        state["original_data_format"] = {
            "column_order": original_column_order,
            "column_types": original_column_types,
            "sample_row": df.iloc[0].to_dict() if len(df) > 0 else {}
        }
        print(f"[DEBUG] Saved original data format: {len(original_column_order)} columns")
        
        # ===== æå– SMILES â†’ åç§°æ˜ å°„ï¼ˆç”¨äºåç»­æ¨èæ˜¾ç¤ºï¼‰ =====
        # ä½¿ç”¨è§„èŒƒåŒ–åçš„ SMILES ä½œä¸ºé”®ï¼Œå› ä¸º BayBE æ¨èä½¿ç”¨çš„æ˜¯è§„èŒƒåŒ– SMILES
        smiles_to_name_map = _extract_smiles_name_mapping_with_canonical(
            df, smiles_validation.get("canonical_smiles_mapping", {})
        )
        state["smiles_to_name_map"] = smiles_to_name_map
        print(f"[DEBUG] SMILES-to-name mapping saved to state: {len(smiles_to_name_map)} entries")
        
        # åœ¨äº¤äº’æ•°æ®ä¸­è®°å½•è‡ªåŠ¨çº æ­£çš„SMILESï¼ˆä»…ç”¨äºå‘ç”¨æˆ·è¯´æ˜ï¼Œä¸å½±å“åç»­è®¡ç®—ï¼‰
        if auto_corrections:
            # ç®€è¦å½¢å¼ï¼šåªä¿ç•™ç‰©è´¨åå’Œè¡Œå·ï¼Œé¿å…æç¤ºè¿‡é•¿
            try:
                from copy import deepcopy
                # åªæå–å…³é”®ä¿¡æ¯
                simple_corrections = [
                    {
                        "substance": c.get("substance"),
                        "row": c.get("row"),
                        "original_smiles": c.get("original_smiles"),
                        "corrected_smiles": c.get("corrected_smiles"),
                    }
                    for c in auto_corrections
                ]
            except Exception:
                simple_corrections = auto_corrections

        # ===== ä»»åŠ¡3: æ™ºèƒ½å‚æ•°å»ºè®® =====
        parameter_advisor = IntelligentParameterAdvisor()
        parameter_suggestions = parameter_advisor.analyze_experimental_context(df)
        
        # ===== ä»»åŠ¡4: ç”¨æˆ·å®šä¹‰ç¼–ç è¯†åˆ« =====
        encoding_handler = UserDefinedEncodingHandler()
        
        # æ™ºèƒ½åˆ†ç±»æ‰€æœ‰ç”¨æˆ·åˆ—
        column_classification = encoding_handler.classify_user_columns(df)
        
        # è¯†åˆ«ç”¨æˆ·æä¾›çš„ç‰¹æ®Šç‰©è´¨å’Œç¼–ç ä¿¡æ¯
        user_special_data = encoding_handler.identify_user_special_substances(df)
        
        # ä¸ºç‰¹æ®Šç‰©è´¨åˆ›å»ºBayBEå‚æ•°
        special_parameters = encoding_handler.create_baybe_parameters_for_special_substances(user_special_data, df)
        
        # æ•´ç†ç¼–ç ä¿¡æ¯ç”¨äºåç»­å¤„ç†
        custom_encodings = {
            "column_classification": column_classification,
            "user_special_substances": user_special_data,
            "baybe_parameters": special_parameters,
            "encoding_strategy": "user_defined"  # æ ‡æ˜è¿™æ˜¯ç”¨æˆ·å®šä¹‰çš„ç¼–ç 
        }
        
        # ===== ä»»åŠ¡5 & 6: ç”¨æˆ·äº¤äº’å’Œå‚æ•°é…ç½®å‡†å¤‡ =====
        # å‡†å¤‡ç”¨æˆ·äº¤äº’æ‰€éœ€çš„ä¿¡æ¯
        user_interaction_data = _prepare_user_interaction_data(
            df, quality_report, smiles_validation, parameter_suggestions, custom_encodings
        )
        # å°†è‡ªåŠ¨çº æ­£ä¿¡æ¯æ³¨å…¥åˆ° smiles_status ä¸­ï¼Œä¾›æç¤ºä½¿ç”¨
        if auto_corrections:
            user_interaction_data.setdefault("smiles_status", {})
            user_interaction_data["smiles_status"]["autocorrections"] = simple_corrections
        
        # ===== ä»»åŠ¡7: æ•°æ®æ ‡å‡†åŒ– =====
        standardized_data = _standardize_data(df, smiles_validation)
        
        # ===== åˆå§‹åŒ–ç»Ÿä¸€çš„å®éªŒè®°å½•è¡¨ =====
        session_dir = state.get("session_dir", ".")
        unified_experiment_log = os.path.join(session_dir, "experiment_log.csv")
        
        # å¦‚æœç»Ÿä¸€è®°å½•è¡¨ä¸å­˜åœ¨ï¼Œä½¿ç”¨åŸå§‹æ•°æ®ä½œä¸ºåˆå§‹è®°å½•
        if not os.path.exists(unified_experiment_log):
            # æ·»åŠ è½®æ¬¡æ ‡è®°åˆ—
            df_with_round = df.copy()
            df_with_round["optimization_round"] = 0
            df_with_round["experiment_status"] = "completed"  # åˆå§‹æ•°æ®éƒ½æ˜¯å·²å®Œæˆçš„
            df_with_round.to_csv(unified_experiment_log, index=False, encoding="utf-8-sig")
            print(f"[DEBUG] Created unified experiment log: {unified_experiment_log}")
        else:
            print(f"[DEBUG] Unified experiment log already exists: {unified_experiment_log}")
        
        state["unified_experiment_log_path"] = unified_experiment_log
        
        # ä¿å­˜çŠ¶æ€ä¿¡æ¯
        state["verification_results"] = {
            "quality_report": quality_report,
            "smiles_validation": smiles_validation,
            "parameter_suggestions": parameter_suggestions,
            "custom_encodings": custom_encodings,
            "standardized_data_path": f"standardized_data_{session_id}.csv",
            "ready_for_user_interaction": True
        }
        
        # ä¿å­˜æ ‡å‡†åŒ–æ•°æ®
        output_path = f"standardized_data_{session_id}.csv"
        standardized_data.to_csv(output_path, index=False)
        
        # ç”Ÿæˆç”¨æˆ·äº¤äº’æç¤º
        return _generate_user_interaction_prompt(user_interaction_data)
        
    except Exception as e:
        return f"Enhanced Verification å¤„ç†é”™è¯¯: {str(e)}"


def _perform_data_quality_check(file_path: str) -> dict:
    """
    æ‰§è¡Œæ•°æ®è´¨é‡æ£€æŸ¥ï¼ˆä»»åŠ¡1ï¼‰
    """
    try:
        df = _read_csv_clean(file_path)
        
        quality_report = {
            "is_valid": True,
            "issues": [],
            "statistics": {
                "total_rows": int(len(df)),
                "total_columns": int(len(df.columns)),
                "missing_percentage": float((df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100)
            }
        }
        
        # æ£€æŸ¥ç¼ºå¤±å€¼
        missing_cols = df.isnull().sum()
        high_missing_cols = missing_cols[missing_cols > len(df) * 0.5].index.tolist()
        if high_missing_cols:
            quality_report["issues"].append({
                "type": "high_missing_data",
                "columns": high_missing_cols,
                "severity": "warning"
            })
        
        # æ£€æŸ¥å¿…éœ€åˆ—
        required_patterns = ['Substance', 'SMILE', 'Target_']
        for pattern in required_patterns:
            matching_cols = [col for col in df.columns if pattern in str(col)]
            if not matching_cols:
                quality_report["issues"].append({
                    "type": "missing_required_columns",
                    "pattern": pattern,
                    "severity": "error"
                })
                quality_report["is_valid"] = False
        
        # æ£€æŸ¥æ•°å€¼åˆ—çš„å¼‚å¸¸å€¼
        for col in df.columns:
            # å°è¯•å°†åˆ—è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
            numeric_data = pd.to_numeric(df[col], errors='coerce')
            # åªå¤„ç†è‡³å°‘æœ‰ä¸€äº›æœ‰æ•ˆæ•°å€¼çš„åˆ—
            if numeric_data.notna().sum() < len(df) * 0.1:  # å¦‚æœæœ‰æ•ˆæ•°å€¼å°‘äº10%ï¼Œè·³è¿‡
                continue
                
            # ä½¿ç”¨æ¸…ç†åçš„æ•°å€¼æ•°æ®è®¡ç®—ç»Ÿè®¡é‡
            clean_data = numeric_data.dropna()
            if len(clean_data) < 2:  # éœ€è¦è‡³å°‘2ä¸ªå€¼æ¥è®¡ç®—IQR
                continue
                
            Q1 = clean_data.quantile(0.25)
            Q3 = clean_data.quantile(0.75)
            IQR = Q3 - Q1
            outliers = clean_data[(clean_data < Q1 - 1.5*IQR) | (clean_data > Q3 + 1.5*IQR)]
            
            if not outliers.empty:
                quality_report["issues"].append({
                    "type": "outliers_detected",
                    "column": col,
                    "count": int(len(outliers)),
                    "severity": "info"
                })
        
        return quality_report
        
    except Exception as e:
        return {
            "is_valid": False,
            "error": str(e),
            "issues": [{"type": "file_read_error", "severity": "error"}]
        }


def _prepare_user_interaction_data(df, quality_report, smiles_validation, parameter_suggestions, custom_encodings):
    """
    å‡†å¤‡ç”¨æˆ·äº¤äº’æ‰€éœ€çš„æ•°æ®ï¼ˆä»»åŠ¡5æ”¯æŒï¼‰
    """
    # è¯†åˆ«ç›®æ ‡å˜é‡
    target_columns = [col for col in df.columns if col.startswith('Target_')]
    
    # è¯†åˆ«å¯è°ƒå˜é‡
    adjustable_vars = []
    ratio_cols = [col for col in df.columns if 'ratio' in col.lower()]
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    adjustable_vars = list(set(ratio_cols + numeric_cols) - set(target_columns))
    
    interaction_data = {
        "data_summary": {
            "total_experiments": len(df),
            "substances_count": len([col for col in df.columns if 'Substance' in col and 'name' in col]),
            "targets_count": len(target_columns),
            "adjustable_variables_count": len(adjustable_vars)
        },
        "target_variables": target_columns,
        "adjustable_variables": adjustable_vars,
        "parameter_suggestions": parameter_suggestions,
        "smiles_status": {
            "valid_smiles": len(smiles_validation["canonical_smiles_mapping"]),
            "invalid_smiles": len(smiles_validation["invalid_smiles"]),
            "invalid_smiles_details": smiles_validation["invalid_smiles"],  # æ·»åŠ è¯¦ç»†çš„æ— æ•ˆSMILESä¿¡æ¯
            "substances_validated": smiles_validation["substances_validated"],
            # è¿™é‡Œå…ˆå ä½ï¼Œå…·ä½“å†…å®¹åœ¨ enhanced_verification ä¸­æ ¹æ® state['smiles_autocorrections'] å¡«å……
            "autocorrections": []
        },
        "special_molecules": custom_encodings,
        "quality_score": 100 - quality_report["statistics"]["missing_percentage"]
    }
    
    return interaction_data


def _standardize_data(df: pd.DataFrame, smiles_validation: dict) -> pd.DataFrame:
    """
    æ•°æ®æ ‡å‡†åŒ–å¤„ç†ï¼ˆä»»åŠ¡7ï¼‰
    """
    standardized_df = df.copy()
    
    # 1. æ›¿æ¢ä¸ºè§„èŒƒåŒ–SMILES
    smiles_columns = [col for col in df.columns if 'SMILE' in col.upper()]
    for col in smiles_columns:
        for original_smiles, canonical_smiles in smiles_validation["canonical_smiles_mapping"].items():
            standardized_df[col] = standardized_df[col].replace(original_smiles, canonical_smiles)
    
    # 2. å®‰å…¨çš„æ•°æ®ç±»å‹æ ‡å‡†åŒ–å’Œç¼ºå¤±å€¼å¤„ç†
    for col in standardized_df.columns:
        if col not in smiles_columns:  # è·³è¿‡SMILESåˆ—
            # å°è¯•è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
            numeric_data = pd.to_numeric(standardized_df[col], errors='coerce')
            valid_numeric_ratio = numeric_data.notna().sum() / len(standardized_df)
            
            # å¦‚æœè‡³å°‘50%çš„æ•°æ®å¯ä»¥è½¬æ¢ä¸ºæ•°å€¼ï¼Œåˆ™è®¤ä¸ºè¿™æ˜¯æ•°å€¼åˆ—
            if valid_numeric_ratio >= 0.5:
                standardized_df[col] = numeric_data
                # ç”¨ä¸­ä½æ•°å¡«å……ç¼ºå¤±å€¼
                median_val = numeric_data.median()
                if not pd.isna(median_val):
                    standardized_df[col] = standardized_df[col].fillna(median_val)
                    
    # 3. ç‰¹å®šåˆ—ç±»å‹å¼ºåˆ¶è½¬æ¢
    for col in standardized_df.columns:
        if 'ratio' in col.lower() or 'temperature' in col.lower() or 'target' in col.lower():
            # å¼ºåˆ¶è½¬æ¢ä¸ºæ•°å€¼ï¼Œæ— æ•ˆå€¼è®¾ä¸ºNaN
            standardized_df[col] = pd.to_numeric(standardized_df[col], errors='coerce')
        elif col.startswith('Target_'):
            standardized_df[col] = pd.to_numeric(standardized_df[col], errors='coerce')
    
    return standardized_df


def _generate_user_interaction_prompt(interaction_data: dict) -> str:
    """
    ç”Ÿæˆç”¨æˆ·äº¤äº’æç¤ºï¼ˆä»»åŠ¡5ï¼‰
    """
    prompt = f"""
ğŸ” **æ•°æ®éªŒè¯å®Œæˆ - éœ€è¦æ‚¨çš„ä¼˜åŒ–ç›®æ ‡ç¡®è®¤**

ğŸ“Š **æ•°æ®æ¦‚è§ˆ**:
- å®éªŒæ•°é‡: {interaction_data['data_summary']['total_experiments']}
- ç‰©è´¨ç§ç±»: {interaction_data['data_summary']['substances_count']}
- ç›®æ ‡å˜é‡: {interaction_data['data_summary']['targets_count']}
- å¯è°ƒå˜é‡: {interaction_data['data_summary']['adjustable_variables_count']}
- æ•°æ®è´¨é‡è¯„åˆ†: {interaction_data['quality_score']:.1f}/100

ğŸ¯ **ç›®æ ‡å˜é‡**: {', '.join(interaction_data['target_variables'])}

ğŸ”§ **å¯è°ƒå˜é‡**: {', '.join(interaction_data['adjustable_variables'])}

ğŸ§ª **SMILESéªŒè¯çŠ¶æ€**:
- æœ‰æ•ˆåˆ†å­: {interaction_data['smiles_status']['valid_smiles']}
- æ— æ•ˆåˆ†å­: {interaction_data['smiles_status']['invalid_smiles']}
- å·²éªŒè¯ç‰©è´¨: {', '.join(interaction_data['smiles_status']['substances_validated'])}
"""
    # å¦‚æœæœ‰æ— æ•ˆSMILESï¼Œæ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
    invalid_smiles_details = interaction_data.get('smiles_status', {}).get('invalid_smiles_details', [])
    if invalid_smiles_details:
        prompt += "\nâš ï¸ **æ— æ•ˆSMILESè¯¦æƒ…**:\n"
        # æ˜¾ç¤ºæ‰€æœ‰æ— æ•ˆSMILESçš„è¯¦ç»†ä¿¡æ¯
        for i, invalid_item in enumerate(invalid_smiles_details, 1):
            if isinstance(invalid_item, dict):
                substance = invalid_item.get('substance', 'Unknown')
                row = invalid_item.get('row', 'N/A')
                smiles = invalid_item.get('smiles', 'N/A')
                error = invalid_item.get('error', 'æœªçŸ¥é”™è¯¯')
                prompt += f"  {i}. ç‰©è´¨: {substance}, è¡Œ: {row}, SMILES: `{smiles}`, é”™è¯¯: {error}\n"
            else:
                # å…¼å®¹æ—§æ ¼å¼ï¼ˆå­—ç¬¦ä¸²ï¼‰
                prompt += f"  {i}. {invalid_item}\n"
        prompt += "\nğŸ’¡ **å»ºè®®**: è¯·æ£€æŸ¥å¹¶ä¿®æ­£è¿™äº›æ— æ•ˆçš„SMILESå­—ç¬¦ä¸²ï¼Œç„¶åé‡æ–°ä¸Šä¼ æ•°æ®ã€‚\n"
    
    prompt += """

"""
    # å¦‚æœæœ‰è‡ªåŠ¨çº æ­£çš„SMILESï¼Œå‘ç”¨æˆ·æ˜ç¡®è¯´æ˜ï¼ˆä½†ä¸è¦æ±‚ç”¨æˆ·æ‰‹åŠ¨ä¿®æ”¹ï¼‰
    autocorrections = interaction_data.get('smiles_status', {}).get('autocorrections')
    if autocorrections:
        prompt += "\nğŸ§  **SMILESè‡ªåŠ¨çº æ­£è¯´æ˜**:\n"
        prompt += f"- ç³»ç»Ÿå·²æ ¹æ®åŒ–åˆç‰©åç§°è‡ªåŠ¨çº æ­£ {len(autocorrections)} æ¡SMILESï¼Œé¿å…å› å°çš„å½•å…¥é”™è¯¯ä¸­æ–­ä¼˜åŒ–æµç¨‹ã€‚\n"
        # åªå±•ç¤ºå‰å‡ æ¡ç¤ºä¾‹ï¼Œé¿å…ä¿¡æ¯è¿‡é•¿
        max_examples = 3
        for i, c in enumerate(autocorrections[:max_examples], 1):
            substance = c.get('substance', 'Unknown')
            row = c.get('row', 'N/A')
            orig = c.get('original_smiles', '')
            corrected = c.get('corrected_smiles', '')
            prompt += f"  - ç‰©è´¨ {substance}ï¼Œè¡Œ {row}: `{orig}` â†’ `{corrected}`\n"
        if len(autocorrections) > max_examples:
            prompt += f"  - å…¶ä½™ {len(autocorrections) - max_examples} æ¡å·²åŒæ ·è‡ªåŠ¨çº æ­£ã€‚\n"
        prompt += "  - æ‚¨æ— éœ€åœ¨CSVä¸­æ‰‹åŠ¨ä¿®æ”¹è¿™äº›ä½ç½®ï¼Œå¦‚éœ€æŸ¥çœ‹å®Œæ•´åˆ—è¡¨å¯æŸ¥çœ‹ç³»ç»Ÿæ—¥å¿—æˆ–è”ç³»å¼€å‘è€…ã€‚\n"

    prompt += "\nğŸ’¡ **æ™ºèƒ½å‚æ•°å»ºè®®**ï¼š"
    
    # æ·»åŠ å‚æ•°å»ºè®®è¯¦æƒ…ï¼ˆè·³è¿‡ä»¥_å¼€å¤´çš„ç‰¹æ®Šé”®å¦‚_reaction_infoï¼‰
    for param, suggestion in interaction_data['parameter_suggestions'].items():
        # è·³è¿‡å…ƒä¿¡æ¯é”®
        if param.startswith('_'):
            continue
        # ç¡®ä¿suggestionæ˜¯å­—å…¸ä¸”åŒ…å«å¿…è¦çš„é”®
        if not isinstance(suggestion, dict):
            continue
        if 'current_range' not in suggestion:
            continue
            
        prompt += f"\nğŸ“Œ **{param}**:"
        prompt += f"\n   - å½“å‰èŒƒå›´: {suggestion.get('current_range', 'N/A')}"
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºç¦»æ•£å‚æ•°
        if suggestion.get('type') == 'NumericalDiscreteParameter':
            discrete_values = suggestion.get('values', [])
            prompt += f"\n   - å‚æ•°ç±»å‹: ç¦»æ•£å‚æ•° (NumericalDiscreteParameter)"
            prompt += f"\n   - ç¦»æ•£å€¼: {discrete_values}"
            prompt += f"\n   - æ£€æµ‹ç†ç”±: {suggestion.get('discrete_reasoning', 'è‡ªåŠ¨æ£€æµ‹')}"
            
            # è¯´æ˜è¿™äº›å€¼æ¥è‡ªå®é™…æ•°æ®
            if suggestion.get('data_based', False):
                unique_count = suggestion.get('unique_count', len(discrete_values))
                total_count = suggestion.get('total_count', 'N/A')
                prompt += f"\n   - æ•°æ®æ¥æº: ä»å®é™…æ•°æ®ä¸­æ£€æµ‹åˆ° {unique_count} ä¸ªå”¯ä¸€å€¼ï¼ˆå…± {total_count} è¡Œæ•°æ®ï¼‰"
                prompt += f"\n   - æ³¨æ„: è¿™äº›ç¦»æ•£å€¼æ˜¯åŸºäºå½“å‰æ•°æ®è‡ªåŠ¨æ£€æµ‹çš„ï¼Œå¦‚æœæ•°æ®é‡è¾ƒå°‘ï¼Œå»ºè®®å€¼å¯èƒ½ä¸å®Œæ•´"
        else:
            prompt += f"\n   - å‚æ•°ç±»å‹: è¿ç»­å‚æ•° (NumericalContinuousParameter)"
            prompt += f"\n   - å»ºè®®èŒƒå›´: {suggestion.get('suggested_bounds', 'N/A')}"
            prompt += f"\n   - ç†ç”±: {suggestion.get('reasoning', 'N/A')}"
    
    # æ·»åŠ æ™ºèƒ½åˆ—åˆ†ç±»ç»“æœ
    if interaction_data['special_molecules'].get('column_classification'):
        classification = interaction_data['special_molecules']['column_classification']
        
        prompt += f"\n\nğŸ“‹ **æ•°æ®ç»“æ„åˆ†æ**:"
        
        # æ˜¾ç¤ºè¯†åˆ«çš„æ‰©å±•åˆ—
        if classification['è¯†åˆ«çš„æ‰©å±•åˆ—']:
            prompt += f"\nğŸ¯ **æ™ºèƒ½è¯†åˆ«çš„æ‰©å±•åˆ—ç±»å‹**:"
            for category, columns in classification['è¯†åˆ«çš„æ‰©å±•åˆ—'].items():
                if columns:
                    prompt += f"\n   ğŸ“Œ {category}:"
                    for col_info in columns:
                        confidence_str = f"({col_info['confidence']:.1%}ç½®ä¿¡åº¦)" if col_info['confidence'] > 0 else ""
                        prompt += f"\n      - {col_info['column_name']}: {col_info['actual_data_type']} {confidence_str}"
                        prompt += f"\n        æ ·æœ¬å€¼: {col_info['sample_values'][:3]}"
        
        # æ˜¾ç¤ºç‰¹æ®Šç‰©è´¨
        if interaction_data['special_molecules'].get('user_special_substances', {}).get('substances_without_smiles'):
            user_special = interaction_data['special_molecules']['user_special_substances']
            prompt += f"\n\nğŸ”¬ **è¯†åˆ«åˆ°æ‚¨çš„ç‰¹æ®Šç‰©è´¨**:"
            special_substances_summary = {}
            for substance in user_special['substances_without_smiles']:
                name = substance['name']
                if name not in special_substances_summary:
                    special_substances_summary[name] = []
                special_substances_summary[name].append(substance['row'])
            
            for name, rows in special_substances_summary.items():
                prompt += f"\n   - {name}: å‡ºç°åœ¨ {len(rows)} ä¸ªå®éªŒä¸­ï¼Œæ— SMILESï¼Œå°†ä½¿ç”¨åç§°ç¼–ç "
        
        # æ˜¾ç¤ºæœªåˆ†ç±»åˆ—å»ºè®®
        if classification['æœªåˆ†ç±»åˆ—']:
            prompt += f"\n\nâ“ **æœªåˆ†ç±»çš„åˆ—** (å¯èƒ½éœ€è¦æ‚¨çš„è¯´æ˜):"
            for col in classification['æœªåˆ†ç±»åˆ—']:
                prompt += f"\n   - {col}"
        
        # æ˜¾ç¤ºæ ‡å‡†æ ¼å¼å»ºè®®
        if classification['å»ºè®®çš„æ ‡å‡†æ ¼å¼']['å½“å‰æ•°æ®æ˜ å°„å»ºè®®']:
            prompt += f"\n\nğŸ’¡ **æ•°æ®æ ¼å¼ä¼˜åŒ–å»ºè®®**:"
            for current_col, suggestion in classification['å»ºè®®çš„æ ‡å‡†æ ¼å¼']['å½“å‰æ•°æ®æ˜ å°„å»ºè®®'].items():
                prompt += f"\n   - {suggestion}"
    
    prompt += f"""

â“ **è¯·å›ç­”ä»¥ä¸‹é—®é¢˜ä»¥å®Œæˆä¼˜åŒ–é…ç½®**:

1. **ä¼˜åŒ–ç›®æ ‡ç¡®è®¤**: 
   å¯¹äºæ¯ä¸ªç›®æ ‡å˜é‡ï¼Œè¯·æŒ‡å®šï¼š
   - ä¼˜åŒ–æ–¹å‘ (æœ€å¤§åŒ–/æœ€å°åŒ–/ç›®æ ‡å€¼åŒ¹é…)
   - æœŸæœ›çš„ç›®æ ‡å€¼èŒƒå›´

2. **å¤šç›®æ ‡ä¼˜åŒ–ç­–ç•¥** (å¦‚æœæœ‰å¤šä¸ªç›®æ ‡):
   ğŸ’¡ **æœŸæœ›åº¦æ–¹æ³• (desirability)**: 
      - é€‚ç”¨äºæ‚¨çŸ¥é“å„ç›®æ ‡çš„ç›¸å¯¹é‡è¦æ€§
      - éœ€è¦æŒ‡å®šå„ç›®æ ‡çš„æƒé‡ï¼ˆå¦‚50%:50%ï¼‰
      - è¿”å›å•ä¸€æœ€ä¼˜è§£
   
   ğŸ’¡ **å¸•ç´¯æ‰˜æ–¹æ³• (pareto)**: 
      - é€‚ç”¨äºç›®æ ‡ç›¸äº’å†²çªï¼ˆå¦‚å¼ºåº¦vså»¶å±•æ€§ï¼‰
      - ä¸éœ€è¦æŒ‡å®šæƒé‡
      - è¿”å›å¸•ç´¯æ‰˜å‰æ²¿ä¸Šçš„å¤šä¸ªæ–¹æ¡ˆä¾›æ‚¨é€‰æ‹©

3. **å‚æ•°è¾¹ç•Œç¡®è®¤**:
   æ˜¯å¦æ¥å—ä¸Šè¿°æ™ºèƒ½å»ºè®®çš„å‚æ•°èŒƒå›´ï¼Ÿå¦‚éœ€è°ƒæ•´è¯·è¯´æ˜ã€‚

4. **çº¦æŸæ¡ä»¶**:
   æ˜¯å¦æœ‰ç‰¹æ®Šçš„çº¦æŸæ¡ä»¶ï¼Ÿç³»ç»Ÿæ”¯æŒä»¥ä¸‹çº¦æŸç±»å‹ï¼š
   
   âœ… **æ”¯æŒçš„çº¦æŸç±»å‹**:
   - **ç´¯åŠ ç­‰äº**: `SubstanceA_ratio + SubstanceB_ratio = 1.0` (ç±»å‹: `sum_equals_one`)
   - **ç´¯åŠ å¤§äº**: `SubstanceA_ratio + SubstanceB_ratio >= 1.0` (ç±»å‹: `sum_greater_than`)
   - **ç´¯åŠ å°äº**: `SubstanceA_ratio + SubstanceB_ratio <= 0.8` (ç±»å‹: `sum_less_than`)
   - **çº¿æ€§ç­‰å¼**: `a1*x1 + a2*x2 = b` (ç±»å‹: `linear_equality`)
   - **çº¿æ€§ä¸ç­‰å¼**: `a1*x1 + a2*x2 >= b` æˆ– `<= b` (ç±»å‹: `linear_inequality`)
   
   ğŸ’¡ **ç¤ºä¾‹**:
   - "SubstanceA_ratio + SubstanceB_ratio å¿…é¡»å¤§äº 1.0"
   - "æ‰€æœ‰æ¯”ä¾‹ä¹‹å’Œå¿…é¡»ç­‰äº 1.0"
   - "SubstanceA_ratio + SubstanceB_ratio <= 0.8"

5. **è·å–å‡½æ•°ï¼ˆå¯é€‰ï¼‰**:
   - é»˜è®¤ä½¿ç”¨ BayBE å†…ç½®ç­–ç•¥
   - å¯é€‰é¡¹ç¤ºä¾‹: `qEI`, `qUCB`, `qNEI`, `qPI`
   - å¦‚ä¸ç¡®å®šå¯å›ç­” â€œé»˜è®¤â€

6. **æ¯”ä¾‹å’Œä¸º1çš„è‡ªåŠ¨çº¦æŸï¼ˆå¯é€‰ï¼‰**:
   - æ˜¯å¦å¯ç”¨è‡ªåŠ¨â€œæ¯”ä¾‹ä¹‹å’Œ = 1.0â€çš„çº¦æŸ
   - é»˜è®¤å¯ç”¨ï¼›å¦‚ä¸éœ€è¦å¯å›ç­”â€œå…³é—­â€

7. **å®éªŒè®¾è®¡å‚æ•°**:
   - è®¡åˆ’çš„å®éªŒæ‰¹æ¬¡å¤§å° (batch_size)
   - æœ€å¤§å®éªŒè½®æ•° (max_iterations)
   - é¢„ç®—çº¦æŸ (æ€»å®éªŒæ•°é‡é™åˆ¶)

è¯·æä¾›æ‚¨çš„å›ç­”ï¼Œæˆ‘å°†æ ¹æ®æ‚¨çš„éœ€æ±‚ç”Ÿæˆä¼˜åŒ–é…ç½®ã€‚

**ç¤ºä¾‹å›ç­”**:
"æœ€å¤§åŒ–Target_alpha_tgå’ŒTarget_gamma_elongationï¼Œä½¿ç”¨å¸•ç´¯æ‰˜æ–¹æ³•å› ä¸ºè¿™ä¸¤ä¸ªç›®æ ‡å¯èƒ½å†²çªã€‚æ¥å—å»ºè®®çš„å‚æ•°èŒƒå›´ã€‚æ²¡æœ‰å…¶ä»–çº¦æŸã€‚æ¯æ‰¹10ç»„å®éªŒï¼Œæœ€å¤š20è½®ï¼Œæ€»å…±200æ¬¡å®éªŒã€‚"
"""
    
    return prompt


# ä¸»è¦çš„å¢å¼ºéªŒè¯å·¥å…·å‡½æ•°
def collect_optimization_goals(
    targets: str,
    batch_size: int,
    max_iterations: int,
    total_budget: int,
    accept_suggested_parameters: bool,
    tool_context: ToolContext,
    optimization_strategy: str = "desirability",
    constraints: str = "[]",
    custom_parameter_bounds: str = "{}",
    acquisition_function: str = "default",
    auto_ratio_sum_constraint: bool = True
) -> str:
    """
    æ”¶é›†ç”¨æˆ·çš„ä¼˜åŒ–ç›®æ ‡å’Œé…ç½®ï¼ˆä»»åŠ¡5å’Œ6ï¼‰
    
    LLMè´Ÿè´£ç†è§£ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€è¾“å…¥å¹¶æå–ç»“æ„åŒ–å‚æ•°ï¼Œæ­¤å·¥å…·åªæ¥æ”¶ç»“æ„åŒ–æ•°æ®ã€‚
    
    Args:
        targets: JSONæ ¼å¼çš„ç›®æ ‡åˆ—è¡¨ï¼Œä¾‹å¦‚ï¼š
                 '[{"name": "Target_alpha_tg", "mode": "MAX", "weight": 0.5, "bounds": [0, 100]}, 
                   {"name": "Target_gamma_elongation", "mode": "MAX", "weight": 0.5, "bounds": [0, 100]}]'
                 modeå¯é€‰å€¼: "MAX"(æœ€å¤§åŒ–), "MIN"(æœ€å°åŒ–), "MATCH"(ç›®æ ‡å€¼åŒ¹é…)
                 weight: 0-1ä¹‹é—´çš„æƒé‡å€¼ï¼ˆä»…desirabilityç­–ç•¥éœ€è¦ï¼‰
                 bounds: ç›®æ ‡å€¼çš„è¾¹ç•ŒèŒƒå›´ï¼ˆdesirabilityç­–ç•¥å¿…éœ€ï¼‰
        
        batch_size: æ¯æ‰¹å®éªŒçš„æ•°é‡ï¼ˆå¦‚ç”¨æˆ·è¯´"åŒæ—¶å¼€å±•10ç»„å®éªŒ"åˆ™ä¸º10ï¼‰
        
        max_iterations: æœ€å¤§è¿­ä»£è½®æ•°ï¼ˆå¦‚ç”¨æˆ·è¯´"æœ€å¤§20è½®"åˆ™ä¸º20ï¼‰
        
        total_budget: æ€»å®éªŒæ¬¡æ•°é¢„ç®—ï¼ˆå¦‚ç”¨æˆ·è¯´"æ€»å…±200æ¬¡å®éªŒ"åˆ™ä¸º200ï¼‰
        
        accept_suggested_parameters: ç”¨æˆ·æ˜¯å¦æ¥å—ç³»ç»Ÿå»ºè®®çš„å‚æ•°èŒƒå›´ï¼ˆTrue/Falseï¼‰
        
        optimization_strategy: å¤šç›®æ ‡ä¼˜åŒ–ç­–ç•¥ï¼Œå¯é€‰å€¼ï¼š
            - "desirability": æœŸæœ›åº¦æ–¹æ³• - ä½¿ç”¨æƒé‡å°†å¤šç›®æ ‡åˆå¹¶ä¸ºå•ä¸€æ ‡é‡
                             é€‚ç”¨äºç”¨æˆ·æ˜ç¡®çŸ¥é“å„ç›®æ ‡ç›¸å¯¹é‡è¦æ€§çš„æƒ…å†µ
                             éœ€è¦æŒ‡å®šæ¯ä¸ªç›®æ ‡çš„weightå’Œbounds
            - "pareto": å¸•ç´¯æ‰˜æ–¹æ³• - æ¢ç´¢å¸•ç´¯æ‰˜å‰æ²¿ï¼Œè¿”å›æ‰€æœ‰éæ”¯é…è§£
                       é€‚ç”¨äºç›®æ ‡ç›¸äº’å†²çªã€ç”¨æˆ·æƒ³çœ‹æ‰€æœ‰æƒè¡¡æ–¹æ¡ˆçš„æƒ…å†µ
                       ä¸éœ€è¦æŒ‡å®šæƒé‡ï¼Œæ¨èç»“æœä¼šåˆ†å¸ƒåœ¨å¸•ç´¯æ‰˜å‰æ²¿ä¸Š
            é»˜è®¤ä¸º "desirability"
        
        constraints: JSONæ ¼å¼çš„çº¦æŸæ¡ä»¶åˆ—è¡¨ï¼Œæ”¯æŒçš„çº¦æŸç±»å‹ï¼š
                    
                    **1. sum_equals_one**: ç´¯åŠ ç­‰äºçº¦æŸ
                       '[{"type": "sum_equals_one", "parameters": ["SubstanceA_ratio", "SubstanceB_ratio"]}]'
                       è¡¨ç¤º: SubstanceA_ratio + SubstanceB_ratio = 1.0
                    
                    **2. sum_greater_than**: ç´¯åŠ å¤§äºç­‰äºçº¦æŸ
                       '[{"type": "sum_greater_than", "parameters": ["SubstanceA_ratio", "SubstanceB_ratio"], "threshold": 1.0}]'
                       è¡¨ç¤º: SubstanceA_ratio + SubstanceB_ratio >= 1.0
                    
                    **3. sum_less_than**: ç´¯åŠ å°äºç­‰äºçº¦æŸ
                       '[{"type": "sum_less_than", "parameters": ["SubstanceA_ratio", "SubstanceB_ratio"], "threshold": 0.8}]'
                       è¡¨ç¤º: SubstanceA_ratio + SubstanceB_ratio <= 0.8
                    
                    **4. linear_equality**: çº¿æ€§ç­‰å¼çº¦æŸ
                       '[{"type": "linear_equality", "parameters": ["x1", "x2"], "coefficients": [1.0, 2.0], "rhs": 1.0}]'
                       è¡¨ç¤º: 1.0*x1 + 2.0*x2 = 1.0
                    
                    **5. linear_inequality**: çº¿æ€§ä¸ç­‰å¼çº¦æŸ
                       '[{"type": "linear_inequality", "parameters": ["x1", "x2"], "coefficients": [1.0, 1.0], "rhs": 1.0, "operator": ">="}]'
                       è¡¨ç¤º: 1.0*x1 + 1.0*x2 >= 1.0 (operator å¯é€‰: ">=", "<=")
                    
                    å¦‚æœç”¨æˆ·è¯´"æ²¡æœ‰çº¦æŸ"ï¼Œåˆ™ä¸ºç©ºåˆ—è¡¨"[]"
        
        custom_parameter_bounds: JSONæ ¼å¼çš„è‡ªå®šä¹‰å‚æ•°è¾¹ç•Œï¼Œä¾‹å¦‚ï¼š
                                '{"SubstanceA_ratio": {"min": 0.5, "max": 0.9}}'
                                å¦‚æœç”¨æˆ·æ¥å—å»ºè®®çš„å‚æ•°èŒƒå›´ï¼Œåˆ™ä¸ºç©ºå¯¹è±¡"{}"

        acquisition_function: è·å–å‡½æ•°åå¥½ï¼Œå¯é€‰å€¼:
            - "default" (ä½¿ç”¨ BayBE é»˜è®¤ç­–ç•¥)
            - "qEI" / "qUCB" / "qNEI" / "qPI"

        auto_ratio_sum_constraint: æ˜¯å¦å¯ç”¨è‡ªåŠ¨â€œæ¯”ä¾‹ä¹‹å’Œ=1.0â€çº¦æŸï¼ˆé»˜è®¤ Trueï¼‰
        
        tool_context: ADKå·¥å…·ä¸Šä¸‹æ–‡
    
    Returns:
        é…ç½®å®Œæˆçš„ç¡®è®¤ä¿¡æ¯
    """
    state = tool_context.state
    verification_results = state.get("verification_results", {})
    
    # è°ƒè¯•ä¿¡æ¯
    print(f"\n[DEBUG] collect_optimization_goals state:")
    print(f"   verification_results exists: {bool(verification_results)}")
    if verification_results and isinstance(verification_results, dict):
        print(f"   verification_results keys: {list(verification_results.keys())}")
    
    try:
        # è§£æJSONå‚æ•°
        targets_list = json.loads(targets) if isinstance(targets, str) else targets
        constraints_list = json.loads(constraints) if isinstance(constraints, str) else constraints
        custom_bounds = json.loads(custom_parameter_bounds) if isinstance(custom_parameter_bounds, str) else custom_parameter_bounds
        
        # è°ƒè¯•ï¼šæ‰“å°è¾¹ç•Œä¿¡æ¯
        print(f"[DEBUG] collect_optimization_goals: custom_parameter_bounds (åŸå§‹): {custom_parameter_bounds}")
        print(f"[DEBUG] collect_optimization_goals: custom_bounds (è§£æå): {custom_bounds}")
        print(f"[DEBUG] collect_optimization_goals: custom_bounds ç±»å‹: {type(custom_bounds)}")
        if isinstance(custom_bounds, dict):
            print(f"[DEBUG] collect_optimization_goals: custom_bounds keys: {list(custom_bounds.keys())}")
            for key, value in custom_bounds.items():
                print(f"[DEBUG] collect_optimization_goals:   {key}: {value} (type: {type(value)})")
        
        # éªŒè¯ä¼˜åŒ–ç­–ç•¥
        valid_strategies = ["desirability", "pareto"]
        if optimization_strategy not in valid_strategies:
            optimization_strategy = "desirability"  # é»˜è®¤ä½¿ç”¨æœŸæœ›åº¦æ–¹æ³•
        
        # éªŒè¯ç›®æ ‡åˆ—è¡¨
        if not targets_list:
            return """
âŒ **é…ç½®é”™è¯¯**: æœªæä¾›ä¼˜åŒ–ç›®æ ‡

è¯·å‘Šè¯‰æˆ‘æ‚¨è¦ä¼˜åŒ–çš„ç›®æ ‡å˜é‡ï¼Œä¾‹å¦‚ï¼š
- "æˆ‘æƒ³æœ€å¤§åŒ– Target_alpha_tg å’Œ Target_gamma_elongation"
- "æœ€å°åŒ– Target_cost"

å¯ç”¨çš„ç›®æ ‡å˜é‡è¯·æŸ¥çœ‹ä¸Šæ–¹çš„æ•°æ®éªŒè¯ç»“æœã€‚

ğŸ’¡ **ä¼˜åŒ–ç­–ç•¥é€‰æ‹©**:
- å¦‚æœæ‚¨çŸ¥é“å„ç›®æ ‡çš„ç›¸å¯¹é‡è¦æ€§ï¼Œä½¿ç”¨ **desirability**ï¼ˆæœŸæœ›åº¦ï¼‰æ–¹æ³•å¹¶æŒ‡å®šæƒé‡
- å¦‚æœç›®æ ‡ç›¸äº’å†²çªä¸”æ‚¨æƒ³çœ‹æ‰€æœ‰å¯èƒ½çš„æƒè¡¡æ–¹æ¡ˆï¼Œä½¿ç”¨ **pareto**ï¼ˆå¸•ç´¯æ‰˜ï¼‰æ–¹æ³•
"""
        
        # æ ¹æ®ç­–ç•¥å¤„ç†ç›®æ ‡
        if optimization_strategy == "desirability":
            # æœŸæœ›åº¦æ–¹æ³•ï¼šéœ€è¦æƒé‡å’Œè¾¹ç•Œ
            # éªŒè¯å¹¶è§„èŒƒåŒ–ç›®æ ‡æƒé‡
            total_weight = sum(t.get("weight", 0) for t in targets_list)
            if abs(total_weight - 1.0) > 0.01:
                # è‡ªåŠ¨å½’ä¸€åŒ–æƒé‡
                for t in targets_list:
                    if total_weight > 0:
                        t["weight"] = t.get("weight", 1.0 / len(targets_list)) / total_weight
                    else:
                        t["weight"] = 1.0 / len(targets_list)
            
            # ç¡®ä¿æ¯ä¸ªç›®æ ‡æœ‰boundsï¼ˆdesirabilityæ–¹æ³•å¿…éœ€ï¼‰
            for t in targets_list:
                if "bounds" not in t or t["bounds"] is None:
                    # ä½¿ç”¨é»˜è®¤è¾¹ç•Œ
                    t["bounds"] = [0, 100]
                    
        elif optimization_strategy == "pareto":
            # å¸•ç´¯æ‰˜æ–¹æ³•ï¼šä¸éœ€è¦æƒé‡ï¼Œæ¢ç´¢å¸•ç´¯æ‰˜å‰æ²¿
            # ç§»é™¤æƒé‡ä¿¡æ¯ï¼ˆå¸•ç´¯æ‰˜ä¸ä½¿ç”¨ï¼‰
            for t in targets_list:
                t.pop("weight", None)
        
        # æ„å»ºä¼˜åŒ–é…ç½®
        optimization_config = {
            "targets": targets_list,
            "optimization_strategy": optimization_strategy,
            "parameters": verification_results.get("parameter_suggestions", {}),
            "constraints": constraints_list,
            "experimental_settings": {
                "batch_size": batch_size,
                "max_iterations": max_iterations,
                "total_budget": total_budget
            },
            "accept_suggested_parameters": accept_suggested_parameters,
            "custom_parameter_bounds": custom_bounds,
            "acquisition_function": acquisition_function,
            "auto_ratio_sum_constraint": auto_ratio_sum_constraint
        }
        
        # ç”ŸæˆBayBEå…¼å®¹çš„é…ç½®
        baybe_config = _generate_baybe_config(optimization_config, verification_results)
        
        # æ›´æ–°çŠ¶æ€
        state["optimization_config"] = optimization_config
        state["baybe_campaign_config"] = baybe_config
        state["verification_status"] = "completed_with_user_input"
        state["ready_for_optimization"] = True
        
        # æ„å»ºè¯¦ç»†çš„ç›®æ ‡ä¿¡æ¯æ˜¾ç¤ºï¼ˆæ ¹æ®ç­–ç•¥ä¸åŒæ˜¾ç¤ºä¸åŒå†…å®¹ï¼‰
        if optimization_strategy == "desirability":
            strategy_info = "ğŸ¯ **ä¼˜åŒ–ç­–ç•¥**: DesirabilityObjectiveï¼ˆæœŸæœ›åº¦æ–¹æ³•ï¼‰\n"
            strategy_info += "   - ä½¿ç”¨æƒé‡å°†å¤šç›®æ ‡åˆå¹¶ä¸ºå•ä¸€æ ‡é‡è¿›è¡Œä¼˜åŒ–\n"
            strategy_info += "   - é€‚åˆå·²çŸ¥å„ç›®æ ‡ç›¸å¯¹é‡è¦æ€§çš„æƒ…å†µ\n\n"
            
            targets_summary = "ğŸ¯ **ä¼˜åŒ–ç›®æ ‡è¯¦æƒ…**:\n"
            for i, target in enumerate(targets_list, 1):
                mode = target.get('mode', 'MAX')
                mode_str = "æœ€å¤§åŒ–" if mode == 'MAX' else ("æœ€å°åŒ–" if mode == 'MIN' else "ç›®æ ‡å€¼åŒ¹é…")
                weight_pct = target.get('weight', 0) * 100
                bounds = target.get('bounds', [0, 100])
                targets_summary += f"   {i}. {target.get('name')}: {mode_str}\n"
                targets_summary += f"      æƒé‡: {weight_pct:.1f}%, è¾¹ç•Œ: {bounds}\n"
        else:  # pareto
            strategy_info = "ğŸ¯ **ä¼˜åŒ–ç­–ç•¥**: ParetoObjectiveï¼ˆå¸•ç´¯æ‰˜æ–¹æ³•ï¼‰\n"
            strategy_info += "   - æ¢ç´¢å¸•ç´¯æ‰˜å‰æ²¿ï¼Œè¿”å›æ‰€æœ‰éæ”¯é…è§£\n"
            strategy_info += "   - é€‚åˆç›®æ ‡ç›¸äº’å†²çªã€æƒ³çœ‹æ‰€æœ‰æƒè¡¡æ–¹æ¡ˆçš„æƒ…å†µ\n"
            strategy_info += "   - æ¨èç»“æœä¼šåˆ†å¸ƒåœ¨å¸•ç´¯æ‰˜å‰æ²¿ä¸Šï¼Œä¾›æ‚¨é€‰æ‹©\n\n"
            
            targets_summary = "ğŸ¯ **ä¼˜åŒ–ç›®æ ‡è¯¦æƒ…**:\n"
            for i, target in enumerate(targets_list, 1):
                mode = target.get('mode', 'MAX')
                mode_str = "æœ€å¤§åŒ–" if mode == 'MAX' else ("æœ€å°åŒ–" if mode == 'MIN' else "ç›®æ ‡å€¼åŒ¹é…")
                targets_summary += f"   {i}. {target.get('name')}: {mode_str}\n"
        
        # çº¦æŸæ¡ä»¶æ˜¾ç¤º
        constraints_summary = ""
        if constraints_list:
            constraints_summary = "\nğŸ“ **çº¦æŸæ¡ä»¶**:\n"
            for i, constraint in enumerate(constraints_list, 1):
                constraints_summary += f"   {i}. {constraint.get('type', 'æœªçŸ¥ç±»å‹')}: {constraint.get('description', str(constraint))}\n"
        else:
            constraints_summary = "\nğŸ“ **çº¦æŸæ¡ä»¶**: æ— ç‰¹æ®Šçº¦æŸ\n"
        
        # å‚æ•°è¾¹ç•Œæ˜¾ç¤º
        params_summary = "\nğŸ“ **å‚æ•°è¾¹ç•Œ**: "
        if accept_suggested_parameters:
            params_summary += "ä½¿ç”¨ç³»ç»Ÿå»ºè®®çš„å‚æ•°èŒƒå›´\n"
        else:
            params_summary += "ä½¿ç”¨ç”¨æˆ·è‡ªå®šä¹‰èŒƒå›´\n"
            if custom_bounds:
                for param, bounds in custom_bounds.items():
                    params_summary += f"   - {param}: [{bounds.get('min', '?')}, {bounds.get('max', '?')}]\n"
        
        return f"""
âœ… **ä¼˜åŒ–é…ç½®å·²å®Œæˆ**

ğŸ“‹ **é…ç½®æ‘˜è¦**:
- ç›®æ ‡æ•°é‡: {len(targets_list)}
- å‚æ•°æ•°é‡: {len(optimization_config.get('parameters', {}))}
- çº¦æŸæ¡ä»¶: {len(constraints_list)}
- ç‰¹æ®Šç¼–ç : {len(verification_results.get('custom_encodings', {}))}

{strategy_info}{targets_summary}{constraints_summary}{params_summary}
âš™ï¸ **å®éªŒè®¾ç½®**:
- æ‰¹æ¬¡å¤§å° (batch_size): {batch_size}
- æœ€å¤§è½®æ•° (max_iterations): {max_iterations}
- æ€»å®éªŒé¢„ç®—: {total_budget}

ğŸš€ **ä¸‹ä¸€æ­¥**: ç³»ç»Ÿå°†æ„å»ºBayBEæœç´¢ç©ºé—´å¹¶å‡†å¤‡ä¼˜åŒ–Campaignã€‚

ğŸ“„ **BayBEé…ç½®å·²ä¿å­˜åˆ°ä¼šè¯çŠ¶æ€**ï¼Œå¯ä»¥ä¼ é€’ç»™ Recommender Agentã€‚
        """
        
    except json.JSONDecodeError as e:
        return f"""
âŒ **JSONè§£æé”™è¯¯**: {str(e)}

è¯·ç¡®ä¿ç›®æ ‡å’Œçº¦æŸæ¡ä»¶ä½¿ç”¨æ­£ç¡®çš„JSONæ ¼å¼ã€‚

ç›®æ ‡æ ¼å¼ç¤ºä¾‹:
[{{"name": "Target_alpha_tg", "mode": "MAX", "weight": 0.5}}]

çº¦æŸæ ¼å¼ç¤ºä¾‹:
[{{"type": "sum_equals_one", "parameters": ["ratio_A", "ratio_B"]}}]
"""
    except Exception as e:
        import traceback
        return f"é…ç½®å¤„ç†å‡ºé”™: {str(e)}\n{traceback.format_exc()}\nè¯·é‡æ–°æä¾›é…ç½®ä¿¡æ¯ã€‚"


def _generate_baybe_config(optimization_config: dict, verification_results: dict) -> dict:
    """
    ç”ŸæˆBayBEå…¼å®¹çš„é…ç½®æ ¼å¼ï¼ˆä»»åŠ¡6ï¼‰
    
    æ”¯æŒä¸¤ç§å¤šç›®æ ‡ä¼˜åŒ–ç­–ç•¥ï¼š
    1. DesirabilityObjective - æœŸæœ›åº¦æ–¹æ³•ï¼Œä½¿ç”¨æƒé‡åˆå¹¶å¤šç›®æ ‡
    2. ParetoObjective - å¸•ç´¯æ‰˜æ–¹æ³•ï¼Œæ¢ç´¢å¸•ç´¯æ‰˜å‰æ²¿
    """
    if not BAYBE_AVAILABLE:
        return {"error": "BayBE not available"}
    
    optimization_strategy = optimization_config.get("optimization_strategy", "desirability")
    targets = optimization_config.get("targets", [])
    
    # æ„å»ºç›®æ ‡é…ç½®
    target_configs = []
    for target in targets:
        target_config = {
            "name": target.get("name"),
            "mode": target.get("mode", "MAX"),
        }
        # desirability æ–¹æ³•éœ€è¦ bounds å’Œ transformation
        if optimization_strategy == "desirability":
            bounds = target.get("bounds", [0, 100])
            target_config["bounds"] = bounds
            # æ ¹æ®æ¨¡å¼é€‰æ‹©è½¬æ¢å‡½æ•°
            if target.get("mode") == "MATCH":
                target_config["transformation"] = "BELL"  # æˆ– "TRIANGULAR"
            else:
                target_config["transformation"] = "LINEAR"
        target_configs.append(target_config)
    
    # æ ¹æ®ç­–ç•¥æ„å»ºobjectiveé…ç½®
    if optimization_strategy == "pareto":
        objective_config = {
            "type": "ParetoObjective",
            "description": "æ¢ç´¢å¸•ç´¯æ‰˜å‰æ²¿ï¼Œè¿”å›æ‰€æœ‰éæ”¯é…è§£",
            "note": "æ¨èç»“æœä¼šåˆ†å¸ƒåœ¨å¸•ç´¯æ‰˜å‰æ²¿ä¸Šï¼Œé€‚åˆç›®æ ‡ç›¸äº’å†²çªçš„æƒ…å†µ"
        }
    else:  # desirability
        # æå–æƒé‡
        weights = [t.get("weight", 1.0 / len(targets)) for t in targets]
        objective_config = {
            "type": "DesirabilityObjective",
            "weights": weights,
            "scalarizer": "GEOM_MEAN",  # å‡ ä½•å¹³å‡ï¼Œå¯¹æç«¯å€¼æ›´æ•æ„Ÿ
            "description": "ä½¿ç”¨æƒé‡å°†å¤šç›®æ ‡åˆå¹¶ä¸ºå•ä¸€æ ‡é‡"
        }
    
    # æ ¹æ®å¼€å‘æ–‡æ¡£çš„æ ‡å‡†æ ¼å¼ç”Ÿæˆé…ç½®
    baybe_config = {
        "campaign_info": {
            "name": "chemical_optimization",
            "created_at": datetime.now().isoformat(),
            "description": "ChemBoMAS Enhanced Verification Agent generated configuration",
            "optimization_strategy": optimization_strategy
        },
        "targets": target_configs,
        "parameters": [],  # ç”± Recommender Agent å¡«å……
        "constraints": optimization_config.get("constraints", []),
        "objective_config": objective_config,
        "experimental_config": {
            "batch_size": optimization_config["experimental_settings"]["batch_size"],
            "max_iterations": optimization_config["experimental_settings"]["max_iterations"],
            "total_budget": optimization_config["experimental_settings"]["total_budget"],
            "recommender": "TwoPhaseMetaRecommender"
        }
    }
    
    return baybe_config


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("ğŸ§ª Enhanced Verification Tools åŠŸèƒ½æµ‹è¯•")
    print("=" * 50)
    
    # æµ‹è¯•1: SMILESéªŒè¯å™¨
    print("\n1. æµ‹è¯•SMILESéªŒè¯å™¨...")
    validator = SimplifiedSMILESValidator()
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_data = pd.DataFrame({
        'SubstanceA_SMILE': ['CCO', 'CCCCO', 'invalid_smiles', ''],
        'SubstanceB_SMILE': ['CC(C)O', 'CCCCCO', 'CCC', 'another_invalid'],
        'SubstanceA_ratio': [0.5, 0.6, 0.7, 0.8],
        'Target_alpha_tg': [80, 85, 90, 95]
    })
    
    validation_results = validator.validate_smiles_data(test_data)
    print(f"   æœ‰æ•ˆSMILES: {len(validation_results['canonical_smiles_mapping'])}")
    print(f"   æ— æ•ˆSMILES: {len(validation_results['invalid_smiles'])}")
    print(f"   éªŒè¯çš„ç‰©è´¨: {validation_results['substances_validated']}")
    
    # æµ‹è¯•2: å‚æ•°å»ºè®®å™¨ï¼ˆåŸºäºçŸ¥è¯†åº“ï¼‰
    print("\n2. æµ‹è¯•å‚æ•°å»ºè®®å™¨ï¼ˆå·²æ•´åˆåŒ–å­¦çŸ¥è¯†åº“ï¼‰...")
    advisor = IntelligentParameterAdvisor()
    suggestions = advisor.analyze_experimental_context(test_data, "ç¯æ°§æ ‘è„‚å›ºåŒ–å®éªŒ")
    
    # æ˜¾ç¤ºè¯†åˆ«çš„ååº”ç±»å‹
    reaction_info = suggestions.get("_reaction_info", {})
    print(f"   è¯†åˆ«çš„ååº”ç±»å‹: {reaction_info.get('reaction_name', 'æœªçŸ¥')}")
    print(f"   å®‰å…¨è­¦å‘Šæ•°é‡: {len(reaction_info.get('safety_warnings', []))}")
    
    # æ˜¾ç¤ºå‚æ•°å»ºè®®
    param_count = len([k for k in suggestions.keys() if not k.startswith('_')])
    print(f"   å‚æ•°å»ºè®®æ•°é‡: {param_count}")
    for param, suggestion in suggestions.items():
        if not param.startswith('_'):  # è·³è¿‡å…ƒä¿¡æ¯
            print(f"   {param}:")
            print(f"      å½“å‰èŒƒå›´: {suggestion.get('current_range')}")
            # æ£€æŸ¥æ˜¯å¦ä¸ºç¦»æ•£å‚æ•°
            if suggestion.get('type') == 'NumericalDiscreteParameter':
                print(f"      å‚æ•°ç±»å‹: ç¦»æ•£å‚æ•°")
                print(f"      ç¦»æ•£å€¼: {suggestion.get('values', [])}")
                print(f"      æ£€æµ‹ç†ç”±: {suggestion.get('discrete_reasoning', 'è‡ªåŠ¨æ£€æµ‹')}")
            else:
                print(f"      å‚æ•°ç±»å‹: è¿ç»­å‚æ•°")
                print(f"      å»ºè®®èŒƒå›´: {suggestion.get('suggested_bounds')}")
                print(f"      ç†ç”±: {suggestion.get('reasoning', 'N/A')}")
            print(f"      æ¥æº: {suggestion.get('source', 'unknown')}")
    
    # æµ‹è¯•3: ç”¨æˆ·å®šä¹‰ç¼–ç å¤„ç†å™¨
    print("\n3. æµ‹è¯•ç”¨æˆ·å®šä¹‰ç¼–ç å¤„ç†å™¨...")
    encoder = UserDefinedEncodingHandler()
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_df = pd.DataFrame({
        'SubstanceA_name': ['æ ‘è„‚A', 'æ ‘è„‚B'],
        'SubstanceA_SMILES': ['CCO', 'CCCCO'], 
        'SubstanceB_name': ['ç¨€é‡Šå‰‚A', 'ç¨€é‡Šå‰‚B'],
        'SubstanceB_SMILES': ['', ''],  # ç‰¹æ®Šç‰©è´¨
        'SubstanceA_density': [1.15, 1.18],  # ç‰©ç†æ€§è´¨
        'Process_temperature': [80, 90]  # å·¥è‰ºå‚æ•°
    })
    
    user_special_data = encoder.identify_user_special_substances(test_df)
    classification = encoder.classify_user_columns(test_df)
    
    print(f"   è¯†åˆ«åˆ°ç‰¹æ®Šç‰©è´¨: {len(user_special_data['substances_without_smiles'])} ä¸ª")
    print(f"   è¯†åˆ«åˆ°æ‰©å±•åˆ—ç±»å‹: {len(classification['è¯†åˆ«çš„æ‰©å±•åˆ—'])} ç§")
    
    # æµ‹è¯•4: BayBEå¯ç”¨æ€§
    print("\n4. æµ‹è¯•BayBEå¯ç”¨æ€§...")
    if BAYBE_AVAILABLE:
        print("   âœ… BayBEå·²å®‰è£…ï¼Œå¯ä»¥ä½¿ç”¨å®Œæ•´åŠŸèƒ½")
        
        # æµ‹è¯•BayBEå‚æ•°åˆ›å»º
        try:
            parameters = validator.prepare_baybe_parameters(test_data, validation_results)
            print(f"   âœ… æˆåŠŸåˆ›å»º {len(parameters)} ä¸ªBayBEå‚æ•°")
        except Exception as e:
            print(f"   âŒ BayBEå‚æ•°åˆ›å»ºå¤±è´¥: {e}")
    else:
        print("   âš ï¸ BayBEæœªå®‰è£…ï¼Œä½¿ç”¨é™çº§æ¨¡å¼")
        print("   å»ºè®®è¿è¡Œ: pip install baybe")
    
    print("\n" + "=" * 50)
    print("ğŸ“Š Enhanced Verification Tools æµ‹è¯•å®Œæˆ")
    
    if BAYBE_AVAILABLE:
        print("ğŸ‰ æ‰€æœ‰åŠŸèƒ½å¯ç”¨ï¼ç³»ç»Ÿå·²å‡†å¤‡å¥½è¿›è¡Œå®Œæ•´çš„BayBEé›†æˆ")
    else:
        print("ğŸ”§ æ ¸å¿ƒåŠŸèƒ½å¯ç”¨ï¼å®‰è£…BayBEåå³å¯ä½¿ç”¨å®Œæ•´åŠŸèƒ½")
        print("   è¿è¡Œ: pip install baybe")
